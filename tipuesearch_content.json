{"pages":[{"title":"About","text":"Why I started this blog My background My family My hobbies","tags":"pages","url":"pages/about.html"},{"title":"[Review] Guess word and hangman game","text":"Guess max score word 这个游戏来自 Pybites 。 整个游戏的流程是这样的： 首先随机的给出十个英文字母，如 O, S, J, I, O, O, R, H, X, D , 之后玩家以这些字母拼出合理的英文单词。以给出字母为例，可以组成 DOOR ，但无法使用超过给出字母数量的单词来组词。之后会根据字母的稀有度给玩家所组单词来打分，如 E 、 A 等字母为一分，而 Y , Z 等字母则算作八九分。系统同时计算出这些字母实际能组成的最高分的单词，并计算玩家的打分比例。如果我们猜测的值为 DOOR ，则我们获得的字母分为五分，而给出的字母能组成的最高分单词为 SHOJI (商会)分值为十五分，我们的得分比例则为33%。 该游戏的源码可在此 获得 。 在寻找给出字母能组成的最高分单词时，我们使用了以下代码: def get_possible_dict_words ( draw ): \"\"\"Get all possible words from draw which are valid dictionary words. Use the _get_permutations_draw helper and DICTIONARY constant\"\"\" return set ( DICTIONARY ) & set ( map ( \"\" . join , _get_permutations_draw ( draw ))) def _get_permutations_draw ( draw ): \"\"\"Helper for get_possible_dict_words to get all permutations of draw letters. Hint: use itertools.permutations\"\"\" for length in range ( 1 , len ( draw ) + 1 ): yield from set ( itertools . permutations ( draw , length )) _get_permutations_draw 生成器使用了 itertools.permutations 方法生成了所给单词组的所有长度的排列。这里 ab 和 ba 要被作为两个单词存在，所以需要使用排列。我在生成器里就早早使用了 set 方法去重，之后在 get_possible_dict_words 方法里又将组成的字符串再次去重，实际上是没有必要的。可以只保留后面一个去重步骤来提高整体函数的运行效率。 在 get_possible_dict_words 中，我运用了python集合的交集操作来取得所有在字典里的有效词组组合。最终返回所要求的单词集合。 那些最高分的单词中都是一些冷门词汇，所以总的来说，这还是个比较有趣的游戏。 Hangman 这里还有另一个有意思的 Hangman 小游戏。 Hangman是一系列猜词游戏的简称。在我们的Hangman中，我们需要猜的是电源的英文名称。每次失败将都会在Hangman添加一笔，最终形成一副完整的Hangman图，游戏失败。 ________ | | | 0 | /|\\ | / \\ |\"\"\" 该游戏的源码可在此 获得 。 首先我们先处理一下要猜的单词： def _construct_word ( self , word : str ) -> defaultdict : character_dict = defaultdict ( set ) for index , character in enumerate ( word ): if character . strip () and character in ASCII : character_dict [ character ] . add ( index ) return character_dict 在此我们将所有的有效字母及其序号保存在字典中，如 tootsie 中的 o 则被保存为 o: (1, 2) 。此外，将谜面作为一个列表储存，还未揭晓的数字用 PlACEHOLDER 替代，则猜到 o 后的谜面为 ['_', 'o', 'o', '_', '_', '_', '_'] 。这样在之后玩家猜到字典中的字母时我们: 1.将该字母从字典里弹出；2.将谜面列表中所有该字母储存的序号的元素替换为该字母。当字典里没有元素时候，用户便猜出了相关单词。 另外需要考虑的是用户猜测失败的例子，有两种情况会导致一次猜测的失败。一是用户所猜字母不在我们所给的谜底之中，这时我们输出一张Hangman的图片，如果用户的错误次数过多将导致游戏的失败。二是用户猜了一个之前猜过的字母，我们这时仁慈的弹出提示并给予用户一次额外的猜测机会。这部分实现的代码如下: @types.coroutine def _hangman_popper ( self ): \"\"\"print a hangman graph if guess is not right\"\"\" guessed_character = set () graphics = hang_graphics () graph = next ( graphics ) while True : guess = yield False if guess not in guessed_character : guessed_character . add ( guess ) if guess in self . _word : print ( f '{colored(len(self._word[guess]),\"green\")} of {colored(guess,\"green\")} in the word.' ) for index in self . _word . pop ( guess ): self . _guess [ index ] = guess if not self . _word : yield True else : print ( f '{colored(guess,\"green\")} is not in the word! \\n ' f '{graph} \\n ' ) try : graph = next ( graphics ) except StopIteration : raise NoChance else : print ( f 'You have guessed {colored(guess,\"green\")} before, please choose another character. \\n ' ) 输出越来越完整的Hangman图片是以生成器的方式实现的，这启发了我以协程的方式实现游戏与用户的交互过程。协程有三种输出，分别是猜测失败时的 yield False ，完成谜底时的 yield True 和机会用完时的 raise NoChane 。一个还需要注意的地方是，我们需要在Hangman类的构造函数中预激该生成器。 这也是一个相当长知识的游戏，不仅如此，用协程实现的过程也是相当有趣的，是一种不一样的编程体验。 Additional","tags":"posts","url":"review-guess-word-and-hangman-game.html"},{"title":"StackOverflow list generator algorithms","text":"Faltten nested lists with indices 这个问题来自 StackOverflow 。 对于一个嵌套的列表L: L = [[[ 1 , 2 , 3 ], [ 4 , 5 ]], [ 6 ], [ 7 ,[ 8 , 9 ]], 10 ] 希望有一个函数能够yield出每个元素的嵌套位置元组： ( 1 , ( 0 , 0 , 0 )) ( 2 , ( 0 , 0 , 1 )) ( 3 , ( 0 , 0 , 2 )) ( 4 , ( 0 , 1 , 0 )) ( 5 , ( 0 , 1 , 1 )) ( 6 , ( 1 , 0 )) ( 7 , ( 2 , 0 )) ( 8 , ( 2 , 1 , 0 )) ( 9 , ( 2 , 1 , 1 )) ( 10 , ( 3 ,)) 以 4 为例，元组 (0,1,0) 代表它处于第一层的 首个列表 中的 第二个列表 的 第一个元素 的的位置上。 之前我也遇到过一个 yield from 的示例，在此不得不提一下 def flatten ( iter ): for item in iter : if isinstance ( item , list ): yield from flatten ( item ) else : yield item 这个函数用递归的方式讲嵌套列表里的元素取出， yield 出一个 flatten 版本的列表： [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] 。 回归到这个问题，提问者给出了一个基于递归和生成器的解决方案： def flatten ( l ): for i , e in enumerate ( l ): try : for x , y in flatten ( e ): yield x , ( i ,) + y except : yield e , ( i ,) 这个函数的关键在于利用了元组加法的性质 (1, 2) + (3,) = (1, 2, 3) 来返回出嵌套位置的信息，而元组本身则是由变量 e 和 x 传递。 而底下的答案则提供了一个非递归的方案： def flatten ( l ): stack = [ enumerate ( l )] path = [ None ] while stack : for path [ - 1 ], x in stack [ - 1 ]: if isinstance ( x , list ): stack . append ( enumerate ( x )) path . append ( None ) else : yield x , tuple ( path ) break else : stack . pop () path . pop () 这个算法定义了两个栈 stack 和 path 。 path 用来存储当前追踪元素的嵌套位置信息，而 stack 则是以外层至内层的顺序将列表的 enumerate 迭代器入栈。并按层次遍历。算法本身巧妙的一点是将入栈的初始位置置为 None ，之后再通过对 enumerate 的拆包用元素的位置信息替换 path 栈的最后一个元素。这个方法没有使用递归。 Iterate through array while finding the mean of the top k elements 来源于Stack Overflow的问题: Python iterate through array while finding the mean of the top k elements 。 问题的描述是这样的：对于一个列表 a = [3, 5, 2, 7, 5, 3, 6, 8, 4] , 我们希望找到所有连续三个元素中最大两个元素的均值。对于举例的列表 a 将生成列表 [4, 6, 6, 6, 5.5, 7, 7] 。首个连续三元素组合 [3, 5, 2] 的计算结果为 4 。 我们可把整个问题分解成三个部分： 1. 获得每个连续三元组 2. 取出前两大的元素 3. 计算平均值。对于所有问题，我比较喜欢这个解答： # Sliding window def windowed_iterator ( iterable , n = 2 ): iterators = itertools . tee ( iterable , n ) iterators = ( itertools . islice ( it , i , None ) for i , it in enumerate ( iterators )) yield from zip ( * iterators ) windows = windowed_iterator ( iterable = a , n = 3 ) # Top 2 elements from heapq import nlargest top_n = map ( lambda x : nlargest ( 2 , x ), windows ) # Mean from statistics import mean means = map ( mean , top_n ) 比较有趣的是 Sliding window 的部分，这里我们用 itertools.tee 生成三个独立的迭代器。并且用 islice 和 zip 进行错位和组合，最终生成连续三元组序列。 Additional","tags":"posts","url":"stackoverflow-list-generator-algorithms.html"},{"title":"Build a circular list and regex trie","text":"Dancing programs 这个问题来自 Advent of code 的 第十六日 。 解决代码如下: def solution ( lst , order ): # 循环数组，start为头指针 result_list = list ( order ) # 减少Partner时间复杂度的字典 result_dict = { s : i for i , s in enumerate ( result_list )} length = len ( result_list ) start = 0 for action in lst : if action . startswith ( 's' ): start = ( start + length ) % length - int ( action [ 1 :]) continue if action . startswith ( 'x' ): x , y = action [ 1 :] . split ( '/' ) x_real = ( start + int ( x )) % length y_real = ( start + int ( y )) % length elif action . startswith ( 'p' ): x , y = action [ 1 :] . split ( '/' ) x_real = result_dict [ x ] y_real = result_dict [ y ] result_list [ x_real ], result_list [ y_real ] = result_list [ y_real ], result_list [ x_real ] result_dict [ result_list [ x_real ]], result_dict [ result_list [ y_real ]] = result_dict [ result_list [ y_real ]], \\ result_dict [ result_list [ x_real ]] return \"\" . join ( result_list [ i % length ] for i in range ( start , start + length )) def solution2 ( lst ， order ): orders = [] while order not in orders : orders . append ( order ) order = solution ( lst , order ) return orders [ 1000000000 % len ( orders )] 对于第一部分涉及到的三个操作。第一个操作Spin sX ，只需将头指针后移X位。对于第二个操作Exchange和第三个操作Partner，由于涉及到index和值的比较，因此我们通过建立{值：index}字典的形式来减少Partner操作的时间复杂度。 第二部分问题由于要循环一亿次，全部遍历时间复杂度过高。测试发现当输入相同的操作序列时，多次遍历后会出现循环的情况，因此有解法二。 Regex replacement 来源于Stack Overflow的一个问题: Speed up millions of regex replacements in Python 3 。 赞同最多的 \"\\b(word1|word2|word3)\\b\" 方法其实不是最好的算法，正则并集会带来O(1)的最好时间复杂度，O(n)的平均及最差时间复杂度。验证代码如下： import re import timeit import random with open ( '/usr/share/dict/american-english' ) as wordbook : english_words = [ word . strip () . lower () for word in wordbook ] random . shuffle ( english_words ) print ( \"First 10 words :\" ) print ( english_words [: 10 ]) test_words = [ ( \"Surely not a word\" , \"#surely_NöTäWORD_so_regex_engine_can_return_fast\" ), ( \"First word\" , english_words [ 0 ]), ( \"Last word\" , english_words [ - 1 ]), ( \"Almost a word\" , \"couldbeaword\" ) ] def find ( word ): def fun (): return union . match ( word ) return fun for exp in range ( 1 , 6 ): print ( \" \\n Union of %d words\" % 10 ** exp ) union = re . compile ( r \"\\b( %s )\\b\" % '|' . join ( english_words [: 10 ** exp ])) for description , test_word in test_words : time = timeit . timeit ( find ( test_word ), number = 1000 ) * 1000 print ( \" %-17s : %.1f ms\" % ( description , time )) Output: First 10 words : [ \"geritol's\" , \"sunstroke's\" , 'fib' , 'fergus' , 'charms' , 'canning' , 'supervisor' , 'fallaciously' , \"heritage's\" , 'pastime' ] Union of 10 words Surely not a word : 0.7 ms First word : 0.8 ms Last word : 0.7 ms Almost a word : 0.7 ms Union of 100 words Surely not a word : 0.7 ms First word : 1.1 ms Last word : 1.2 ms Almost a word : 1.2 ms Union of 1000 words Surely not a word : 0.7 ms First word : 0.8 ms Last word : 9.6 ms Almost a word : 10.1 ms Union of 10000 words Surely not a word : 1.4 ms First word : 1.8 ms Last word : 96.3 ms Almost a word : 116.6 ms Union of 100000 words Surely not a word : 0.7 ms First word : 0.8 ms Last word : 1227.1 ms Almost a word : 1404.1 ms 这会导致整体算法的时间复杂度变为O(n&#94;2),因此有如下两个解法： import re with open ( '/usr/share/dict/american-english' ) as wordbook : banned_words = set ( word . strip () . lower () for word in wordbook ) def delete_banned_words ( matchobj ): word = matchobj . group ( 0 ) if word . lower () in banned_words : return \"\" else : return word sentences = [ \"I'm eric. Welcome here!\" , \"Another boring sentence.\" , \"GiraffeElephantBoat\" , \"sfgsdg sdwerha aswertwe\" ] * 250000 for sentence in sentences : sentence = re . sub ( '\\w+' , delete_banned_words , sentence ) 这个解法时间发复杂度为O(n)，但是会在去掉单词时留下空格。因为它是扫描 \\w+ 的，因此无法避免这个问题。 一个对之前正则并集的改进是通过字典树的方式创建正则并集，该算法的时间复杂度也为O(n)。 class Trie (): \"\"\"Regex::Trie in Python. Creates a Trie out of a list of words. The trie can be exported to a Regex pattern. The corresponding Regex should match much faster than a simple Regex union.\"\"\" def __init__ ( self ): self . data = {} def add ( self , word ): ref = self . data for char in word : ref [ char ] = char in ref and ref [ char ] or {} ref = ref [ char ] ref [ '' ] = 1 def dump ( self ): return self . data def quote ( self , char ): return re . escape ( char ) def _pattern ( self , pData ): data = pData if \"\" in data and len ( data . keys ()) == 1 : return None alt = [] cc = [] q = 0 for char in sorted ( data . keys ()): if isinstance ( data [ char ], dict ): recurse = self . _pattern ( data [ char ]) if recurse : alt . append ( self . quote ( char ) + recurse ) else : cc . append ( self . quote ( char )) else : q = 1 cconly = not len ( alt ) > 0 if len ( cc ) > 0 : if len ( cc ) == 1 : alt . append ( cc [ 0 ]) else : alt . append ( '[' + '' . join ( cc ) + ']' ) if len ( alt ) == 1 : result = alt [ 0 ] else : result = \"(?:\" + \"|\" . join ( alt ) + \")\" if q : if cconly : result += \"?\" else : result = \"(?: %s )?\" % result return result def pattern ( self ): return self . _pattern ( self . dump ()) 这段代码首先会生成所有被ban单词的字典树，然后parse该树生成像如下这样的正则表达式： (?:a(?:(?:\\'s|a(?:\\'s|chen|liyah(?:\\'s)?|r(?:dvark(?:(?:\\'s|s))?|on))|b(?:\\'s|a(?:c(?:us(?:(?:\\'s|es))?|[ik])|ft|lone(?:(?:\\'s|s))?|ndon(?:(?:ed|ing|ment(?:\\'s)?|s))?|s(?:e(?:(?:ment(?:\\'s)?|[ds]))?|h(?:(?:e[ds]|ing))?|ing)|t(?:e(?:(?:ment(?:\\'s)?|[ds]))?|ing|toir(?:(?:\\'s|s))?))|b(?:as(?:id)?|e(?:ss(?:(?:\\'s|es))?|y(?:(?:\\'s|s))?)|ot(?:(?:\\'s|t(?:\\'s)?|s))?|reviat(?:e[ds]?|i(?:ng|on(?:(?:\\'s|s))?))|y(?:\\'s)?|\\é(?:(?:\\'s|s))?)|d(?:icat(?:e[ds]?|i(?:ng|on(?:(?:\\'s|s))?))|om(?:en(?:(?:\\'s|s))?|inal)|u(?:ct(?:(?:ed|i(?:ng|on(?:(?:\\'s|s))?)|or(?:(?:\\'s|s))?|s))?|l(?:\\'s)?))|e(?:(?:\\'s|am|l(?:(?:\\'s|ard|son(?:\\'s)?))?|r(?:deen(?:\\'s)?|nathy(?:\\'s)?|ra(?:nt|tion(?:(?:\\'s|s))?))|t(?:(?:t(?:e(?:r(?:(?:\\'s|s))?|d)|ing|or(?:(?:\\'s|s))?)|s))?|yance(?:\\'s)?|d))?|hor(?:(?:r(?:e(?:n(?:ce(?:\\'s)?|t)|d)|ing)|s))?|i(?:d(?:e[ds]?|ing|jan(?:\\'s)?)|gail|l(?:ene|it(?:ies|y(?:\\'s)?)))|j(?:ect(?:ly)?|ur(?:ation(?:(?:\\'s|s))?|e[ds]?|ing))|l(?:a(?:tive(?:(?:\\'s|s))?|ze)|e(?:(?:st|r))?|oom|ution(?:(?:\\'s|s))?|y)|m\\'s|n(?:e(?:gat(?:e[ds]?|i(?:ng|on(?:\\'s)?))|r(?:\\'s)?)|ormal(?:(?:it(?:ies|y(?:\\'s)?)|ly))?)|o(?:ard|de(?:(?:\\'s|s))?|li(?:sh(?:(?:e[ds]|ing))?|tion(?:(?:\\'s|ist(?:(?:\\'s|s))?))?)|mina(?:bl[ey]|t(?:e[ds]?|i(?:ng|on(?:(?:\\'s|s))?)))|r(?:igin(?:al(?:(?:\\'s|s))?|e(?:(?:\\'s|s))?)|t(?:(?:ed|i(?:ng|on(?:(?:\\'s|ist(?:(?:\\'s|s))?|s))?|ve)|s))?)|u(?:nd(?:(?:ed|ing|s))?|t)|ve(?:(?:\\'s|board))?)|r(?:a(?:cadabra(?:\\'s)?|d(?:e[ds]?|ing)|ham(?:\\'s)?|m(?:(?:\\'s|s))?|si(?:on(?:(?:\\'s|s))?|ve(?:(?:\\'s|ly|ness(?:\\'s)?|s))?))|east|idg(?:e(?:(?:ment(?:(?:\\'s|s))?|[ds]))?|ing|ment(?:(?:\\'s|s))?)|o(?:ad|gat(?:e[ds]?|i(?:ng|on(?:(?:\\'s|s))?)))|upt(?:(?:e(?:st|r)|ly|ness(?:\\'s)?))?)|s(?:alom|c(?:ess(?:(?:\\'s|e[ds]|ing))?|issa(?:(?:\\'s|[es]))?|ond(?:(?:ed|ing|s))?)|en(?:ce(?:(?:\\'s|s))?|t(?:(?:e(?:e(?:(?:\\'s|ism(?:\\'s)?|s))?|d)|ing|ly|s))?)|inth(?:(?:\\'s|e(?:\\'s)?))?|o(?:l(?:ut(?:e(?:(?:\\'s|ly|st?))?|i(?:on(?:\\'s)?|sm(?:\\'s)?))|v(?:e[ds]?|ing))|r(?:b(?:(?:e(?:n(?:cy(?:\\'s)?|t(?:(?:\\'s|s))?)|d)|ing|s))?|pti... Additional","tags":"posts","url":"build-a-circular-list-and-regex-trie.html"},{"title":"Top 10 Python libraries of 2017","text":"December is the time when you sit back and think about the accomplishments of the past year. For us programmers, this is often looking at the open source libraries that were either released this year (or close enough), or whose popularity has recently boomed because they are simply great tools to solve a particular problem. For the past two years , we have done this in the form of a blog post with what we consider to be some of the best work that has been done in the Python community. Now, as we are wrapping up 2017, we had to do the same. This time, the list comes with a Machine Learning (ML) flavor . Forgive us, authors of great non-ML libraries, but we are just too biased ;) Hopefully, people in the comments help us do some justice and mention other great pieces of software, some of which have surely have escaped our radar. So, without further ado, here it goes! 1. Pipenv We couldn't make this list without reserving the top spot for a tool that was only released early this year, but has the power to affect the workflow of every Python developer, especially more now since it has become the officially recommended tool on Python.org for managing dependencies! Pipenv, originally started as a weekend project by the awesome Kenneth Reitz , aims to bring ideas from other package managers (such as npm or yarn ) into the Python world. Forget about installing virtualenv , virtualenvwrapper , managing requirements.txt files and ensuring reproducibility with regards to versions of dependencies of the dependencies (read here for more info about this). With Pipenv, you specify all your dependencies in a Pipfile — which is normally built by using commands for adding, removing, or updating dependencies. The tool can generate a Pipfile.lock file, enabling your builds to be deterministic , helping you avoid those difficult to catch bugs because of some obscure dependency that you didn't even think you needed. Of course, Pipenv comes with many other perks and has great documentation , so make sure to check it out and start using it for all your Python projects, as we do at Tryolabs :) 2. PyTorch If there is a library whose popularity has boomed this year, especially in the Deep Learning (DL) community, it's PyTorch, the DL framework introduced by Facebook this year. PyTorch builds on and improves the (once?) popular Torch framework, especially since it's Python based — in contrast with Lua. Given how people have been switching to Python for doing data science in the last couple of years, this is an important step forward to make DL more accessible. Most notably, PyTorch has become one of the go-to frameworks for many researchers, because of its implementation of the novel Dynamic Computational Graph paradigm. When writing code using other frameworks like TensorFlow , CNTK or MXNet , one must first define something called a computational graph . This graph specifies all the operations that will be run by our code, which are later compiled and potentially optimized by the framework, in order to allow for it to be able to run even faster, and in parallel on a GPU. This paradigm is called Static Computational Graph , and is great since you can leverage all sorts of optimizations and the graph, once built, can potentially run in different devices (since execution is separate from building ). However, in many tasks such as Natural Language Processing, the amount of \"work\" to do is often variable: you can resize images to a fixed resolution before feeding them to an algorithm, but cannot do the same with sentences which come in variable length. This is where PyTorch and dynamic graphs shine, by letting you use standard Python control instructions in your code, the graph will be defined when it is executed, giving you a lot of freedom which is essential for several tasks. Of course, PyTorch also computes gradients for you (as you would expect from any modern DL framework), is very fast, and extensible , so why not give it a try? 3. Caffe2 It might sound crazy, but Facebook also released another great DL framework this year. The original Caffe framework has been widely used for years, and known for unparalleled performance and battle-tested codebase. However, recent trends in DL made the framework stagnate in some directions. Caffe2 is the attempt to bring Caffe to the modern world. It supports distributed training, deployment (even in mobile platforms), the newest CPUs and CUDA-capable hardware. While PyTorch may be better for research, Caffe2 is suitable for large scale deployments as seen on Facebook. Also, check out the recent ONNX effort . You can build and train your models in PyTorch, while using Caffe2 for deployment! Isn't that great? 4. Pendulum Last year, Arrow , a library that aims to make your life easier while working with datetimes in Python, made the list. This year, it is the turn of Pendulum. One of Pendulum's strength points is that it is a drop-in replacement for Python's standard datetime class, so you can easily integrate it with your existing code, and leverage its functionalities only when you actually need them. The authors have put special care to ensure timezones are handled correctly, making every instance timezone-aware and UTC by default. You will also get an extended timedelta to make datetime arithmetic easier. Unlike other existing libraries, it strives to have an API with predictable behavior, so you know what to expect. If you are doing any non trivial work involving datetimes, this will make you happier! Check out the docs for more. 5. Dash You are doing data science, for which you use the excellent available tools in the Python ecosystem like Pandas and scikit-learn . You use Jupyter Notebooks for your workflow, which is great for you and your colleagues. But how do you share the work with people who do not know how to use those tools? How do you build an interface so people can easily play around with the data, visualizing it in the process? It used to be the case that you needed a dedicated frontend team, knowledgeable in Javascript, for building these GUIs. Not anymore. Dash, announced this year , is an open source library for building web applications, especially those that make good use of data visualization, in pure Python. It is built on top of Flask , Plotly.js and React , and provides abstractions that free you from having to learn those frameworks and let you become productive quickly. The apps are rendered in the browser and will be responsive so they will be usable in mobile devices. If you would like to know more about what is possible with Dash, the Gallery is a great place for some eye-candy. 6. PyFlux There are many libraries in Python for doing data science and ML, but when your data points are metrics that evolve over time (such as stock prices, measurements obtained from instruments, etc), that is not the case. PyFlux is an open source library in Python built specifically for working with time series . The study of time series is a subfield of statistics and econometrics, and the goals can be describing how time series behave (in terms of latent components or features of interest), and also predicting how they will behave the future. PyFlux allows for a probabilistic approach to time series modeling, and has implementations for several modern time series models like GARCH . Neat stuff. 7. Fire It is often the case that you need to make a Command Line Interface (CLI) for your project. Beyond the traditional argparse , Python has some great tools like click or docopt . Fire, announced by Google this year , has a different take on solving this same problem. Fire is an open source library that can automatically generate a CLI for any Python project. The key here is automatically : you almost don't need to write any code or docstrings to build your CLI! To do the job, you only need to call a Fire method and pass it whatever you want turned into a CLI: a function, an object, a class, a dictionary, or even pass no arguments at all (which will turn your entire code into a CLI). Make sure to read the guide so you understand how it works with examples. Keep it under your radar, because this library can definitely save you a lot of time in the future. 8. imbalanced-learn In an ideal world, we would have perfectly balanced datasets and we would all train models and be happy. Unfortunately, the real world is not like that, and certain tasks favor very imbalanced data. For example, when predicting fraud in credit card transactions, you would expect that the vast majority of the transactions (+99.9%?) are actually legit. Training ML algorithms naively will lead to dismal performance, so extra care is needed when working with these types of datasets. Fortunately, this is a studied research problem and a variety of techniques exist. Imbalanced-learn is a Python package which offers implementations of some of those techniques, to make your life much easier. It is compatible with scikit-learn and is part of scikit-learn-contrib projects. Useful! 9. FlashText When you need to search for some text and replace it for something else, as is standard in most data-cleaning work, you usually turn to regular expressions. They will get the job done, but sometimes it happens that the number of terms you need to search for is in the thousands, and then, reg exp can become painfully slow to use. FlashText is a better alternative just for this purpose. In the author's initial benchmark , it improved the runtime of the entire operation by a huge margin: from 5 days to 15 minutes. The beauty of FlashText is that the runtime is the same no matter how many search terms you have, in contrast with regexp in which the runtime will increase almost linearly with the number of terms. FlashText is a testimony to the importance of the design of algorithms and data structures, showing that, even for simple problems, better algorithms can easily outdo even the fastest CPUs running naive implementations. 10. Luminoth Disclaimer: this library was built by Tryolabs' R&D area. Images are everywhere nowadays, and understanding their content can be critical for several applications. Thankfully, image processing techniques have advanced a lot, fueled by the advancements in DL. Luminoth is an open source Python toolkit for computer vision, built using TensorFlow and Sonnet . Currently, it out-of-the-box supports object detection in the form of a model called Faster R-CNN. But Luminoth is not only an implementation of a particular model. It is built to be modular and extensible, so customizing the existing pieces or extending it with new models to tackle different problems should be straightforward, with as much code reuse as there can be. It provides tools for easily doing the engineering work that are needed when building DL models: converting your data (in this case, images) to adequate format for feeding your data pipeline ( TensorFlow's tfrecords ), doing data augmentation, running the training in one or multiple GPUs (distributed training will be a must when working with large datasets), running evaluation metrics, easily visualizing stuff in TensorBoard and deploying your trained model with a simple API or browser interface, so people can play around with it. Moreover, Luminoth has straightforward integration with Google Cloud's ML Engine , so even if you don't own a powerful GPU, you can train in the cloud with a single command, just as you do in your own local machine. If you are interested in learning more about what's behind the scenes, you can read the announcement blog post and watch the video of our talk at ODSC. Bonus: watch out for these PyVips You may have never heard of the libvips library. In that case, you must know that it's an image processing library, like Pillow or ImageMagick , and supports a wide range of formats. However, when comparing to other libraries, libvips is faster and uses less memory . For example, some benchmarks show it to be about 3x faster and use less than 15x memory as ImageMagick. You can read more about why libvips is nice here . PyVips is a recently released Python binding for libvips, which is compatible with Python 2.7-3.6 (and even PyPy), easy to install with pip and drop-in compatible with the old binding, so if you are using that, you don't have to modify your code. If doing some sort of image processing in your app, definitely something to keep an eye on. Requestium Disclaimer: this library was built by Tryolabs. Sometimes, you need to automatize some actions in the web. Be it when scraping sites, doing application testing, or filling out web forms to perform actions in sites that do not expose an API, automation is always necessary. Python has the excellent Requests library which allows you perform some of this work, but unfortunately (or not?) many sites make heavy client side use of Javascript. This means that the HTML code that Requests fetches, in which you could be trying to find a form to fill for your automation task, may not even have the form itself! Instead, it will be something like an empty div of some sort that will be generated in the browser with a modern frontend library such as React or Vue . One way to solve this is to reverse-engineer the requests that Javascript code makes, which will mean many hours of debugging and fiddling around with (probably) uglified JS code. No thanks. Another option is to turn to libraries like Selenium , which allow you to programmatically interact with a web browser and run the Javascript code. With this, the problems are no more, but it is still slower than using plain Requests which adds very little overhead. Wouldn't it be cool if there was a library that let you start out with Requests and seamlessly switch to Selenium, only adding the overhead of a web browser when actually needing it? Meet Requestium, which acts as a drop-in replacement for Requests and does just that. It also integrates Parsel , so writing all those selectors for finding the elements in the page is much cleaner than it would otherwise be, and has helpers around common operations like clicking elements and making sure stuff is actually rendered in the DOM. Another time saver for your web automation projects! skorch You like the awesome API of scikit-learn, but need to do work using PyTorch? Worry not, skorch is a wrapper which will give PyTorch an interface like sklearn. If you are familiar with those libraries, the syntax should be straightforward and easy to understand. With skorch, you will get some code abstracted away, so you can focus more on the things that really matter, like doing your data science. Conclusion What an exciting year! If you know of a library that deserves to be on this list, make sure you mention it in the comments below. There are so many good developments that it's hard to keep up. As usual, thanks to everybody in the community for such great work! Finally, don't forget to subscribe to our newsletter so that you don't miss out future editions of this post or our ML related content.","tags":"Python","url":"top-10-python-libraries-of-2017.html"},{"title":"Find longest word in dictionary that is a subsequence of a given string","text":"Description Given a string S and a set of words D , find the longest word in D that is a subsequence of S . Word W is a subsequence of S if some number of characters, possibly zero, can be deleted from S to form W , without reordering the remaining characters. Note: D can appear in any format (list, hash table, prefix tree, etc. For example, given the input of S = \"abppplee\" and D = {\"able\", \"ale\", \"apple\", \"bale\", \"kangaroo\"} the correct output would be \"apple\" . The words \"able\" and \"ale\" are both subsequences of S, but they are shorter than \"apple\". The word \"bale\" is not a subsequence of S because even though S has all the right letters, they are not in the right order. The word \"kangaroo\" is the longest word in D, but it isn't a subsequence of S. Solution Check each dictionary word using a greedy algorithm 一个比较容易想到的方法就是将字典中的单词与 S 逐个比较，这样最差时间复杂度为O(N W M)。 W 位字典中的单词数量， M 为单词的平均长度。虽然可以将字典中的单词按长度降序排列来减少一般状态下的运行时间，但不会减少最差时间复杂度。将 L 设为字典中所有单词字母长度之和，时间复杂度也可以表示为O(N*L)。 代码如下： s = 'abppplee' d = [ 'able' , 'ale' , 'apple' , 'bale' , 'kangaroo' ] def compare ( s , w ): i = 0 for character in w : while i < len ( s ): if character == s [ i ]: i += 1 break i += 1 else : break else : return True return False def solution ( s , d ): for w in sorted ( d , key = len , reverse = True ): if compare ( s , w ): return w print ( solution ( s , d )) Improving the greedy approach 我们也可以对 S 做一些预处理操作，记录其中字母的出现： S = \"abppplee\" a -> [0] b -> [1] p -> [2, 3, 4] l -> [5] e -> [6, 7] 这一操作的的时间复杂度为O(n)。在我们查找时，对于一个字典中的单词 w ，首先判断它的字母 X 是否在上述数据结构中，之后再去二分查找到数据结构中的满足：最小的index大于 X 的 Y 字母，其中 Y 为 X 的下一个字母。这样处理的时间复杂度为O(N + L * logN)。 Google提供的代码如下： import collections import sys def find_longest_word_in_string ( letters , words ): letter_positions = collections . defaultdict ( list ) # For each letter in 'letters', collect all the indices at which it appears. # O(#letters) space and speed. for index , letter in enumerate ( letters ): letter_positions [ letter ] . append ( index ) # For words, in descending order by length... # Bails out early on first matched word, and within word on # impossible letter/position combinations, but worst case is # O(#words # avg-len) * O(#letters / 26) time; constant space. # With some work, could be O(#W * avg-len) * log2(#letters/26) # But since binary search has more overhead # than simple iteration, log2(#letters) is about as # expensive as simple iterations as long as # the length of the arrays for each letter is # \"small\". If letters are randomly present in the # search string, the log2 is about equal in speed to simple traversal # up to lengths of a few hundred characters. for word in sorted ( words , key = lambda w : len ( w ), reverse = True ): pos = 0 for letter in word : if letter not in letter_positions : break # Find any remaining valid positions in search string where this # letter appears. It would be better to do this with binary search, # but this is very Python-ic. possible_positions = [ p for p in letter_positions [ letter ] if p >= pos ] if not possible_positions : break pos = possible_positions [ 0 ] + 1 else : # We didn't break out of the loop, so all letters have valid positions return word if __name__ == '__main__' : print subdict ( sys . argv [ 1 ], sys . argv [ 2 :]) An optimal O(N + L) approach for any alphabet 首先是一个基于上述算法的，适用于 S 的长度不长的情况的优化方案。我们将 p -> [2, 3, 4] 拓展成 p -> [2, 2, 3, 4, -1, -1, -1, -1] ，其中列表的每个元素对应 S 中该位置后出现该列表key的字母的序号（包括该位置的序号）。如果不存在就以 -1 表示。这样我们就不用二分法来查找列表，但随之带来的问题是：算法的实际复杂度变为O(N*A + L)， A 为 S 的字母集合的长度，并且消耗O（NA）的空间。因此当 S 过长时并非一个很好的优化。 除此之外，我们还可以通过同时遍历所有 D 中的单词来压榨我们的时间复杂度。字典中的每个单词放入(w,i)元组。其中w为单词本身，i记录了序号为i的字母已经达成匹配。这样我们就可以形成类似这样的数据结构： D = {\"able\", \"ale\", \"apple\", \"bale\", \"kangaroo\"} a -> [(\"able\", 0), (\"ale\", 0), (\"apple\", 0)] b -> [(\"bale\", 0)] k -> [(\"kangaroo\", 0)] 我们遍历 S 中的每个元素时，将所有该字母key对应列表里的元组 t 的i增加1，将这些元素移到 t.w[t.i] 对应的键下。当某个 t 的i等于w的长度时它就是一个符合条件的单词，将它移至一个结果列表。最终我们找出这个列表中的最长单词。这么做的时间复杂度为O(W + N + W' * logW')，其中 W' 为结果集中的单词数量，最差等于 W ，使得其复杂度非常接近理论最优的O(N + L)。代码如下： s = 'abppplee' d = [ 'able' , 'ale' , 'apple' , 'bale' , 'kangaroo' ] from collections import defaultdict Alphabets = defaultdict ( list ) for word in d : Alphabets [ word [ 0 ]] . append ([ word , 0 ]) def solution ( s , d ): result = '' for character in s : alphabet_list = Alphabets [ character ] for i in reversed ( range ( len ( alphabet_list ))): temp = alphabet_list . pop ( i ) temp [ 1 ] += 1 if len ( temp [ 0 ]) == temp [ 1 ]: if temp [ 1 ] >= len ( result ): result = temp [ 0 ] else : Alphabets [ temp [ 0 ][ temp [ 1 ]]] . append ( temp ) return result print ( solution ( s , d )) Additional","tags":"Python","url":"find-longest-word-in-dictionary-that-is-a-subsequence-of-a-given-string.html"},{"title":"Pyjion、python debug、pandas优化笔记","text":"Description Brett Cannon和Dino Viehland在 Pycon2016 的 Pyjion: who doesn't want faster for free? 演讲中介绍了Microsoft为cpython提供JIT的c++项目 Pyjion 。之后我也在talkpython的往期访谈中找到与Brett Cannon的 Pyjion 相关的访谈，其中的很多看法让我大受脾益。 Elizaveta Shashkova在 Pycon2017 的 Debugging in Python 3 6 Better, Faster, Stronger 演讲非常令人印象深刻。她在开发pycharm的JetBrains公司工作，而pycharm因为它优秀的debug功能成为了我最喜欢的IDE，没有之一。虽然俄罗斯老姐有一点口音，但并不影响这是一次让人感到Bingo的演讲。 同样在 Pycon2017 上，美丽的Sofia Heisler告诉了我们 No More Sad Pandas Optimizing Pandas Code for Speed and Efficiency 。这同样也是一次既实用而又让人能学到很多新知的讲解，非常高兴能有很多优秀的女性开发者参与到pycon中来。 Pyjion Brett Cannon介绍了各种python实现。 IronPython 和 Jython 分别是python基于C#和Java的实现，这样它们就可以兼容.Net和Java的应用了。 PyPy 是我之前比较关注的一个实现。它主要有两部分组成，一是它有一组为编程语言定制JIT的工具，你不仅可以为python来实现JIT，也可以用Rpython为其他语言写一个JIT的实现。二是刚刚提到的Rpython，虽然 PyPy 是由python编写的，但这里的python实际上是python的超集Rpython。Rpython是静态类型版的python，因此能编译成c代码提高运行的效率。 IronPython 、 Jython 和 PyPy 有一个共同的问题就是无法有效的兼容python的c api，致使使用者无法利用很多优秀的c库。 IronPython 和 Jython 由于它们并非是c语言实现具有如上兼容性问题非常容易理解， PyPy 则是由于使用CFFI模块的原因对c api只有部分的支持。像 NumPy 这种模块 PyPy 只能开个新的项目重写，这也是 PyPy 作为最快的Python实现而得不到科学计算社区广泛应用的原因所在。 Pyston 是Dropbox赞助的项目，它的目标是用JIT（LLVM JIT）提高python运行速度的同时尽可能地兼容python的c extension，因此它冲用了大部分 CPython 的代码。但比较遗憾的是 Pyston 现在只支持python2.7版本。 而 Pyjion 存在的意义则是直接为 Cpython 提供JIT的同时兼容更多的c extension。它由Dino发起，使用c++编写，单向支持python3版本。现在JIT是基于coreclr实现的，但在演讲中他们也提到这种实现也是可以作为一个后台系统更换的。 Brett Cannon也提到，python社区推广python3的关键是提高python3的速度，因此有很多核心开发者在从事这方面的工作， Pyjion 也是其中之一。社区在建立一个对象的缓存系统：通过判断对象的版本来自省对象是否被改变，当未改变对象在缓存中时我们就不用对命名空间进行层层筛选来获取对象了。 另一个至关重要的优化就是之前一篇笔记中提到的可能会在python3.7中实装的调用函数速度的优化。调用函数瓶颈的产生是由于python的多种入参形式（位置参数，关键字参数， args， * kwargs，python3中新添加的只允许关键字的参数，函数闭包）。这些入参形式是的了python在具备动态性的同时不丧失太多功能，但在构建参数列表时则会对系统产生极大地符合。Yuri针对这种情况开开发了新的加载实例方法和调用函数的字节码。 Debug Pycharm中的debug功能是基于 sys.settrace 函数完成的。而设置断点的功能则是在断点前插入一个字节码级别的监听用户输入的死循环。在加入新的字节码之后，还需要更新原有的变量和字节码的偏移量。当我们 dis 如下函数时： def maximun ( a , b ): if a > b : return a else : return b 将得到这些字节码： 2 0 LOAD_FAST 0 (a) 2 LOAD_FAST 1 (b) 4 COMPARE_OP 4 (>) 6 POP_JUMP_IF_FALSE 12 3 8 LOAD_FAST 0 (a) 10 RETURN_VALUE 5 >> 12 LOAD_FAST 1 (b) 14 RETURN_VALUE 16 LOAD_CONST 0 (None) 18 RETURN_VALUE 我们想要在 return a 语句这一行打上断点时，插入了类似这样的函数： def _stop_at_break (): # a lot of code here def breakpoint (): _stop_at_break () 字节码为： 0 LOAD_GLOBAL 0 (_stop_at_break) 2 CALL_FUNCTION 0 4 POP_TOP 6 LOAD_CONST 0 (None) 8 RETURN_VALUE 在python3.6之前，由于Pycharm使用的是 sys.settrace ，运行程序时每一行都会触发一次trace函数，因此调试时的运行时间将会增加25倍。Debug使得时间敏感的程序失去准确性。好在python3.6中部署了PEP523中的frame evaluation api。PEP523中有两个主要的内容: Handle evaluation of frames Add a new field to code objects 为了更好的让我们理解，Elizaveta Shashkova给了我们c code的python版示例： def frame_eval ( frame , exc ): func_name = frame . f_code . co_name line_number = frame . f_lineno print ( line_number , func_name ) return _PyEval_EvalFrameDefault ( frame , exc ) def set_frame_eval (): state = PyThreadState_Get () state . interp . eval_frame = frame_eval 这样我们就可以调用 set_frame_eval 来最终frame了。它只在每次进入frame时候触发，能大大减少debug运行时候的效率。但是当出现频繁调用函数致使进入frame的次数过多的时候，我们的debug运行效率将会退化到 sys.settrace 的水平。 这时候PEP523中的第二点就可以为我们所用了。这个特性拓展了 PyCodeObject 的结构，增加了 co_extra 属性。利用这个属性我们可以以不插入 breakpoint 函数的方式来标记代码。示例代码如下： def frame_eval ( frame , exc ): flag = _PyCode_GetExtra ( frame . f_code , index ) if flag == NO_BREAKS_IN_FRAME : return _PyEval_EvalFrameDefault ( frame , exc ) # check for breakpoints 最终的效率提升情况大致如下： Pandas Sofia Heisler测试的函数如下,测试数据文件 在此 ： def haversine ( lat1 , lon1 , lat2 , lon2 ): miles_constant = 3959 lat1 , lon1 , lat2 , lon2 = map ( np . deg2rad , [ lat1 , lon1 , lat2 , lon2 ]) dlat = lat2 - lat1 dlon = lon2 - lon1 a = np . sin ( dlat / 2 ) ** 2 + np . cos ( lat1 ) * np . cos ( lat2 ) * np . sin ( dlon / 2 ) ** 2 c = 2 * np . arcsin ( np . sqrt ( a )) mi = miles_constant * c return mi 我们利用利用Jupyter notebook的 %%timeit 魔术方法及 line_profiler 来跑分。首先我们用循环的方式来遍历整个df: %% timeit ### Haversine applied on rows via iteration haversine_series = [] for index , row in df . iterrows (): haversine_series . append ( haversine ( 40.671 , - 73.985 , \\ row [ 'latitude' ], row [ 'longitude' ])) df [ 'distance' ] = haversine_series 得到的结果是 197 ms ± 6.65 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) 。 之后是apply方法： % timeit df [ 'distance' ] = \\ df . apply ( lambda row : haversine ( 40.671 , - 73.985 , \\ row [ 'latitude' ], row [ 'longitude' ]), axis = 1 ) 得到的结果是 78.1 ms ± 6.65 ms per loop (mean ± std. dev. of 7 runs, 10 loop each) 。如果用 line_profiler 来进行分析，就会发现性能瓶颈来自于频繁调用函数的第三行和第六行。 接下来我们将数据向量化。在Pandas中的向量化是如此的简单： ### Vectorized implementation of Haversine applied on Pandas series % timeit df [ 'distance' ] = haversine ( 40.671 , - 73.985 , \\ df [ 'latitude' ], df [ 'longitude' ]) 跑分的结果是 2.21 ms ± 230 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) 。调用 line_profiler 后发现函数第三行的执行频率下降到了和其他语句同一数量级（529)，第六行也从一万六千多次下降到了三千五百多次。但这还不是最优的结果。 之后我们将输入由pd.Series变为np.ndarray，减少了Pandas Series索引及检查数据类型等开销: ### Vectorized implementation of Haversine applied on NumPy arrays % timeit df [ 'distance' ] = haversine ( 40.671 , - 73.985 , \\ df [ 'latitude' ] . values , df [ 'longitude' ] . values ) 得到的结果是 370 µs ± 18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each) 。 Sofia Heisler最后的实验是用Cython来优化函数本身的运算速度。因为如果有些原因我们不能将数据向量化亦或者向量化无法表达出原有的业务逻辑，那么我们就必须迭代我们的数据集了。为了提高效率我们将原函数写成这样然后用 apply 迭代： %% cython - a ### Haversine cythonized from libc.math cimport sin , cos , acos , asin , sqrt cdef deg2rad_cy ( float deg ): cdef float rad rad = 0.01745329252 * deg return rad cpdef haversine_cy_dtyped ( float lat1 , float lon1 , float lat2 , float lon2 ): cdef : float dlon float dlat float a float c float mi lat1 , lon1 , lat2 , lon2 = map ( deg2rad_cy , [ lat1 , lon1 , lat2 , lon2 ]) dlat = lat2 - lat1 dlon = lon2 - lon1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * asin ( sqrt ( a )) mi = 3959 * c return mi % timeit df [ 'distance' ] = \\ df . apply ( lambda row : haversine_cy_dtyped ( 40.671 , - 73.985 , \\ row [ 'latitude' ], row [ 'longitude' ]), axis = 1 ) 尽管如此，跑分的结果还是不如我们的向量化 51.1 ms ± 2.74 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) 。 最终的实验结果如下： Additional","tags":"Python","url":"pyjion-python-debug-pandasyou-hua-bi-ji.html"},{"title":"Instagram python migration笔记","text":"Description Lisa Guo和Hui Ding在 Pycon2017 的 keynote 分享了Instagram成功从python2.7、Django1.3升级到python3.6、Django1.8的经历。 在Talk Python fm之前的episode中，我惊喜地发现了有一期Michael Kennedy邀请到了 David Beazley 。在哪一期节目中他们畅谈了python concurrency相关及David在Github上的async项目 Curio 。 Instagram 很难想象Instagram能在保持产品功能高速迭代的同时完成从python2.7版本到3.5版本的跨越。他们在版本更新途中在代码层面遇到的主要有以下几类问题。 Unicode Python3最显著的改动就是严格化了UNICODE/STR/BYTE的转换。Instagram编写了一些功能函数（ ensure_binary , ensure_str , ensure_text ）来确保之前代码里的输入和输出符合python3的标准。 Data format incompatible 在Instagram中经常用到了 pickle 模块。该模块在python2和3中的区别是：python3的pickle的协议最高版本提升到了4。他们发现即使将协议版本hardcode到python2里的2，由于版本2和3的同时存在，他们之间的相互序列化的转化也会存在问题。因此他们将不同版本的pickle结果隔离，做到自取自拿。 Iterator Python3中的许多函数的返回值被修改成了迭代器（ map , filter , dict.item ），迭代器使得Instagram中那些会遍历两遍这些函数返回结果的代码全部失效。这其实非常难以调试，因此他们首先将所有该类函数的返回值用 list 方法还原成列表，然后将优化只迭代一遍的实例。 Dictionary ordering 之前的一篇note 也讲过，由于python字典的版本改动，字典中的返回值将大大不同。 testdict = { 'a' : 1 , 'b' : 2 , 'c' : 3 } json . dumps ( testdict ) 各版本的返回结果大致为： python2: {'a':1, 'c':2, 'b':3} (hardcode hashcode) python3.5.1: (random) python3.6：{'a':1, 'b':2, 'c':3} (keep order) 为了追求兼容性，他们对 json.dumps 函数加了参数： json . dumps ( testdict , sort_keys = True ) 另外他们也提到了以下的一个小的Unicode的改动将Instagram的性能提升了12%： if uwsgi . opt . get ( 'optimize_men' , None ) == 'True' optimize_men () to if uwsgi . opt . get ( 'optimize_men' , None ) == b 'True' optimize_men () 未来他们计划使用了python3完善好的特性type hints和mypy来提高他们codebase的健壮性，使用asyncio库用异步来处理之前的一些线程操作。 Curio David Beazley一直是一位我非常喜欢的python授业者，同时也是python cookbook的作者。他在这次访谈中提到了在python中实现async的另一种思路，也就是他写 Curio 库的目的。 在以教授python为业之前，他是教计算机科学系操作系统课程的教授。在课上他会告诉学生们如何用C语言来写操作系统的内核，内核做的事情主要就是多任务管理和IO，这和 asyncio 库所做的非常相似。既然 asyncio 库能将 callback , futures , coroutine 神奇地组合成一个奇妙的任务管理系统，为什么我们不能模仿系统内核也去实现一个任务管理系统呢？本身async编程也是一个焕发第二春的旧想法。 David也提到，由于python3.5中 async/await 关键字的加入，他之前的许多演讲和教程都有些过时了。这使他觉得，我们或许不该关注aync底层到底是如何实现的：无论是用 callback 还是用其他技术，而是关注我们该如何利用async编程或者说是async应该在的的领域。 可能现在async最大的问题就是它的传染性，一旦你代码中的一部分变成async，拿它所对应的整个代码生态链也必须是async的，否则小小的一段同步代码就会阻塞整个程序。虽然没有特别夸张，但是对于线程编程来说，这一点确实是值得考虑的。虽然asyncio库提供了一个函数作为对async中线程的支持，但是并没有将线程纳入 eventloop 处理的范畴之中， coroutine 也不是线程安全的。David的设想就是在不远的将来能将线程和async统一结合起来，使得 eventloop 能同时兼任地处理协程和线程，这样对async的未来定是大有裨益。事实上David在 Curio 中已经完成了一个统一队列的组件（universal queue object）作为线程和协程的交流媒介。 Additional","tags":"Python","url":"instagram-python-migrationbi-ji.html"},{"title":"Python async，dictionary，machine learning moudles笔记","text":"Description Jesse Jiryu Davis在 Pycon2014 的演讲 A. Jesse Jiryu Davis: What Is Async, How Does It Work, And When Should I Use It? 分析了为什么以及在什么情况下要用异步的方式处理io。之前在 300line 里，他和Guido合著的async crawler部分里就介绍了使用协程相对线程的优势：在python里线程需要50K的内存，而协程只需要3K。在该演讲中，他进一步阐述了协程的优势及应用场景。 Raymond Hettinger在 Pycon2017 的演讲 Modern Python Dictionaries A confluence of a dozen great ideas 和Brandon Rhodes的 The Dictionary Even Mightier 的主题是python字典在3.6版本的极大改进：减少20%-30%的内存占用及保持元素的插入顺序。 TalkPython 中Pete Garcin的访谈 Top 10 machine learning libraries 介绍了目前最流行的十个机器学习库，谈论了它们的异同以及初学者的学习路径。 Async Jesse举了纽约三种餐厅的例子作为三种不同场景。 第一个是三明治商店。顾客到柜台排队，厨师接到订单就开始制作三明治直到完成三明治。这个例子描述的是cpu密集型服务的机制，这里没有也无法使用异步机制，整个服务的吞吐量受到计算能力的限制。 第二个是披萨店。顾客点单后，厨师需要将制作好的披萨用微波炉加热后交给顾客。由于需要等待披萨的加热，因此就有了异步操作的必要性。这种服务的吞吐量受到内存的限制，服务器也需要后台来处理pending的请求。 第三个是一种寿司店。在这里顾客的需求由服务员告知厨房，同时厨房完成的寿司也需要服务器送到客户面前。现实中这样服务的例子就是谷歌邮箱服务：当客户登录谷歌邮箱之后并不会做太多动作，当客户收到邮件的这一事件发生的时候服务器才会将数据推到客户的面前。这就致使了大量长连接的产生。这些链接大部分的时间处于空闲状态，如果对每个链接都创建一个线程的话很快就会消耗完系统的内存。而异步正是为了最小化每个链接消耗的资源而诞生的。 协程相对线程另一个区别与优点就是，当我们进行多线程编程时要时时注意竞态，导致我们不得不用锁来控制共享资源。这是由于线程是程序员并没有对线程的完全控制权导致的，我们并不能知道线程什么时候切换，什么时候运行和阻塞——我们将这些都交给操作系统来完成了。而协程则只会在yield处暂停和接受输入，我们完全可以控制整个异步过程。 在演讲的最后，Jesse告诉了我们哪类服务适合async而哪类不适合： 他认为第二和第三种类型的服务是适合async的服务。这里也也指出了async的不足之处，async要求服务处理时至上而下每一部分都是异步的。因此我们的DB driver也需要支持异步，否则它将会阻塞整个程序。还有一点，async程序与线程编程将会非常不同，因此当你要实现一个异步服务时，一个async专家是必不可少的。 Dictionary in 3.6 Brandon Rhodes在他的演讲中提到了十几年来python开发者为了使字典越来越人性化所做的努力： - 在python2.6之前字典没有类似列表推导式的功能，python在3中加入了字典推导式并且回推给了2.7，使得字典不再是一个推导式的特例 - 字典的 keys() 等函数变成了生成器。为了和以前的借口，还实现了 __contains__ , __sub__ , __and__ , __xor__ , isdisjoint , __iter__ 方法。这点也得益于python协议式的设计。 - 内存共享。同一个类的多个实例在内存中共享相同的 hashcode 和 key 的储存，使得python运行时刻的字典内存占用减少了10%-20%。字典创建在一个类的第一个实例 __init__ 时，因此我们建议在类的 __init__ 中初始化全部可能将要用到的实例变量，否则新的变量被添加时候将会用之前机制的变量字典，从而导致无法享受内存共享的优势。 - python为了防止因为hash conflict而引起的DOS攻击，在3.3版本中将hash方法的因子设置为一个随机值，因此我们每次从resize后的字典中取值时将会的到不同顺序的数值。 - SipHash是python3.4-3.6版本中对随机hash的替代。 - 受数据库及索引的启发，Raymond Hettinger在python3.6版本中部署了新的字典机制。原先的字典机制将会使字典哈希表保持1/3的空间来减少hash conflit的产生。这样每个空闲键值对将会产生24比特的内存浪费。新的机制用一个索引数组机制去除了这些空间的浪费并且使得字典中的键值对在字典resize后也能保持键值对的原本输入顺序。这个机制相较于3.5版本能减少字典20%至25%的内存使用。 - 现在每个字典会有个版本，这样我们处理当相同版本的字典时就不用从头遍历每一个字典了。 dit = { - 1 : 'a' } for i in range ( 27 , 20 , - 1 ): dit [ i ] = i print ( list ( dit . keys ())) for i in range ( 6 , 20 ): dit [ i ] = i print ( list ( dit . keys ())) 以上这段代码在python2.7中由于resize可能会得到以下的结果： [ 21 , 22 , 23 , 24 , 25 , 26 , 27 , - 1 ] [ 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , - 1 ] 而在pytho3.6中则能保持元素录入的顺序： [ - 1 , 27 , 26 , 25 , 24 , 23 , 22 , 21 ] [ - 1 , 27 , 26 , 25 , 24 , 23 , 22 , 21 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 ] Machine learning libraries 在这个谈话中，Michael Kennedy和Pete Garcin谈论了他们认为的python最好的十个库。 - Numpy 和 Scipy 是一切模块的基础。 - Scikit-learn 是一个较早的模块，是 Scipy 家族的一部分。它提供给了我们一些常用的机器学习算法和分类，聚类，回归，模型工具。它非常简明直接，但缺少对GPU运算的支持。 - Keras 是一个比较新的high-level模块，可以 Theano , Tensorflow , CNTK 中的任意一个模块作为后台。它的设计目标是机器学期程序的快速开发，因此具有极高的易用性。 - Tensorflow 是现在热度最高的机器学习模块，它能充分利用GPU的矩阵及并行计算优势。而从Google要推出TPU这一举措来看，Google也正在大力推广这一模块。 - Theano 比较老迈，也是同 Tensorflow 一样的low-level的机器学习模块。由于它的核心开发都跑去谷歌了，所以它和 Tensorflow 其实非常相像。 - 除了机器学习算法，为了获得干净的数据，我们也需要 Pandas 来帮助我们处理矩阵数据。它也是 Scipy 家族的一部分。 - Caffe 和 Caffe2 是由Facebook在背后支持的机器视觉项目。它们针对互联网及移动部署做了相对的优化。 - Jupyter 改变了数据科学家们交流和发表研究成果的方式。 - CNTK 是微软最新的机器学习模块，擅长low-level的计算，如有向图。 - NLTK 是一个相对成熟自然语言处理模块，擅长文本分析和处理。 Pete Garcin最后向初学者推荐了 Keras 作为了解机器学习的入口。 Additional","tags":"Python","url":"python-asyncdictionarymachine-learning-moudlesbi-ji.html"},{"title":"Python yield tutorial markdown","text":"Description 本文是我的 jupyter notebook 的markdown版本。 An introduction to Python yield WHAT A function and a generater def fun (): return 'fun' def gen (): yield 'gen' fun () 'fun' gen () <generator object gen at 0x0000000007503780> A range() like example for ... in range() loop is used quite often when iterating objects in python . In below example, we made a generator to mock range() function. def counter ( top ): n = 0 while n < top : yield n n += 1 for i in counter ( 10 ): print ( i ) 0 1 2 3 4 5 6 7 8 9 Under the covers Generator object runs in response to next() or send() def counter_sample ( top ): n = 0 while n < top : print ( '-> before yield' ) yield n print ( '-> after yield' ) n += 1 c = counter_sample ( 5 ) print ( next ( c )) -> before yield 0 StopIteration raised when function returns print ( c . send ( None )) -> after yield -> before yield 1 Create coroutine with yield You can send data to a coroutine. def generator (): item = yield print ( 'item = {}' . format ( item )) yield 10 Prime the coroutine g = generator () g . send ( None ) Send data value = g . send ( 20 ) item = 20 print ( 'value = {}' . format ( value )) value = 10 A coroutine which receives data as well as produces data def averager (): total , count , average = 0.0 , 0 , None while True : term = yield average total += term count += 1 average = total / count avg = averager () avg . send ( None ) avg . send ( 10 ) 10.0 avg . send ( 20 ) 15.0 avg . send ( 30 ) 20.0 Three features of coroutines: - When a coroutine run into yield , it will suspend - A caller should schedule the coroutine when it suspended - When a coroutine suspended, it will return control to the caller WHY A tornado example import time import tornado.ioloop import tornado.web import tornado.gen class BadStupidHandler ( tornado . web . RequestHandler ): def get ( self ): for i in range ( 20 ): self . write ( '{}<br>' . format ( i )) self . flush () time . sleep ( 0.5 ) class GoodStupidHandler ( tornado . web . RequestHandler ): @tornado.gen.coroutine def get ( self ): for i in range ( 20 ): self . write ( '{}<br>' . format ( i )) self . flush () yield tornado . gen . sleep ( 0.5 ) app = tornado . web . Application ([ ( r '/bad' , BadStupidHandler ), ( r '/good' , GoodStupidHandler ) ]) Refer to documentation Frequently Asked Questions . HOW Use coroutine to simplfy your context manager A context manager is to change: try : f = open ( 'some.txt' ) print ( f . readline ()) # do something with f finally : f . close () coroutine To: with open ( 'some.txt' ) as f : print ( f . readline ()) # do something with f coroutine You can define your own context manager with a class implements __enter__ and __exit__ method class Mirror : def __init__ ( self , num ): self . num = num def __enter__ ( self ): import sys def reverse_write ( text ): self . original_write ( text [:: - 1 ]) self . original_write = sys . stdout . write sys . stdout . write = reverse_write return 'This is mirror {}' . format ( self . num ) def __exit__ ( self , exc_type , exc_value , traceback ): import sys sys . stdout . write = self . original_write with Mirror ( 1000 ) as first_string : print ( first_string ) print ( 123456789 ) print ( 'Out there' ) 0001 rorrim si sihT 987654321 Out there Use decorator contextlib.contextmanager and generator to simplfy your own context manager import contextlib @contextlib.contextmanager def Mirror_new ( num ): import sys def reverse_write ( text ): original_write ( text [:: - 1 ]) original_write = sys . stdout . write sys . stdout . write = reverse_write yield 'This is mirror {}' . format ( num ) sys . stdout . write = original_write with Mirror_new ( 1000 ) as first_string : print ( first_string ) print ( 123456789 ) print ( 'Out there' ) 0001 rorrim si sihT 987654321 Out there How does it work? We define a warpper class to proxy our generator: class GeneratorCM : def __init__ ( self , func ): self . _func = func def __call__ ( self , * args , ** kwargs ): self . _gen = self . _func ( * args , ** kwargs ) return self def __enter__ ( self ): return self . _gen . send ( None ) def __exit__ ( self , exc_type , exc_value , traceback ): try : self . _gen . send ( None ) except StopIteration : return True @GeneratorCM def Mirror_custom ( num ): import sys original_write = sys . stdout . write def reverse_write ( text ): original_write ( text [:: - 1 ]) sys . stdout . write = reverse_write yield 'This is mirror {}' . format ( num ) sys . stdout . write = original_write with Mirror_custom ( 1000 ) as first_string : print ( first_string ) print ( 123456789 ) print ( 'Out there' ) 0001 rorrim si sihT 987654321 Out there GeneratorCM with full try catch is listed below: class GeneratorCM : def __init__ ( self , func ): self . _func = func def __call__ ( self , * args , ** kwargs ): self . _gen = self . _func ( * args , ** kwargs ) return self def __enter__ ( self ): return self . _gen . send ( None ) def __exit__ ( self , exc_type , exc_value , traceback ): try : if exc_type is None : next ( self . _gen ) else : self . _gen . throw ( exc_type , exc_value , traceback ) raise RuntimeError ( \"Generator didn't stop\" ) except StopIteration : return True except : if sys . exc_info ()[ 1 ] is not exc_value : raise Inlined yield Following statement is very common in tornado framwork from tornado import gen @gen.coroutine def fetch_coroutine ( url ): http_client = AsyncHTTPClient () response = yield http_client . fetch ( url ) raise gen . Return ( response . body ) How does it work? First,define a slow function to mock http.fetch : import time import random def func ( x , y ): sleep_time = random . random () * 3 time . sleep ( sleep_time ) print ( 'sleep for {} seconds' . format ( sleep_time )) return x + y We need to our function work like this: from concurrent.futures import ThreadPoolExecutor , Future pool = ThreadPoolExecutor ( max_workers = 8 ) @inlined_future def do_func(x, y): result = yield pool.submit(func, x, y) print('Got:', result) Inspired by @contextmanager： import wrapt class Task : def __init__ ( self , gen ): self . _gen = gen initive = Future () initive . set_result ( None ) self . step ( initive ) def step ( self , future ): try : next_future = self . _gen . send ( future . result ()) except StopIteration as exc : if exc . value is not None : raise exc else : next_future . add_done_callback ( self . step ) @wrapt.decorator def inlined_future ( wrapped , instance , args , kwargs ): Task ( wrapped ( * args , ** kwargs )) @inlined_future def do_func ( x , y ): result = yield pool . submit ( func , x , y ) print ( 'Got:' , result ) def do_func_slow ( x , y ): result = func ( x , y ) print ( 'Got:' , result ) for i in range ( 5 ): do_func_slow ( i , i ) sleep for 2.4873064812324777 seconds Got: 0 sleep for 1.4432468827933995 seconds Got: 2 sleep for 1.9532320529696823 seconds Got: 4 sleep for 0.584825861467138 seconds Got: 6 sleep for 1.8091265383050155 seconds Got: 8 for i in range ( 5 ): do_func ( i , i ) Additional: How coroutine work import dis import inspect def gen_fn (): result = yield 1 print ( 'result of yield: {}' . format ( result )) result2 = yield 2 print ( 'result of 2nd yield: {}' . format ( result2 )) return 'done' def normal_fn (): return 1 a = gen_fn () a . send ( None ) 1 gen_fn <function __main__.gen_fn> normal_fn <function __main__.normal_fn> bool ( gen_fn . __code__ . co_flags & inspect . CO_GENERATOR ) True bin ( inspect . CO_GENERATOR ) '0b100000' bool ( normal_fn . __code__ . co_flags & inspect . CO_GENERATOR ) False gen1 = gen_fn () type ( gen1 ) generator gen1 . gi_code . co_name 'gen_fn' All generators from calls to gen_fn point to this same code. But each has its own stack frame. This stack frame is not on any actual stack, it sits in heap memory. gen2 = gen_fn () gen1 . gi_code is gen2 . gi_code True gen1 . gi_frame is gen2 . gi_frame False gen1 . send ( None ) gen1 . gi_frame . f_lasti 2 dis . dis ( gen1 ) 5 0 LOAD_CONST 1 (1) 2 YIELD_VALUE 4 STORE_FAST 0 (result) 6 6 LOAD_GLOBAL 0 (print) 8 LOAD_CONST 2 ('result of yield: {}') 10 LOAD_ATTR 1 (format) 12 LOAD_FAST 0 (result) 14 CALL_FUNCTION 1 16 CALL_FUNCTION 1 18 POP_TOP 7 20 LOAD_CONST 3 (2) 22 YIELD_VALUE 24 STORE_FAST 1 (result2) 8 26 LOAD_GLOBAL 0 (print) 28 LOAD_CONST 4 ('result of 2nd yield: {}') 30 LOAD_ATTR 1 (format) 32 LOAD_FAST 1 (result2) 34 CALL_FUNCTION 1 36 CALL_FUNCTION 1 38 POP_TOP 9 40 LOAD_CONST 5 ('done') 42 RETURN_VALUE gen1 . send ( 'hello' ) gen1 . gi_frame . f_lasti result of yield: hello 22 gen1 . gi_frame . f_locals {'result': 'hello'} gen1 . send ( 'world' ) result of 2nd yield: world --------------------------------------------------------------------------- StopIteration Traceback (most recent call last) <ipython-input-124-b432c08ca417> in <module>() ----> 1 gen1.send('world') StopIteration: done sleep for 0.9637312558255126 seconds Got: 8 sleep for 1.0164703671835906 seconds Got: 2 sleep for 1.0941822049442602 seconds Got: 4 sleep for 2.2731928774129755 seconds Got: 0 sleep for 2.4067928659249773 seconds Got: 6 Reference Generators: The Final Frontier Effective Python：Consider Coroutines to Run Many Functions Concurrently Python Cookbook 3: 不用递归实现访问者模式 500 line or less:A Web Crawler With asyncio Coroutines 一个简单的文本解析计算器","tags":"Python","url":"python-yield-tutorial-markdown.html"},{"title":"Python垃圾处理机制","text":"Description 本文环境为python3.6。与c中拥有的malloc和free不同，python中的内存是编译器自动完成，因此我们并不需要时刻关心内存的使用情况。在 Things you need to know about garbage collection in Python 一文中给我们详细介绍了python gc的具体实现及细节，本文主要参考该博客来对python垃圾处理机制做一些总结。 内存管理 在python中小于512k的对象是会在内存中缓存的，这就导致小于256的整形及一些短字符串在程序中实际共享了同一个内存地址。像这样，python为了提高对内存操作的效率及减少碎片，在通用内存分配器上假设了一个特殊的管理器———— PyMalloc 。在 cpython 中，内存模型被形容为大致如下： _____ ______ ______ ________ [ int ] [ dict ] [ list ] ... [ string ] Python core | +3 | <----- Object-specific memory -----> | <-- Non-object memory --> | _______________________________ | | [ Python's object allocator ] | | +2 | ####### Object memory ####### | <------ Internal buffers ------> | ______________________________________________________________ | [ Python's raw memory allocator (PyMem_ API) ] | +1 | <----- Python memory (under PyMem manager's control) ------> | | __________________________________________________________________ [ Underlying general-purpose allocator (ex: C library malloc) ] 0 | <------ Virtual memory allocated for the python process -------> | ========================================================================= _______________________________________________________________________ [ OS-specific Virtual Memory Manager (VMM) ] -1 | <--- Kernel dynamic storage allocation & management (page-based) ---> | __________________________________ __________________________________ [ ] [ ] -2 | <-- Physical memory: ROM/RAM --> | | <-- Secondary storage (swap) --> | python中占用内存较大的对象会被分配到标准的c内存分配器，小对象分配器则由三个级别的抽象构成 Arena 、 Pool 、 Block 。 Block Block（块）是固定大小（8-512k）的内存块。为了方便起见，这些块被分为64类： Request in bytes Size of allocated block size class idx 1-8 8 0 9-16 16 1 17-24 24 2 25-32 32 3 33-40 40 4 41-48 48 5 ... ... ... 505-512 512 63 Pool Pool（池）是相同大小Block（块）的集合。通常来说，池的大小等于内存页的大小。固定块的大小能减少内存碎片的产生————当一个对象被销毁的时候，内存管理器能轻松地将相同大小的对象装载入块中。 在cpython中，池被定义为如下结构： /* Pool for small blocks. */ struct pool_header { union { block * _padding ; uint count ; } ref ; /* number of allocated blocks */ block * freeblock ; /* pool's free list head */ struct pool_header * nextpool ; /* next pool of this size class */ struct pool_header * prevpool ; /* previous pool \"\" */ uint arenaindex ; /* index into arenas of base adr */ uint szidx ; /* block size class index */ uint nextoffset ; /* bytes to virgin block */ uint maxnextoffset ; /* largest valid nextoffset */ }; 池使用的是双向链表的结构， nextpool 和 prevpool 字段指向链表节点。 szidx 字段保存了上文提到的该池的大小类别的序号（size class index），而 ref.count 字段则储存了被占用了的块的数量。 arenaindex 储存了该池在所在的内存空间的序号。对于 freeblock 则是这么解释的： pool->freeblock points to the start of a singly-linked list of free blocks within the pool. When a block is freed, it's inserted at the front of its pool's freeblock list. Note that the available blocks in a pool are *not* linked all together when a pool is initialized. Instead only \"the first two\" (lowest addresses) blocks are set up, returning the first such block, and setting pool->freeblock to a one-block list holding the second such block. This is consistent with that pymalloc strives at all levels (arena, pool, and block) never to touch a piece of memory until it's actually needed. So long as a pool is in the used state, we're certain there *is* a block available for allocating, and pool->freeblock is not NULL. If pool->freeblock points to the end of the free list before we've carved the entire pool into blocks, that means we simply haven't yet gotten to one of the higher-address blocks. The offset from the pool_header to the start of \"the next\" virgin block is stored in the pool_header nextoffset member, and the largest value of nextoffset that makes sense is stored in the maxnextoffset member when a pool is initialized. All the blocks in a pool have been passed out at least once when and only when nextoffset > maxnextoffset. 大概就是说， freeblock 字段指向了一个单项链表，这个链表连接了该池中的一部分可用的块。之所以说是‘一部分'是因为在初始化池的时候系统只会给前两给内存块分配内存，这样使得只有当需要分配新内存的时候才会让池占用新的块空间，节省了内存的消耗。初始化池的时会进行的另一个操作是设置 maxnextoffset 字段，即内存指针最大偏移量。当需要拓展新的块空间时，通过 szidx 及 nextoffset 字段计算出新块所占用的内存地址区域并分配内存。最后，当 nextoffset 大于 maxnextoffset 时，整个池的就满了。 为了更有效地进行池的管理，python引入了 usedpools 数组来储存各个 size class 的块： 值得注意的是，块和池并不是直接分配内存，它们所分配的内存来自于其所在的Arena（内存空间）。 Arena Arena（内存空间）是由64个池组成的在堆上的256k的空间，同样也是双向链表。它的结构如下： struct arena_object { uintptr_t address ; block * pool_address ; uint nfreepools ; uint ntotalpools ; struct pool_header * freepools ; struct arena_object * nextarena ; struct arena_object * prevarena ; }; 其中 ntotalpools 和 nfreepools 储存了内存空间上可用池的信息。 freepools 指向一个可用池的链表。 python中的垃圾处理 python中的垃圾处理机制由两部分组成： 引用计数（ reference counting ） 分代垃圾收集器（ generational garbage collector ） 引用计数是我们无法染指只能了解的一部分，但它的机制也很简单，就是为每个对象维护一个引用计数，当这个引用计数落为0的时候立刻释放对象所占内存。它无法处理循环引用的情况，分代垃圾收集器即 gc 模块则是为了应对这种情况而产生。这两种技术相辅相成组成了python中的垃圾处理机制。 基于引用计数的垃圾收集机制是一种相对简单的机制。在一些其他语言中，有一些更现代的机制，如java中的可达性算法。这个算法的基本思路就是通过一系列的称为 GC Roots 的对象作为起始点，从这些节点开始向下搜索所有走过的路径作为引用链，当一个对象到 GC Roots 没有任何引用链相连时候，则证明此对象不可用。这一种算法也能够避免循环引用的产生。 引用计数 在python的 c api文档中 描述了cpython中引用计数的底层实现。 cpython中通过 Py_INCREF 和 Py_DECREF 两个宏来控制引用计数的增加和减少。对象析构器会触发 Py_DECREF 宏，该宏会检查对象的引用计数是否会被降为0————为0时则立刻释放该对象的内存，这就使得引用计数释放内存具有即时性。 你可以使用 sys.getrefcount 方法来取得某个对象的引用计数： import sys foo = [] ### 2 references, 1 from the foo var and 1 from getrefcount print ( sys . getrefcount ( foo )) def bar ( a ): # 4 references # from the foo var, function argument, getrefcount and Python's function stack print ( sys . getrefcount ( a )) bar ( foo ) ### 2 references, the function scope is destroyed print ( sys . getrefcount ( foo )) 分代垃圾收集器 引用计数这个简单的机制会带来许多问题，如无法解决循环引用、需要线程锁及效率低下。为了解决循环引用的问题， gc模块 在python 1.5版本中被加入。 由于循环引用只会在container容器类型中发生，所以 gc 模块并不会追踪python中所有的对象。我们可以使用 gc.is_tracked 函数来判断某个对象是否被追踪： >>> gc . is_tracked ( 0 ) False >>> gc . is_tracked ( \"a\" ) False >>> gc . is_tracked ([]) True >>> gc . is_tracked ({}) #这个字典为空，因此未被追踪 False >>> gc . is_tracked ({ \"a\" : 1 }) #这个字典所有元素都为原子类型，因此未被追踪 False >>> gc . is_tracked ({ \"a\" : []}) True 与引用计数机制的即时触发不同的是，为了保证性能 gc 并不是实时触发的。 首先要提到的是 gc 的 分代机制 。作为一个分代垃圾收集器，所有被 gc 追踪的对象被分为三代：新生代、中年代和老年代，较新代的对象将会被更频繁的处理。所有新对象会被界定为新生代，只有当某个对象在一次 gc 中存活下来时它才会作为一个年迈的对象被标记为更老的一代。分代机制在一定程度上优化了 gc 的性能。 当某一代加入的对象数量超过一个阈值时，就会触发 gc 处理这一代及更新代的对象。这一阈值可以使用 gc.get_threshold 方法获得，如标准的阈值为(700,10,10)分别对应新中老三代的阈值。值得一提的是，为了提升性能，对第三代的‘长寿'对象的收集（即全局垃圾收集）需要达到 一个特性的标准 ———— long_lived_pending / long_lived_total 的比例大于25%。 long_lived_total 为在最近一次全局 gc 中存活下来的对象的数量， long_lived_pending 为在所有非全局 gc 中存活下来的，现在处在老年代的对象的数量。 其次我们要探讨的是python中 找出引用循环的算法 。我们经常看到对该算法的描述为：找到系统的 根 对象，从该对象开始遍历所有被追踪的容器对象，这些可到达的对象是活着的；释放所有其他对象。然而因为我们无法完全找到拓展模块的 根 对象，这种传统的方式已经不能再当今版本的python中使用了，因此我们得采取一种新的处理引用循环的算法。我们只需要处理被追踪的容器对象，得益于这点，我们可以以较小的代价将所有被追踪的对象用双向链表串联起来（减少在任意位置插入或删除节点的代价），并做如下处理： 对链表中的每个对象，设置一个 gc_ref s字段使其等于该对象的引用计数值； 对于链表中的每个对象，找到它所引用的目标对象并减1该容器的 gc_refs 值； 所有 gc_refs 值大于1的对象是有被立案表外对象引用的对象，因此我们不能释放它们的内存，将它们移至另一个集合中去（更年迈的代）； 所有被这些转移的容器对象引用的链表中对象也不能够被释放，也将它们移到另一个集合中，对被它们引用的对象做相同的操作； 现在我们的链表里剩下对象就是被循环引用的的对象，将它们释放； 总结 尽管有分代垃圾收集器帮助我们处理循环引用的问题，但在我们的代码中我们还是要注意避免出现循环引用。因为如果出现大量循环引用的话，启动 gc 机制依然要耗费大量的资源。在这里原作者给大家的建议是使用python中的 weakref 模块，弱引用并不会增加引用计数，当它引用的对象不存在时它会返回None给调用方。 另外，引用计数是一个我们不能控制的机制，但分代垃圾收集机制确是可以用 gc 模块hack的。我们可以使用 gc.disable() 来关闭分代垃圾收集器，并且使用 gc.collect() 函数来手动触发垃圾收集。但很多从业人员并不提倡这一点。 Additional 参考文献： 1. Things you need to know about garbage collection in Python 2. Python internals: Memory management 3. Objects, Types and Reference Counts 4. Garbage Collector interface 5. Garbage Collection for Python","tags":"Python","url":"pythonla-ji-chu-li-ji-zhi.html"},{"title":"Replace asyncio with native coroutine","text":"Description 在David Beazley的 Topics of Interest(Python Asyncio) 演讲 中，他介绍了由于async在python中的种种历史遗留，asyncio库在 aync/await 语句加入之后已经不是一种最直接有效的python async实现。在写了一个简单程序替代asyncio库并与其及gevent库的效率做比较之后，我们发现了python3.5版本中引入的原生协程在处理异步方面具有非常高的效率。在演讲的最后他提出，可能我们需要的async并不是一个库而更像是一些api来供我们更有效地做底层的使用。 这次的演讲启迪了我许多，但碍于并没有相关的ppt和材料提供下载（有部分是David在台上光速Live coding的原因），我只能将其代码誊写下来，以供检验及研究，作为个人的读书笔记。 Core programing of async server 准备工作 本文测试环境为python3.6版本，Intel Core i5-6500,8G RAM，win10。 David用了以下程序来做async服务器的测试。该程序会发送10000个请求并计算服务器处理请求的速率： #benchmark.py import time from socket import socket , AF_INET , SOCK_STREAM , SOL_SOCKET , SO_REUSEADDR def benchmark ( addr , nmessages ): sock = socket ( AF_INET , SOCK_STREAM ) sock . connect ( addr ) start = time . time () for n in range ( nmessages ): sock . send ( b 'x' ) resp = sock . recv ( 10000 ) end = time . time () print ( nmessages / ( end - start ), 'message/sec' ) benchmark (( 'localhost' , 25000 ), 100000 ) asyncio服务器 首先我们用asyncio来搭建一个异步服务器，代码如下： #asyncio_server.py import asyncio from socket import socket , AF_INET , SOCK_STREAM , SOL_SOCKET , SO_REUSEADDR loop = asyncio . get_event_loop () async def echo_server ( address ): sock = socket ( AF_INET , SOCK_STREAM ) sock . setsockopt ( SOL_SOCKET , SO_REUSEADDR , 1 ) sock . bind ( address ) sock . listen ( 5 ) sock . setblocking ( False ) while True : client , addr = await loop . sock_accept ( sock ) print ( 'Connection from' , addr ) loop . create_task ( echo_handler ( client )) async def echo_handler ( client ): with client : while True : data = await loop . sock_recv ( client , 10000 ) if not data : break await loop . sock_sendall ( client , b 'Got:' + data ) print ( 'Connection closed' ) loop . create_task ( echo_server (( '' , 25000 ))) loop . run_forever () 让我们启动服务并运行 benchmark.py 做测试，得到的结果为 9660.489944352334 message/sec 。 原生协程实现eventloop 观察以上asyncio服务器的代码，不难发现其中不仅有asyncio的eventloop（底层用生成器协程和 yield from 实现）还有新引入的 aync/await 原生协程，是一种跨界组合的状态。那么能不能将eventloop也用原生协程也实现呢？David给出了一个简单的方案： #native_coroutine_asyncio.py from types import coroutine from collections import deque from selectors import DefaultSelector , EVENT_READ , EVENT_WRITE @coroutine def read_wait ( sock ): yield 'read_wait' , sock @coroutine def write_wait ( sock ): yield 'write_wait' , sock class Loop : def __init__ ( self ): self . ready = deque () self . selector = DefaultSelector () async def sock_recv ( self , sock , maxbytes ): await read_wait ( sock ) return sock . recv ( maxbytes ) async def sock_accept ( self , sock ): await read_wait ( sock ) return sock . accept () async def sock_sendall ( self , sock , data ): while data : try : nsent = sock . send ( data ) data = data [ nsent :] except BlockingIOError : await write_wait ( sock ) def create_task ( self , coro ): self . ready . append ( coro ) def run_forever ( self ): while True : while not self . ready : events = self . selector . select () for key , _ in events : self . ready . append ( key . data ) self . selector . unregister ( key . fileobj ) while self . ready : self . current_task = self . ready . popleft () try : op , * args = self . current_task . send ( None ) # run to the yield getattr ( self , op )( * args ) # sneaky method call except StopIteration : pass def read_wait ( self , sock ): self . selector . register ( sock , EVENT_READ , self . current_task ) def write_wait ( self , sock ): self . selector . register ( sock , EVENT_WRITE , self . current_task ) 这段代码里有些比较不规范的做法，比如getattr(self,op)(* args)语句用字符串来运行函数，但无伤大雅，我们的目的是检验将eventloop换成原生协程实现的话会怎样。 我们将asyncioo_server中的eventloop换成刚刚实现的版本,其他代码保持不变： import native_coroutine_asyncio as native loop = native . Loop () 运行 benchmark.py 发现每秒请求处理数变为了 19515.515222278125 message/sec ，效率提升了超过了100%。 gevent服务器 为了做平行测试，David又用了gevent这个底层由c实现的库来检测原生协程的效率究竟如何，代码如下： #gevent_server.py from gevent.server import StreamServer ### this handler will be run for each incoming connection in a deficated greenlet def echo ( socket , address ): print ( 'New connection from {}' . format ( address )) while True : data = socket . recv ( 100000 ) if not data : break socket . sendall ( b 'Got:' + data ) socket . close () if __name__ == '__main__' : server = StreamServer (( '0.0.0.0' , 25000 ), echo ) server . serve_forever () 在python3.6下它的每秒处理的请求数为 16807.0229190064 message/sec ，原生协程甚至比它还高一点。在David的线程测试的时候，他调用了python2.7的环境，使它和原生协程的效率是差不多。 结语 asyncio库其实有很大的历史包袱。python中的协程从单纯的yield生成器开始，经历了一个一个版本的变迁，生成器得到可以用 send 、 yield ， throw 与外界通信的功能成为了意义上的协程；之后python3.3版本中加入的 yield from 给予了生成器协程更多的便利与可能；为期三年的郁金香项目逐渐孵化出asyncio又结合了以上的这些协程生成器组件及futures、callback、polling在3.4版本给我们带来了新的async模块。但要演化出这么一个模块所经历的一切实在是太久太长，我们面临的问题是：经过了这么多版本和这么久的时间变迁后是否它是最好的呢？有没有一个一步到位的协程来让我们使用呢？ 在3.5版本中原生协程出现了，经过上面的实验我们可以初步判断它是一个更具效率的协程实现方案。这也让python有了新的优良特性，也让python在aync方向前进了一大步。 在 How the heck does async/await work in Python 3.5? 一文中详细的介绍了python async的历史： 受到 Icon语言 的启发，python2.2中生成器首次被 PEP255 - simple generators 引入。 而python2.5中yield语法的加入使得这一使用更少内存来迭代序列的想法更加实用。在这一版本中， PEP342 - coroutine via enhanced generators 使得python中的生成器不再试一点单纯的迭代器，被暂停的生成器也拥有了可以被send信息，与外界交互的能力。 生成器协程在相安无事了数个版本之后，终于在python3.3版本中 PEP380 - syntax for delegating to a subgenerator 增加了新的语法yield from简化了协程之间的管道式调用。同一个版本中Guido主导的asyncio库作为实验性发布，并且在python3.4中正式成为标准库的一员。 python3.5中协程迎来了新的纪元， PEP492 - Coroutine with async and await syntax 加入async def、async with、async for、await语法及相对应的底层协议。为了以示区分，用这些语法构成的协程称之为原生协程。原生协程不可以await一个非协程生成器，彻底将生成器与协程的界限划分开来。 Additional 源代码： 1. asyncio_server.py 2. native_coroutine_asyncio.py 3. gevnet_server.py 4. benchmark.py","tags":"Python","url":"replace-asyncio-with-native-coroutine.html"},{"title":"How to run PyPy","text":"Description PyPy 是目前速度最快的python解释器。先不说它惊人的用python来编写python解释器的理念，之前在planet python里看到一篇 PyPy 计划移除GIL的 文章 就已经让我惊为天人了。 听了David Beazley在2012年关于 PyPy 和 Rpython 的几个演讲之后( Additional 1,2,3)，就对这个项目起了浓厚的兴趣。 Download and install 可以在 https://pypy.org/download.html 下载到 PyPy 的最新版本。 PyPy 已经支持python2.7和3.5.3了，但基于python3的 PyPy 依旧是一个beta版本，并且声称比 PyPy 2的速度慢很多。因为我们选择安装了基于Ubuntu的64位 PyPy 。解压后，为了日后使用我们使用以下命令在 /usr/local/bin 下创建一个软连接,其中第一个目录为 PyPy 的安装目录： ln -s ~/Program/pypy2-v5.8.0-linux64/bin/pypy /usr/local/bin 现在我们可以用 pypy 命令进入 PyPy 的shell了。 Performance test David给了一段斐波那契数列的测试代码,其中 PyPy 是将 target 函数为入口函数的。经过测试，这个 PyPy 版本已经不需要这个入口函数了，也不需要向David在 Pycon 的分享中那样用 translate.py 先进行编译再执行c源码。（当然也可能是只有 Rpython 需要这么做。）本文所用的斐波那契数列的源码如下： #fib.py import sys def fib ( n ): if n < 2 : return n else : return fib ( n - 1 ) + fib ( n - 2 ) def main ( argv ): print ( fib ( int ( argv [ 1 ]))) return 0 if __name__ == '__main__' : main ( sys . argv ) 我们用python2.7，python3.6，pypy5.8这三个环境做性能测试： python2.7 >>time python fib.py 41 165580141 real 0m47.026s user 0m46.564s sys 0m0.024s python3.6 >>time python fib.py 41 165580141 real 1m9.771s user 1m8.992s sys 0m0.004s pypy5.8 >>time pypy fib.py 41 165580141 real 0m4.208s user 0m3.392s sys 0m0.736s 以上结果是在虚拟机里测试的，仅供参考。可以发现 PyPy 的运算速度比 Cpython 快了非常多，但python3比python2还慢蛮多的确实让我很气啊。当然，这只是一个纯cpu密集型程序的运算结果，根据 PyPy 的网站所说，得益于JIT以及Stackless， PyPy 的平均速度应该是 Cpython 的四倍左右。 值得一提的是，David在 Pycon 演示的那个先编译再运行的 PyPy 版本，运行的斐波那契数列的速度甚至超过了未优化的c，一秒钟就你完成该运算，但在这个版本的pypy中没有编译的过程并且似乎没有了Daivd提到的 Restrict 数据类型的限制。 Type restrict feature 为了说明 PyPy 的缺乏动态类型的支持，Daivd举了以下的例子： lst = [ 1 , 2 , 3 , 'Hello' ] for item in lst ： print ( lst ) class A ( object ): def __int__ ( self , x , y ): self . x = x self . y = y a = A ( 1 , 2 ) b = B ( 'Hello' , 'world' ) 这些代码是通不过当时版本 PyPy 的编译的，然而在现在版本中可以正常运行，这点着实让我惊讶。David提到的一个 PyPy 的独特处理方式，就是讲纯python代码和rpython代码分开编译，我想这大概是其中原因。 Additional Understanding RPython Low Level RPython Let's Talk About PyPy PyPy documentation","tags":"Python","url":"how-to-run-pypy.html"},{"title":"LeetCode - Integer Break","text":"Description Given a positive integer n, break it into the sum of at least two positive integers and maximize the product of those integers. Return the maximum product you can get. For example, given n = 2, return 1 (2 = 1 + 1); given n = 10, return 36 (10 = 3 + 3 + 4). Note: You may assume that n is not less than 2 and not larger than 58. Source link Best practice 经过枚举可以发现，输入元素最终的不可分单位为2和3（所给例子中的10最小可被分为3+3+2+2）。由此可见我们只需要在只产生这两种不可分元素的情况下尽可能贪婪的获取更多3就能使被break的元素积最大了。更深入发现，若欲使元素积最大，只会出现一个2和两个2元素的情况，因此我们设计的代码如下。 python version 1 class Solution ( object ): def integerBreak ( self , n ): \"\"\" :type n: int :rtype: int \"\"\" if n < 4 : return n - 1 if n % 3 == 0 : return 3 ** ( n // 3 ) if ( n - 2 ) % 3 == 0 : return 3 ** (( n - 2 ) // 3 ) * 2 if ( n - 4 ) % 3 == 0 : return 3 ** (( n - 4 ) // 3 ) * 2 ** 2 Mark: 32 ms 这是leetcode上的pop的答案，偏计算机思维。用比目标元素小的元素的生成结果来计算之后的结果，其实多了很多循环，但不知道为什么leetcode声称运行起来更快。 python version 2 class Solution ( object ): def integerBreak ( self , n ): \"\"\" :type n: int :rtype: int \"\"\" if n < 4 : return n - 1 res = [ 0 , 1 , 2 , 3 ] for i in xrange ( 4 , n + 1 ): res [ i % 4 ] = max ( res [( i - 2 ) % 4 ] * 2 , res [( i - 3 ) % 4 ] * 3 ) return res [ n % 4 ] Mark: 28 ms Additional","tags":"Python","url":"leetcode-integer-break.html"},{"title":"Python Descriptor Behavior II","text":"Description 在之前一篇 文章 中我们讨论了python描述符以及其行为，也做了一些总结。但个人能力有限，对描述符访问的优先规则解释的不是非常清楚。 近日看到一篇文章： Python中的属性访问与描述符 中对于属性访问优先规则的部分解释的很清晰，遂记录下来学习。 属性访问的优先规则 属性访问的入口点是 __getattribute__ 方法。它的实现中定义了Python中属性访问的优先规则。Python官方文档中对 __getattribute__ 的底层实现有相关的介绍，本文暂时只是讨论属性查找的规则，相关规则可见下图： 上图是查找b.x这样一个属性的过程。在这里要对此图进行简单的介绍： 查找属性的第一步是搜索基类列表，即type(b). mro ，直到找到该属性的第一个定义，并将该属性的值赋值给 descr 判断descr的类型。它的类型可分为数据描述符、非数据描述符、普通属性、未找到等类型。若descr为数据描述符，则调用desc. get (b, type(b))，并将结果返回，结束执行。否则进行下一步 如果 descr 为非数据描述符、普通属性、未找到等类型，则查找实例b的实例属性，即b. dict 。如果找到，则将结果返回，结束执行。否则进行下一步； 如果在b. dict 未找到相关属性，则重新回到 descr 值的判断上。 若 descr 为非数据描述符，则调用desc. get (b, type(b))，并将结果返回，结束执行 若 descr 为普通属性，直接返回结果并结束执行； 若 descr 为空（未找到），则最终抛出 AttributeError 异常，结束查找。 Additional","tags":"Python","url":"python-descriptor-behavior-ii.html"},{"title":"LeetCode - Maximum XOR of Two Numbers in an Array","text":"Description Given a non-empty array of numbers, $$ a_0, a_1, a_2, … , a_{n-1}, where 0 ≤ a_i < 2&#94;{31} $$ . Find the maximum result of a_i XOR a_j, where 0 ≤ i, j < n . Could you do this in O(n) runtime? Example: Input: [3, 10, 5, 25, 2, 8] Output: 28 Explanation: The maximum result is 5 &#94; 25 = 28. Source link Best practice 我们从左至右对每一位进行判断，在这个过程中逐渐缩小被我们选中的元素范围。假设输入列表包含26个整数（在这里我们以a,b,c,d至z来表示）。向右遍历时，当某几个数a,d,e,h,u在最大二进制位上不同时，就可以确定该位为我们最终答案的最大二进制位了。因为该位为1的情况大于该位右侧所有位为1的情况。在下次遍历时我们就可以检查这几个数中的次大二进制位是否不同，也就可以确定次大二进制位的值，我们的候选组也会从a,d,e,h,u缩小至a,e,h的情况。这个问题的特性是，每次我们缩小候选元素范围时，我们不需要关心哪些元素留下来了，只需要知道我们的最终答案是多少。 python version class Solution ( object ): def findMaximumXOR ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" max , mask = 0 , 0 for i in reversed ( range ( 32 )): mask = mask | ( 1 << i ) #prefix set prefixes = { num & mask for num in nums } temp = max | ( 1 << i ) for prefix in prefixes : # 因为1&#94;0=1,1&#94;1=0,0&#94;0=0,所以当a&#94;b=c时，a&#94;c=b # item1&#94;item2=temp,item1&#94;temp=item2 if prefix &#94; temp in prefixes : max = temp break return max Mark: 129 ms Additional if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Python","url":"leetcode-maximum-xor-of-two-numbers-in-an-array.html"},{"title":"LeetCode - Total Hamming Distance","text":"Description The Hamming distance between two integers is the number of positions at which the corresponding bits are different. Now your job is to find the total Hamming distance between all pairs of the given numbers. Example: Input : 4 , 14 , 2 Output : 6 Explanation : In binary representation , the 4 is 0100 , 14 is 1110 , and 2 is 0010 ( just showing the four bits relevant in this case ). So the answer will be : HammingDistance ( 4 , 14 ) + HammingDistance ( 4 , 2 ) + HammingDistance ( 14 , 2 ) = 2 + 2 + 2 = 6 . Note: 1. Elements of the given array are in the range of 0 to 10&#94;9 2. Length of the array will not exceed 10&#94;4 . Source link Best practice 经过观察可以发现，所有列表中元素的汉明距离也可以用他们各个位中的0和1的分布情况来得出。例如共有八个元素，所有元素的2&#94;0位有三个1五个0，则该位对整体汉明距离的贡献为3 5。本题中所描述的total hamming distance可以由所有位产生的汉明距离的和表示。这样的平均时间复杂度位O(n m),m为元素二进制位平均长度。 python version from collections import defaultdict class Solution ( object ): def totalHammingDistance ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" result = 0 for i in range ( 32 ): count = 0 # 列表中所有元素第i位中的1的数量 bit = ( 1 << i ) # 参照二进制数 for item in nums : if item & bit : count += 1 result += count * ( len ( nums ) - count ) return result Mark: 268 ms Additional","tags":"Python","url":"leetcode-total-hamming-distance.html"},{"title":"LeetCode - Maximum Length of Pair Chain","text":"Description You are given n pairs of numbers. In every pair, the first number is always smaller than the second number. Now, we define a pair (c, d) can follow another pair (a, b) if and only if b < c . Chain of pairs can be formed in this fashion. Given a set of pairs, find the length longest chain which can be formed. You needn't use up all the given pairs. You can select pairs in any order. Example 1: Input: [[1,2], [2,3], [3,4]] Output: 2 Explanation: The longest chain is [1,2] -> [3,4] Note: 1. The number of given pairs will be in the range [1, 1000]. Source link Best practice 本题的关键在于减少时间复杂度。尝试了排序后发现，将列表元组按第二个元素排序是一个时间复杂度为o（n）的解决方案。举例来说，[5,6]前所能接的元素链长度总大于等于[4,6]，因此按此排序后从第一个元素开始遍历筛选就可以了。这里我们没有在遇到拥有相同第二个元素的元组时使用取最大值的方法，因为普通的顺序遍历就能得到o（n）复杂度的算法了。 python version from operator import itemgetter class Solution ( object ): def findLongestChain ( self , pairs ): \"\"\" :type pairs: List[List[int]] :rtype: int \"\"\" last , n = None , 0 for item in sorted ( pairs , key = itemgetter ( 1 )): if last is None : last , n = item , 1 elif item [ 0 ] > last [ 1 ]: last , n = item , n + 1 return n Mark: 75 ms Additional","tags":"Python","url":"leetcode-maximum-length-of-pair-chain.html"},{"title":"一个简单的文本解析计算器","text":"Description 之前在《python cookbook》上8.22节看到了用非递归的方式实现访问者模式的 方法 ，通过巧妙地使用生成器的方式在树遍历或者搜索算法中中消除递归。从而避免了在使用访问者模式遍历深层嵌套树形数据结构时，因为超过嵌套层级限制而失败的情况。 原文是David Dabeaz基于python3.3版本构建的，很不凑巧的是，在python3.4版本又推出了一个非常强大的协程和生成器的新特性： yield from ，这无疑为我们增添了新的玩具。 2014年，Dabeaz的 Final generator 讲座全面地介绍了许多协程编写的诀窍并且炸毁了许多听众的大脑（包括笔者的），在课程的最后一部分他又从头用协程代替了经典的访问者模式，用于计算算术表达式。受他的课程的引导和启发便有了本文。 代码实现 我们希望实现的计算器能够这样使用： cal = Calculator () cal . caculate ( '1+2*4-5&#94;2' ) >>> - 16 从逻辑上来说，这个文本计算器会为我们分三步做事情： 解析传入文本，将文本转换成可识别单元。很容易想到的是使用正则表达式来解析文本。 确定运算的先后顺序，如乘除优先于加减。在这里我们将识别好的单元构造成树来体现运算的优先级。 深度遍历生成树，计算并输出结果。 接下来我们就来一步一步实现这一简易编译器。 准备工作 本文代码环境为python3.6。 首先定义好此次所需的数据结构。在这里我们利用了David在 Python 3 Metaprogramming 中所描述的元编程的方式来批量地构造简单结构类型。这里我们对他的代码稍作改进，使其支持默认参数及参数注释。这也算我们对元编程的一个小小实践，读者不需要理解这部分的内容就可以完成余下的阅读。 这里也用到了python3的新特性， inpect 模块的signature部分，具体可参见 官方文档 。 定义数据结构的代码如下： import re import types from collections import namedtuple from functools import singledispatch from inspect import Parameter , Signature def make_signature ( names ): \"\"\"用一个列表来产生参数签名的模块函数,也可以放在StructureMeta内部\"\"\" parameters = [] #抓出参数的名称、默认值和注释 parameter_re = re . compile ( r '&#94;(?P<name>\\w+)(\\s*=\\s*(?P<default>\\w+))?(\\s*:\\s*(?P<annotation>\\w+))?$' ) for name in names : re_result = parameter_re . match ( name ) if not re_result : raise SyntaxError ( 'Invalid parameter syntax：{}' . format ( name )) parameters . append ( Parameter ( kind = Parameter . POSITIONAL_OR_KEYWORD , ** re_result . groupdict ())) #支持参数默认值和注释 return Signature ( parameters ) class StructureMeta ( type ): \"\"\" Structure类的元类，在生成class的时候将_fields里提供的属性转化为 参数签名类属性。 \"\"\" def __new__ ( cls , name , bases , clsdict ): clsobj = super () . __new__ ( cls , name , bases , clsdict ) sig = make_signature ( clsobj . _fields ) setattr ( clsobj , '__signature__' , sig ) return clsobj class Structure ( metaclass = StructureMeta ): \"\"\"简易数据结构构造父类\"\"\" _fields = [] def __init__ ( self , * args , ** kwargs ): # 这里实际上取的是self.__class___.__signature__ bound = self . __signature__ . bind ( * args , ** kwargs ) for name , val in bound . arguments . items (): setattr ( self , name , val ) class Number ( Structure ): \"\"\"数字型\"\"\" _fields = [ 'value' ] class BinOp ( Structure ): \"\"\"操作符号型\"\"\" _fields = [ 'op' , 'left' , 'right' ] 其次则是要使用的Calculator的基本框架，三个关键步骤的函数讲在余下节内容一一实现： class Calculator : # 可以被tokenize函数解析的字符 TOKENS = [ r '(?P<NUM>\\d+)' , r '(?P<PLUS>\\+)' , r '(?P<MINUS>-)' , r '(?P<TIMES>\\*)' , r '(?P<DIVIDE>/)' , r '(?P<POWER>\\&#94;)' , r '(?P<WS>\\s+)' , ] # 储存字符类型和值的元组 Token = namedtuple ( 'Token' , [ 'type' , 'value' ]) def __init__ ( self , token = None ): if token : self . TOKENS = token # 预编译正则表达式 self . MASTER_RE = re . compile ( '|' . join ( self . TOKENS )) def caculate ( self , text ): \"\"\"解析并计算表达式\"\"\" self . text = text try : token = self . _tokenize ( text ) tree = self . _parse ( token ) result = self . _evaluate ( tree ) except Exception as e : raise e return result def _tokenize ( self , text ): \"\"\"从字符串开始扫描所有匹配字符,输出所有非空元素\"\"\" pass def _parse ( self , toks ): \"\"\"将tokenize后的元素parse成树结构\"\"\" pass def _evaluate ( self , node ): \"\"\"遍历生成树计算结果\"\"\" pass 对于数据结构类我们使用了一种简单粗暴的构造方法：直接设置成类的属性。其实这也是一种蛮常用的方法，适合构造大量简单数据结构类。 对于计算器类，我们开放了caculate api接受传入的字符串，并经过上文论述的三个步骤来输出结果。当然我们也可以直接调用这三个步骤的方法来进行调试和维护，我们接下来的任务就是分别实现这三个方法。 Tokenize def _tokenize ( self , text ): \"\"\"从字符串开始扫描所有匹配字符,输出所有非空元素\"\"\" try : scan = self . MASTER_RE . scanner ( text ) except Exception as e : raise e return ( self . Token ( m . lastgroup , m . group ()) for m in iter ( scan . match , None ) if m . lastgroup != 'WS' ) Tokenize 方法根据类变量 TOKENS 里的正则表达式捕获匹配字符组并且将它们命名，之后返回所有非空格的字符元素。 这里值得一提的是 re 模块的 scanner 方法。不知道是不是刻意而为之，它没有任何官方的文档。在我们这个简单编译器的情景下，它逐个扫描传入字符串里的所有符合正则表达式的元素并输出。 对于 1+2*4-5&#94;2 我们将生成这些元素： cal = Calculator () cal . text = '1+2*4-5&#94;2' for item in cal . _tokenize ( cal . text ): print ( item ) >>> Token ( type = 'NUM' , value = '1' ) Token ( type = 'PLUS' , value = '+' ) Token ( type = 'NUM' , value = '2' ) Token ( type = 'TIMES' , value = '*' ) Token ( type = 'NUM' , value = '4' ) Token ( type = 'MINUS' , value = '-' ) Token ( type = 'NUM' , value = '5' ) Token ( type = 'POWER' , value = '&#94;' ) Token ( type = 'NUM' , value = '2' ) Parse def _parse ( self , toks ): \"\"\"将tokenize后的元素parse成树结构\"\"\" lookahead , current = next ( toks , None ), None def accept ( * toktypes ): \"\"\"判断生成器toks的下个元素是否为传入类型\"\"\" nonlocal lookahead , current if lookahead and lookahead . type in toktypes : current , lookahead = lookahead , next ( toks , None ) return True # 表达式结构： # expr ::= term { +|- term }* # term ::= pow { *|/ pow}* # pow ::= factor { &#94; factor}* # factor ::= NUM def expr (): left = term () while accept ( 'PLUS' , 'MINUS' ): left = BinOp ( current . value , left ) left . right = term () return left def term (): left = pow () while accept ( 'TIMES' , 'DIVIDE' ): left = BinOp ( current . value , left ) left . right = pow () return left def pow (): left = factor () while accept ( 'POWER' ): left = BinOp ( current . value , left ) left . right = factor () return left def factor (): if accept ( 'NUM' ): return Number ( int ( current . value )) else : raise SyntaxError () return expr () Parse 方法通过 accept 函数来遍历 tokenize 返回的字符元素迭代器，并通过树状的函数结构来生成一棵真正的树。非常欣赏它的模仿能力。 它将表达式在语意上分为三种类型：term，pow，factor (实际上这些名字没什么特别的意意义)。例如 1+2*4-5&#94;2 这个表达式，factor为最小单元即数字，factor组成pow即 1 2 4 5&#94;2 为次小单元，pow组成term 1 2*4 5&#94;2 ,term组成表达式来体现运算符的优先级。 对于 1+2*4-5&#94;2 我们可以这样生成树： cal = Calculator () cal . text = '1+2*4-5&#94;2' print ( cal . _parse ( cal . _tokenize ( cal . text ))) Evaluate def _evaluate ( self , node ): \"\"\"遍历生成树计算结果\"\"\" @singledispatch def visit ( obj ): raise NotImplemented @visit.register ( BinOp ) def _ ( node ): \"\"\" 协程。 visit method for BinOp \"\"\" left = yield node . left right = yield node . right # TODO: could be more dynamic switch = { '+' : lambda x , y : x + y , '-' : lambda x , y : x - y , '*' : lambda x , y : x * y , '/' : lambda x , y : x / y , '&#94;' : lambda x , y : x ** y , } try : return switch . get ( node . op , None )( left , right ) # 产生StopIteration并返回结果 except TypeError as e : pass @visit.register ( Number ) def _ ( node ): \"\"\"visit method for number\"\"\" return node . value def gen_visit ( node ): \"\"\" 委派生成器。 返回输入数值及中间值。 \"\"\" result = visit ( node ) return ( yield from result ) if isinstance ( result , types . GeneratorType ) else result stack = [ gen_visit ( node )] # 将根节点的协程放入栈 result = None while stack : try : node = stack [ - 1 ] . send ( result ) # send(None)预激协程，send（result）将计算好的值存入协程 stack . append ( gen_visit ( node )) # 深度遍历添加协程，等待处理 result = None except StopIteration as e : stack . pop () result = e . value # 取得number的值或表达式计算值 return result Evalute 函数遍历 parse 所生成的树并计算结果。在这里我们用3.4新加入的 singledispatch 来替代原先Dabeaz使用的方式： methname = 'visit_' + type ( node ) . __name__ meth = getattr ( self , methname , None ) 比较容易让人炸脑的是用list来管理栈的过程。以 1+2*4-5 这个表达式举例，传入 evaluate 方法的生成树是这样的。 第一层 : BinOp \"-\" Number 第二层： Number \"+\" BinOp 5 第三层： 1 Number \"*\" Number 第四层： 2 4 如果你发现自己无法理解我的绘画作品的话不妨跑下 parse 方法。 我们先将初始化根节点的委派生成器入栈，记该委派生成器 gen_visit 为 A ，传入None预激 A , visit(BinOp('-',x,y)) 返回协程 a ，由于该协程为生成器子类，进入 return 语句中的 yield from result 子句代理的协程 a ， yield 出 BinOp('-',x,y) 的左节点 BinOp('+',x,y) 。将其传入委派生成器 B 并入栈。此时协程 a 走至 left = yield node.left 语句的等号右边。 传入None预激委派生成器 B , visit(BinOp('+',x,y)) 返回协程 b ，进入 yield from result 代理的协程 b 中 yield 出 BinOp('+',x,y) 的左节点 Number(1) 。将其传入委派生成器 C 并入栈。此时协程 b 走至 left = yield node.left 语句的等号右边。 传入None预激委派生成器 C , visit(Number(1)) 返回整形 1 ， 此时 return 将返回 result ，抛出 StopIteration 异常。我们捕获异常获得 result 的值 1 并将 C 出栈。 将 1 传入委派生成器 B 代理的协程 b ，此时 left = yield node.left 中变量 left 获得传入值并继续 yield 出右节点 BinOp('*',x,y) 中间重复过程不再累述，当协程走至 return switch.get(node.op, None)(left, right) 则会抛出 StopIteration 异常和该表达式的运算结果，以此层层回溯得到最终的结果。 总结 至此，我们已经完成对 Calculator 类的编写。 其实我们完全可以用python自带的 eval 方法来执行任意字符串代码。但之所以我们要大费周章地用协程实现这一文本计算器，是为了在python中实践 stackless 的思想。 比如要计算 cal.caculate('+'.join(str(i) for i in range(2017))) ,我们的 parse 函数会生成一棵超过两千深度的树，这时候就无法用递归的方式来遍历树了。 另外，如cookbook里说的，如果我们想用另一种没有 yield 语句的方案，我们不得不处理很多棘手的问题。例如，为了消除递归，我们必须要维护一个栈结构。如果不使用生成器，代码就会变得很臃肿，到处都是栈操作语句、会掉函数等。因此使用 yield 可以让你写出非常漂流的代码，它消除了递归但看上去又很像是递归实现，代码很简洁。 Additional 代码源文件: 1. text_calculator.py","tags":"Python","url":"yi-ge-jian-dan-de-wen-ben-jie-xi-ji-suan-qi.html"},{"title":"Python装饰器的正确打开方式(2)","text":"Description 装饰器是python语言的一个非常常用及pythonic的特性，但往往由于忽视python中的内省，我们会写出一些不是特别完美的自定义装饰器。Graham Dumpleton写了一系列 博客 ，深入剖析了如何实现行为良好的装饰器。此外他还是 wrapt模块 的作者,他将他对装饰器的深厚知识充分应用到这个模块之中。这个模块的作用是简化装饰器和动态函数包装起的实现，使得多层装饰也支持内省且行为正确，既可以应用到方法上，也可以作为描述符使用。 本文拾取了Graham Dumpleton在wrapt模块中附带的一系列博文的牙慧，旨在带来对python装饰器更深的理解和更好的设计。 上一篇文章 论述了普通装饰器可能带来的一些内省缺陷，并提出了一些解决方案。本文接上文更深入地介绍python装饰器的高级用法以便之后继续探讨关于内省缺陷的解决方案。 带参数的装饰器 接上文，我们至今为止的所创建的所有装饰器都不能传入任何参数，但要知道通过传入参数来改变装饰器的部分装饰行为是一种很常见并且强大的特性。通常我们只要用一个接受参数的函数闭包就能构成一个带参数的装饰器。 def with_arguments ( arg ): @decorator def _wrapper ( wrapped , instance , args , kwargs ): return wrapped ( * args , ** kwargs ) return _wrapper @with_arguments ( arg = 1 ) def function (): pass 如果给装饰器参数arg一个默认值的话，就能以 @with_arguments() 的方式调用它了。但这种调用方式会和我们之前使用装饰器的方式不一致。但是我们可以用参数默认值和 partial 函数结合的方法来实现这一前后的统一： def optional_arguments ( wrapped = None , * , arg = 1 ): if wrapped is None : return functools . partial ( optional_arguments , arg = arg ) @decorator def _wrapper ( wrapped , instance , args , kwargs ): return wrapped ( * args , ** kwargs ) return _wrapper ( wrapped ) @optional_arguments ( arg = 2 ) def function1 (): pass @optional_arguments def function2 (): pass 当wrapped参数为空时，则返回一个已经具有默认参数 arg 的装饰器 functools.partial(optional_arguments, arg=arg) 来装饰函数。 给被装饰的函数添加缓存功能 很多时候，当我们多遍调用某个函数的时候就会希望该函数能够\"记录\"下来它之前所运行的结果，并且在下次传入相同参数的时候不再去做它内部那些复杂的运算而是直接返回缓存的值。这听起来是一个有些麻烦的需求。幸运的是，通过在装饰器中定义一个缓存结构，能让我们方便地让任意一个函数拥有这一神奇的功能。 这里我们也用到了上一节所提到的默认参数和 partial 函数结合的装饰器设计： def cache ( wrapped = None , d = None ): if wrapped is None : return functools . partial ( cache , d = d ) if d is None : d = {} @decorator def _wrapper ( wrapped , instance , args , kwargs ): try : key = ( args , frozenset ( kwargs . items ())) #此次传入的参数集合 return d [ key ] #当参数集合已缓存时直接返回之前的结果 except KeyError : result = d [ key ] = wrapped ( * args , ** kwargs ) #当为新参数的缓存到装饰器的字典中 return result return _wrapper ( wrapped ) @cache def function1 (): return time . time () _d = {} #当传入同一个字典的时候，被装饰的不同函数会拥有相同的缓存。 @cache ( d = _d ) def function2 (): return time . time () @cache ( d = _d ) def function3 (): return time . time () 与之前相同，我们也可以将这一缓存装饰器写成类的版本： class cache ( object ): def __init__ ( self , wrapped ): self . wrapped = wrapped self . d = {} def __call__ ( self , * args , ** kwargs ): try : key = ( args , frozenset ( kwargs . items ())) return self . d [ key ] except KeyError : result = self . d [ key ] = self . wrapped ( * args , ** kwargs ) return result @cache def function (): return time . time () python中的同步装饰器 在Java中，如果我们希望一个方法是同步 (synchronized) 的，有两种方式给我们的代码增加同步特性： //第一是使用synchronized关键字的方式使整个方法具有同步特性 public class SynchronizedCounter { private int c = 0 ; public synchronized void increment () { c ++; } public synchronized void decrement () { c --; } public synchronized int value () { return c ; } } //第二是使用synchronized语句使得被包裹的代码块具有同步特性 //与第一个方法不同的是，我们必须制定一个提供内在锁的对象(这 //里是this) public void addName ( String name ) { synchronized ( this ) { lastName = name ; nameCount ++; } nameList . add ( name ); } 简单的来说，同步特性即是让每个类的实例都拥有一个内在的锁，当一个方法或一段代码被触发的时候就取得锁，当该方法返回时随后该锁就被释放。这种锁被称为 重入锁 。当一个线程获取对象锁之后，这个线程可以不用阻塞地再次获取本对象上的锁，而其他的线程是不可以的。这就使得从一个同步的方法运行在同一对象中的另一个同步方法成为了可能。 初探同步装饰器 在python中，我们可以利用上下文管理器和threading模块的锁方法来确保被我们装饰的函数具有同步特性。 import threading def synchronized ( wrapped ): lock = threading . RLock () @functools.wraps ( wrapped ) def _wrapper ( * args , ** kwargs ): with lock : return wrapped ( * args , ** kwargs ) return _wrapper @synchronized def function (): pass 当然我们也可以用上之前提到的带参装饰器技术使我们的同步锁装饰器更加灵活,再加上 decorator 装饰器来获取之前的自省特性。 def synchronized ( wrapped = None , lock = None ): if wrapped is None : return functools . partial ( synchronized , lock = lock ) if lock is None : lock = threading . RLock () @decorator def _wrapper ( wrapped , instance , args , kwargs ): with lock : return wrapped ( * args , ** kwargs ) return _wrapper ( wrapped ) @synchronized def function1 (): pass lock = threading . Lock () @synchronized ( lock = lock ) def function2 (): pass 这使得我们的装饰器能够适用在实例、类和静态方法。但仔细想想以上代码的话会发现这一简单的实现其实并不实用。因为同步锁只对不同线程访问被装饰的方法时才起作用并且他会作用于着个类的所有实例，这和我们希望在java中看到的表现不尽相同。 我们想要的行为是：对于某个类的一个实例的所有被synchronized装饰过的实例方法，它们会同步关联一个类实例的单锁对象。 同时，我们要考虑一个额外的问题，发生竞争创建锁关系时我们该如何保证我们的线程安全性。 在对象中储存同步锁 让我们重新考虑一下问题，除了传入锁或者在函数闭包中创建它，我们可不可以让被装饰对象自己储存锁并通过包装函数来管理？ 答案是肯定的。让我们来看以下代码： @decorator def synchronized ( wrapped , instance , args , kwargs ): if instance is None : owner = wrapped #当被装饰的位普通函数或静态方法时将同步锁绑在函数或方法上 else : owner = instance #当被装饰的为实例方法或类方法时将同步锁绑在实例或类上 lock = vars ( owner ) . get ( '_synchronized_lock' , None ) if lock is None : #使用metalock来确保创建同步锁时不会出现竞争关系 meta_lock = vars ( synchronized ) . setdefault ( '_synchronized_meta_lock' , threading . Lock ()) with meta_lock : #再次确认锁的存在状态，防止在生成meta_lock时同步锁已经被其他线程生成 lock = vars ( owner ) . get ( '_synchronized_lock' , None ) if lock is None : lock = threading . RLock () setattr ( owner , '_synchronized_lock' , lock ) with lock : return wrapped ( * args , ** kwargs ) #对应的绑定关系如下 @synchronized # lock bound to function1 def function1 (): pass @synchronized # lock bound to function2 def function2 (): pass @synchronized # lock bound to Class class Class ( object ): @synchronized # lock bound to instance of Class def function_im ( self ): pass @synchronized # lock bound to Class @classmethod def function_cm ( cls ): pass @synchronized # lock bound to function_sm @staticmethod def function_sm (): pass 在实现我们的设计的时候一个关键的事情就是在第一次创建同步锁的时候我们需要判断锁是否存在，如果存在的话就返回原有的锁。我们通过 lock = vars(wrapped).get('_synchronized_lock', None) 获取锁。 当我们遇到多个线程竞争创建锁时，可以使用 lock = vars(wrapped).setdefault('_synchronized_lock', threading.RLock()) 默认字典的形式来防止任一线程创建的锁被另一个线程覆盖。但如果我们使用这一方法的话，我们会在装饰类方法上遇到问题。因为类的字典 dictproxy 并没有setdefault方法，因此我们只能使用 setattr(Object, '_synchronized_lock', threading.RLock()) 来为类设置同步锁 让同步锁装饰器具有上下文管理器的功能 至此为止，我们已经实现了java的同步的第一个功能了。对于第二个功能，在python中是一个和上下文管理器相似的行为。我们想让我们的synchronized装饰器能够这样使用来同步方法或函数中的部分代码： ```python class Object(object): @synchronized def function_im_1(self): pass def function_im_2(self): with synchronized(self): pass ``` 为了拥有上下文管理器的功能，我们必须让装饰器函数返回一个具有 __enter__ and __exit__ 方法的对象。但我们的 synchronized(None) 实际返回的是一个 <__main__.function_wrapper object at 0x107b7ea10> 对象，再该类里我们还没有着两个方法的定义。因此我们已经不能使用原来的装饰器工厂函数 @decorator 了，作为替代我们首先要直接使用之前定义的 function_wrapper 类装饰器来获得内省功能： ```python def synchronized(wrapped): def _synchronized_lock(owner): lock = vars(owner).get('_synchronized_lock', None) if lock is None: meta_lock = vars(synchronized).setdefault( '_synchronized_meta_lock', threading.Lock()) with meta_lock: lock = vars(owner).get('_synchronized_lock', None) if lock is None: lock = threading.RLock() setattr(owner, '_synchronized_lock', lock) return lock #使用_synchronized_lock同步锁的自定义装饰器 def _synchronized_wrapper(wrapped, instance, args, kwargs): with _synchronized_lock(instance or wrapped): return wrapped(*args, **kwargs) #直接使用function_wrapper自定义同步锁装饰器装饰到被装饰函数warpped上 return function_wrapper(wrapped, _synchronized_wrapper) ``` 现在我们已经完成装饰工程函数的替换工作了，接下来的功能就是在 function_wrapper 类装饰器中加入 __enter__ and __exit__ 魔术方法来实现上下文管理器，这里我们继承了 function_wrapper 类: def synchronized ( wrapped ): def _synchronized_lock ( owner ): lock = vars ( owner ) . get ( '_synchronized_lock' , None ) if lock is None : meta_lock = vars ( synchronized ) . setdefault ( '_synchronized_meta_lock' , threading . Lock ()) with meta_lock : lock = vars ( owner ) . get ( '_synchronized_lock' , None ) if lock is None : lock = threading . RLock () setattr ( owner , '_synchronized_lock' , lock ) #为被装饰器装饰的方法加锁 return lock def _synchronized_wrapper ( wrapped , instance , args , kwargs ): with _synchronized_lock ( instance or wrapped ): return wrapped ( * args , ** kwargs ) class _synchronized_function_wrapper ( function_wrapper ): def __enter__ ( self ): self . _lock = _synchronized_lock ( self . wrapped ) #为上下文管理器枷锁 self . _lock . acquire () return self . _lock def __exit__ ( self , * args ): self . _lock . release () return _synchronized_function_wrapper ( wrapped , _synchronized_wrapper ) 至此为止，我们对java的 synchronized 的移植就全部完成了。 Additional 参考文献: 1. Wrapt blog","tags":"Python","url":"pythonzhuang-shi-qi-de-zheng-que-da-kai-fang-shi-2.html"},{"title":"LeetCode - Replace Words","text":"Description In English, we have a concept called root , which can be followed by some other words to form another longer word - let's call this word successor . For example, the root an , followed by other , which can form another word another. Now, given a dictionary consisting of many roots and a sentence. You need to replace all the successor in the sentence with the root forming it. If a successor has many roots can form it, replace it with the root with the shortest length. You need to output the sentence after the replacement. Example 1: Input : dict = [ \"cat\" , \"bat\" , \"rat\" ] sentence = \"the cattle was rattled by the battery\" Output : \"the cat was rat by the bat\" Note: 1. The input will only have lower-case letters. 2. 1 <= dict words number <= 1000 3. 1 <= sentence words number <= 1000 4. 1 <= root length <= 100 5. 1 <= sentence words length <= 1000 Source link Best practice 在这里用到的数据结构是Tire树。 Trie树，即字典树，又称单词查找树或键树，是一种树形结构，是一种哈希树的变种。典型应用是用于统计和排序大量的字符串（但不仅限于字符串），所以经常被搜索引擎系统用于文本词频统计。它的优点是：最大限度地减少无谓的字符串比较，查询效率比哈希表高。 Trie的核心思想是空间换时间。利用字符串的公共前缀来降低查询时间的开销以达到提高效率的目的。 python version from collections import defaultdict import functools class Solution ( object ): def replaceWords ( self , dict , sentence ): \"\"\" :type dict: List[str] :type sentence: str :rtype: str \"\"\" IS_WORD = True #Tire树中表示节点为单词末字母的标识符，这里额外存储完整单词 def generateTire ( dct ): #这是一个很强大的递归生成defaultdict树的表达式 _tire = lambda : defaultdict ( _tire ) tire = _tire () for word in dct : node = tire for char in word : node = node [ char ] node [ IS_WORD ] = word return tire def searchWord ( tire , word ): #在tire树种查找到最短的单词successor，这里 #只要遇到IS_WORD键就返回查到的单词 node = tire for char in word : if char not in node : break node = node [ char ] if IS_WORD in node : return node [ IS_WORD ] return word #利用partial方法构造map函数 replace = functools . partial ( searchWord , generateTire ( dict )) return \" \" . join ( map ( replace , sentence . split ())) Mark: 116 ms Additional","tags":"Python","url":"leetcode-replace-words.html"},{"title":"Python装饰器的正确打开方式(1)","text":"Description 装饰器是python语言的一个非常常用及pythonic的特性，但往往由于忽视python中的内省，我们会写出一些不是特别完美的自定义装饰器。Graham Dumpleton写了一系列 博客 ，深入剖析了如何实现行为良好的装饰器。此外他还是 wrapt模块 的作者,他将他对装饰器的深厚知识充分应用到这个模块之中。这个模块的作用是简化装饰器和动态函数包装起的实现，使得多层装饰也支持内省且行为正确，既可以应用到方法上，也可以作为描述符使用。 本文拾取了Graham Dumpleton在wrapt模块中附带的一系列博文的牙慧，旨在带来对python装饰器更深的理解和更好的设计。 两类装饰器及内省缺陷 装饰器符 @ 是一种语法糖，深究原理的话使用装饰器实际上是一种猴子补丁的实现方式。以下的两种方式是等价的： @function_wrapper def function (): pass 等价于 #在python2.4版本你会这么做 def function (): pass function = function_wrapper ( function ) 而对于装饰器的实现，我们有以下两种方式： 1.定义类 class function_wrapper : def __init__ ( self , wrapped ): self . wrapped = wrapped def __call__ ( self , * args , ** kwargs ): return self . wrapped ( * args , ** kwargs ) @function_wrapper def function (): pass #通过魔术方法__call__来运行被包裹的方法 2.使用闭包函数 def function_wrapper ( wrapped ): def _wrapper ( * args , ** kwargs ): return wrapped ( * args , ** kwargs ) return _wrapper @function_wrapper def function (): pass 然而由于描述符的原因，使用类作为装饰器是一个更好地选择。 对于直接定义的装饰器，被装饰的方法的 name 和 doc 属性将会丢失，因此标准库functools提供了 warps 和 update_wrapper 装饰器来将被装饰方法的自省属性传递给装饰器： import functools def function_wrapper ( wrapped ): #对闭包使用functools.wraps装饰 @functools.wraps ( wrapped ) def _wrapper ( * args , ** kwargs ): return wrapped ( * args , ** kwargs ) return _wrapper class function_wrapper : def __init__ ( self , wrapped ): self . wrapped = wrapped #对于类使用functools.update_wrapper装饰 functools . update_wrapper ( self , wrapped ) def __call__ ( self , * args , ** kwargs ): return self . wrapped ( * args , ** kwargs ) @function_wrapper def function (): pass 事实上warps()重用了update_wrapper()的代码，为了更好地理解装饰器的运作，不妨让我们来看一下它的源码。 \"\"\" python 3.3版本的update_wrapper()的源码,在这里将被装饰的方法保存在__wrapped__属性中。这是一个bug，3.4中将这一步放在了函数体的最后。 这个函数又将WRAPPER_ASSIGNMENTS中的属性从被装饰函数wrapped中复制到装饰器wrapper中。 最后将被装饰函数__dict__中的内容复制到装饰器中。 \"\"\" WRAPPER_ASSIGNMENTS = ( '__module__' , '__name__' , '__qualname__' , '__doc__' , '__annotations__' ) WRAPPER_UPDATES = ( '__dict__' ,) def update_wrapper ( wrapper , wrapped , assigned = WRAPPER_ASSIGNMENTS , updated = WRAPPER_UPDATES ): wrapper . __wrapped__ = wrapped for attr in assigned : try : value = getattr ( wrapped , attr ) except AttributeError : pass else : setattr ( wrapper , attr , value ) for attr in updated : getattr ( wrapper , attr ) . update ( getattr ( wrapped , attr , {})) 然而，即便使用了functools的修正方法保存了原函数的 name 和 doc 方法，但还是会在以下几个方面存在缺陷： 保存函数参数规范（ inspect.getargspec() ） 保存函数获取源码的能力（ inspect.getsource() ） * 叠加附加在描述符上的能力 解决方案 描述符装饰器 解决问题的一个办法是为普通函数和类中函数分配各自的装饰方法，这样形成的装饰器也会是一种描述符。 class bound_function_wrapper : def __init__ ( self , wrapped ): self . wrapped = wrapped functools . update_wrapper ( self , wrapped ) def __call__ ( self , * args , ** kwargs ): return self . wrapped ( * args , ** kwargs ) class function_wrapper : def __init__ ( self , wrapped ): self . wrapped = wrapped functools . update_wrapper ( self , wrapped ) def __get__ ( self , instance , owner ): wrapped = self . wrapped . __get__ ( instance , owner ) return bound_function_wrapper ( wrapped ) def __call__ ( self , * args , ** kwargs ): return self . wrapped ( * args , ** kwargs ) 如果装饰器附加在一个普通函数上，会使用function_wrapper的__call__方法返回的函数。而如果是附加在类方法上的话，则会调用__get__方法返回一个绑定instance的wrapper，然后它的__call__方法则会被触发。这使得该装饰器能传递描述符协议。 值得一提的是，每次当这个wrapper附加在类方法上被调用的时候，一个新的辅助wrapper将会被创建。这点无疑影响了效率。我们可能需要一个更加高效的方法来实现这种装饰器了。 透明对象代理 对于以上问题的解决方案被称为对象代理。以下是一个与被它包装的对象看上去很相似的wrapper： #这个对象代理的例子只代理了一些基本的方法。 class object_proxy : def __init__ ( self , wrapped ): self . wrapped = wrapped try : self . __name__ = wrapped . __name__ except AttributeError : pass @property def __class__ ( self ): return self . wrapped . __class__ def __getattr__ ( self , name ): return getattr ( self . wrapped , name ) 有了这个wrapper类我们就可以跟 update_wrapper() 说拜拜了。可以对我们的装饰器做以下修改： class bound_function_wrapper ( object_proxy ): def __init__ ( self , wrapped ): super () . __init__ ( wrapped ) def __call__ ( self , * args , ** kwargs ): return self . wrapped ( * args , ** kwargs ) class function_wrapper ( object_proxy ): def __init__ ( self , wrapped ): super () . __init__ ( wrapped ) def __get__ ( self , instance , owner ): wrapped = self . wrapped . __get__ ( instance , owner ) return bound_function_wrapper ( wrapped ) def __call__ ( self , * args , ** kwargs ): return self . wrapped ( * args , ** kwargs ) 这时__name__和__doc__ 之类的属性将会从代理对象中获取，inspect.getargspec()和inspect.getsource()也能够顺利工作。 这个方案还存在的一个明显缺陷就是：每次我们要定义一个新装饰器便要继承object_proxy代理函数并写两个类的代码。为了让我们的装饰器更好用不妨用工厂函数来帮我们完成这项重复工作。 使用装饰器工厂来创建装饰器 在这节我们的目的是创建一个帮助我们更好地创建装饰器的装饰器。这可能听起来有些拗口，但这种设计确实能减少我们构建一个新的装饰器时候的代码。简而言之，我们的目标是让我们可以像这样创建一个装饰器： @decorator def my_function_wrapper ( wrapped , args , kwargs ): return wrapped ( * args , ** kwargs ) @my_function_wrapper def function (): pass 事实上，我们的装饰器工厂的实现方式和使用 partial() 函数很像，它在定义时将新装饰器绑入，在运行时接受被新装饰器装饰的的对象。因此在我们之前的定义的装饰器基础上要传入wrapper参数。 import functools def decorator ( wrapper ): @functools.wraps ( wrapper ) def _decorator ( wrapped ): return function_wrapper ( wrapped , wrapper ) return _decorator class bound_function_wrapper ( object_proxy ): def __init__ ( self , wrapped , wrapper ): super () . __init__ ( wrapped ) self . instace = instance #为之后的内省保存instance属性 self . wrapper = wrapper def __call__ ( self , * args , ** kwargs ): if self . instance is None : #当类方法以Class.method(instance,arg1,arg2)的形式调用 #的时候，会产生self.instance为None的特殊情况，这时第一 #个参数为instance，其余参数为传入变量，以这个形式返回被 #装饰好的类方法 instance , args = args [ 0 ], args [ 1 :] wrapped = functools . partial ( self . wrapped , instance ) return self . wrapper ( wrapped , instance , args , kwargs ) return self . wrapper ( self . wrapped , self . instance , args , kwargs ) class function_wrapper ( object_proxy ): def __init__ ( self , wrapped , wrapper ): super () . __init__ ( wrapped ) self . wrapper = wrapper def __get__ ( self , instance , owner ): wrapped = self . wrapped . __get__ ( instance , owner ) return bound_function_wrapper ( wrapped , instance , self . wrapper ) #当装饰器附加在类方法时传入该instance def __call__ ( self , * args , ** kwargs ): return self . wrapper ( self . wrapped , None , args , kwargs ) #当装饰器附加在普通函数时instance变量传入None 这个装饰器工厂的使用示例如下： @decorator def my_function_wrapper ( wrapped , instance , args , kwargs ): print ( 'INSTANCE' , instance ) print ( 'ARGS' , args ) return wrapped ( * args , ** kwargs ) @my_function_wrapper def function ( a , b ): pass class Class ( object ): @my_function_wrapper def function_im ( self , a , b ): pass >>> function ( 1 , 2 ) INSTANCE None ARGS ( 1 , 2 ) >>> c . function_im ( 1 , 2 ) INSTANCE < __main__ . Class object at 0x1085ca9d0 > ARGS ( 1 , 2 ) >>> Class . function_im ( c , 1 , 2 ) INSTANCE < __main__ . Class object at 0x1085ca9d0 > ARGS ( 1 , 2 ) 可以看到这个设计已经比较好地解决了我们之前遇到的那些问题，但这仍旧不是一个完美的方案。秉持着求真务实的精神，我们可以发现该装饰器如果叠加载classmethod上则会出现问题： class Class ( object ): @my_function_wrapper @classmethod def function_cm ( cls , a , b ): pass >>> Class . function_cm ( 1 , 2 ) INSTANCE 1 ARGS ( 2 ,) 我们也很绝望啊,所以只能改进啦。接下来我们会设计一个统一装饰器（宇宙装饰器）来对装饰器附加在普通函数、实例方法、类方法、静态函数甚至类上的情况分发对应的策略。 讲类方法和静态方法纳入疆界 在解决之前，我们先明确一下问题。我们的目标是让我们的装饰器能够区分以下三种不同的方法： 通过类的途径运行的实例方法 被classmethod装饰的类方法 * 被staticmethod装饰的静态方法 首先能想到的是一个简单的方法，就是在传入一个参数来记录被绑定的方法类型。因此我们可以对代码作如下修改。 class bound_function_wrapper ( object_proxy ): def __init__ ( self , wrapped , instance , wrapper , binding ): super ( bound_function_wrapper , self ) . __init__ ( wrapped ) self . instance = instance self . wrapper = wrapper self . binding = binding #传入绑定的方法类型 def __call__ ( self , * args , ** kwargs ): if self . binding == 'function' : #方法调用 if self . instance is None : #以Class.method(instan,args,kwargs)形式调用，则取第一个参数作为self instance , args = args [ 0 ], args [ 1 :] wrapped = functools . partial ( self . wrapped , instance ) return self . wrapper ( wrapped , instance , args , kwargs ) else : #以instance.method()方式调用 return self . wrapper ( self . wrapped , self . instance , args , kwargs ) else : #如果是类方法或者静态方法调用 instance = getattr ( self . wrapped , '__self__' , None ) #当为时类方法instance变量等于方法__self__即Class对象，当为静态方法时则为None return self . wrapper ( self . wrapped , instance , args , kwargs ) class function_wrapper ( object_proxy ): def __init__ ( self , wrapped , wrapper ): super ( function_wrapper , self ) . __init__ ( wrapped ) self . wrapper = wrapper #判断被绑定方法的种类 if isinstance ( wrapped , classmethod ): self . binding = 'classmethod' elif isinstance ( wrapped , staticmethod ): self . binding = 'staticmethod' else : self . binding = 'function' def __get__ ( self , instance , owner ): wrapped = self . wrapped . __get__ ( instance , owner ) return bound_function_wrapper ( wrapped , instance , self . wrapper , self . binding ) def __call__ ( self , * args , ** kwargs ): return self . wrapper ( self . wrapped , None , args , kwargs ) 接下来我们可以分别对普通类方法im，类方法cm，静态方法sm做测试来检验了： >>> c . function_im ( 1 , 2 ) INSTANCE < __main__ . Class object at 0x10c2c43d0 > ARGS ( 1 , 2 ) >>> Class . function_im ( c , 1 , 2 ) INSTANCE < __main__ . Class object at 0x10c2c43d0 > ARGS ( 1 , 2 ) >>> c . function_cm ( 1 , 2 ) INSTANCE < class ' __main__ . Class '> ARGS ( 1 , 2 ) >>> Class . function_cm ( 1 , 2 ) INSTANCE < class ' __main__ . Class '> ARGS ( 1 , 2 ) >>> c . function_sm ( 1 , 2 ) INSTANCE None ARGS ( 1 , 2 ) >>> Class . function_sm ( 1 , 2 ) INSTANCE None ARGS ( 1 , 2 ) 写完这些代码就可以了嘛，很不幸的告诉你，这还不够。在接下来的博文中会展现GrahamDumpleton对装饰器理解的方方面面，让我们来看看他对完美装饰器的不懈追求吧。 Additional 参考文献: 1. Wrapt blog","tags":"Python","url":"pythonzhuang-shi-qi-de-zheng-que-da-kai-fang-shi-1.html"},{"title":"LeetCode - Linked List Random Node","text":"Description Given a singly linked list, return a random node's value from the linked list. Each node must have the same probability of being chosen. Follow up: What if the linked list is extremely large and its length is unknown to you? Could you solve this efficiently without using extra space? Example: // Init a singly linked list [1,2,3]. ListNode head = new ListNode(1); head.next = new ListNode(2); head.next.next = new ListNode(3); Solution solution = new Solution(head); // getRandom() should return either 1, 2, or 3 randomly. Each element should have equal probability of returning. solution.getRandom(); Source link Analytics 当数据流长度已知或不大的时候可以简单的解决这个问题：遍历链表之后得到长度n，以 $$ {\\frac{1}{n}} $$ 的概率选取元素。 但当给出数据流的长度很大或者未知时，我们将无法做遍历链表得到长度的操作。此时因为数据流很大，为了追求效率，该数据流中数据只能访问一次。有没有这么一个随机选择算法，使得该数据流中的所有数据被选中的概率相等呢？ 这个无边界的问题确实很让人头疼啊，但幸运的是，这我们可以用为蓄水池抽样（Reservoir Sampling）的方法来解决该类问题。 蓄水池抽样介绍 蓄水池抽样是一种从一个包含 n 个元素的列表 S 中随机抽取 k 个样本的随机算法，这里的 n 是一个非常大或者未知的值。 这个算法的基本思想就是先选中 1 到 k 个元素，作为被选中的元素。然后依次对第 k+1 至第 n 个元素做以下操作： 每个元素都有 $$ {\\frac{k}{i}} $$ 的概率被选中，然后以等概率 $$ {\\frac{1}{k}} $$ 替换掉被选中的元素。其中 i 是元素的序号。 算法证明 算法的成立是用数学归纳法证明的: 设每次都是以k/i的概率来选择。假设当前是i+1, 按照我们的规定，i+1这个元素被选中的概率是k/i+1，也即第 i+1 这个元素在蓄水池中出现的概率是k/i+1 此时考虑前i个元素，如果前i个元素出现在蓄水池中的概率都是k/i+1的话，说明我们的算法是没有问题的。 对这个问题可以用归纳法来证明：k < i <=N： 1.当i=k+1的时候，蓄水池的容量为k，第k+1个元素被选择的概率明显为k/(k+1), 此时前k个元素出现在蓄水池的概率为 k/(k+1), 很明显结论成立。 2.假设当 j=i 的时候结论成立，此时以 k/i 的概率来选择第i个元素，前i-1个元素出现在蓄水池的概率都为k/i。 证明当j=i+1的情况： 即需要证明当以 k/i+1 的概率来选择第i+1个元素的时候，此时任一前i个元素出现在蓄水池的概率都为k/(i+1). 前i个元素出现在蓄水池的概率有2部分组成, ①在第i+1次选择前得出现在蓄水池中，②得保证第i+1次选择的时候不被替换掉 ①.由2知道在第i+1次选择前，任一前i个元素出现在蓄水池的概率都为k/i ②.考虑被替换的概率： 首先要被替换得第 i+1 个元素被选中(不然不用替换了)概率为 k/i+1，其次是因为随机替换的池子中k个元素中任意一个，所以不幸被替换的概率是 1/k，故 前i个元素(池中元素)中任一被替换的概率 = k/(i+1) * 1/k = 1/i+1 则(池中元素中)没有被替换的概率为: 1 - 1/(i+1) = i/i+1 综合① ②,通过乘法规则 得到前i个元素出现在蓄水池的概率为 k/i * i/(i+1) = k/i+1 故证明成立 伪代码 Init : a reservoir with the size ： k for i = k + 1 to N M = random ( 1 , i ); if ( M < k ) SWAP the Mth value and ith value end for 加权分布式蓄水池抽样 有时候我们的蓄水池中的数据是有权重，算法希望数据被抽样选中的概率和该数据的权重成正比。2005年Pavlos S. Efraimidis和Paul G. Spirakis的论文 Weighted random sampling with a reservoir 提供了对于加权状态下这一问题的解决方案。他的解法既简单又优雅，基本思想和上面的分布式蓄水池抽样一致：对于每个数据计算一个0-1的值R，并求r的n次方根作为该数据的新的R值。这里的n就是该数据的权重。最终算法返回前k个R值最高的数据然后返回。根据计算规则，权重越大的数据计算所得的R值越接近1，所以越有可能被返回。 Best practice python实现的普通蓄水池算法。 python version ### Definition for singly-linked list. ### class ListNode(object): ### def __init__(self, x): ### self.val = x ### self.next = None import random class Solution ( object ): def __init__ ( self , head ): \"\"\" @param head The linked list's head. Note that the head is guaranteed to be not null, so it contains at least one node. :type head: ListNode \"\"\" self . head = head self . k = 1 #随机选出的数量 def getRandom ( self ): \"\"\" Returns a random node's value. :rtype: int \"\"\" count = 0 node = self . head for _ in range ( k ): result . append ( node . val ) node = node . next while node : count += 1 #为linklist数量计数，视为i r = random . randint ( 1 , count ) if r <= self . k : #以k/i的概率来选择 result [ r ] = node . val #这里没有用交换操作，会有数据丢失 node = node . next return self . result ### Your Solution object will be instantiated and called as such: ### obj = Solution(head) ### param_1 = obj.getRandom() Mark: 362 ms 这是维基百科上关于加权蓄水池算法的R语言实现。 In many applications sampling is required to be according to the weights that are assigned to each items available in set. For example, it might be required to sample queries in a search engine with weight as number of times they were performed so that the sample can be analyzed for overall impact on user experience. There are two ways to interpret weights assigned to each item in the set: 1. Let the weight of each item be $$ {\\displaystyle w_{i}} w_{i} $$ and sum of all weights be W. We can convert weight to probability of item getting selected in sample as $$ {\\displaystyle P_{i}=w_{i}/W} $$ . 2. Let the weight of two items i and j be $$ {\\displaystyle w_{i}} w_{i} and {\\displaystyle w_{j}} w_{j} $$ . Let the probability of item i getting selected in sample be $$ {\\displaystyle p_{i}} p_{i }$$ , then we give $$ {\\displaystyle p_{j}=\\min(1,p_{i}{\\frac {w_{j}}{w_{i}}})} {\\displaystyle p_{j}=\\min(1,p_{i}{\\frac {w_{j}}{w_{i}}})} $$ . Algorithm A-Res ( * S is a stream of items to sample , R will contain the result S.Current returns current item in stream S.Weight returns weight of current item in stream S.Next advances stream to next position The power operator is represented by &#94; min - priority - queue supports : Count -> number of items in priority queue Minimum () -> returns minimum key value of all items Extract - Min () -> Remove the item with minimum key Insert ( key , Item ) -> Adds item with specified key * ) ReservoirSample ( S [ 1.. ? ], R [ 1.. k ]) H = new min - priority - queue while S has data r = Random ( 0 , 1 ) &#94; ( 1 / S.Weight ) // important : inclusive range if H.Count < k H.Insert ( r , S.Current ) else if H.Minimum < r H.Extract - Min () H.Insert ( r , S.Current ) S.Next Additional 参考文献： 1. Reservoir sampling 2. 蓄水池抽样及实现 3. 数据工程师必知算法：蓄水池抽样 if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Python,R","url":"leetcode-linked-list-random-node.html"},{"title":"Pythonic的泛函数","text":"Description 因为python不支持重载方法或函数，所以我们无法像java那样用不同的签名定义某个方法的变体来实现用不同的方式处理不同的数据类型，导致这个区别的根本原因是python语言的动态性。但也正因得益于此，利用动态参数类型以及字典参数列表，在python中我们常常可以把方法变成一个分派函数，使用一串if/elif/else调用专门的函数来实现类java重载的功能。然而这样并不便于模块的用户拓展，还显得很愚蠢：实现一长，分派函数会变得很大，而且它与各个专门函数之间的耦合也很紧密。 我们这里的泛型函数是指由一组为不同类型参数执行相似操作的函数组成的函数，具体调用哪一个函数的实现取决于分发算法和参数类型（也即是python中的参数列表类型重载）。在这里我们例举了 Fluent Python 里的singledispath和 Python Cookbook 里的利用函数注解实现方法重载的例子。来展现函数重载这一静态类型特性在python这个动态类型语言中的实现以及局限。对于这个特性实现牵扯到了python的多种元特性让它成为了一个实在有趣的话题。 Python单分发器（Singledispatch）是实现泛型函数的一种形式，由一个单一参数来决定选择和调用哪个函数。在Python3.4中，singledispath方法第一出现在functools模块中，你可以在 PEP 443 — Single-dispatch generic function 找到关于它更多特性的介绍。 在 Python Cookbook 中介绍了一个不完美但可行的利用函数注解方式实现方法重载的方法，分别用元函数和与singledispath类似的装饰器实现。这是一个比较深入的关于函数注解的应用。你也可以在之前的博文中找到一些比较不深入的对于函数注解的 应用 。 代码示例 Singledispatch Python 3.4 新增的 functools.singledispatch 装饰器可以把整体方案拆分成多个模块，甚至可以为你无法修改的类提供专门的函数。使用 @singledispatch 装饰的普通函数会变成泛函数（generic function）：根据第一个参数的类型，以不同方式执行相同操作的一组函数。以下是fluent python中关于singledispatch的示例。 from functools import singledispatch from collections import abc import numbers import html @singledispatch # @singledispatch 标记处理 object 类型的基函数。 def htmlize ( obj ): content = html . escape ( repr ( obj )) return '<pre>{}</pre>' . format ( content ) @htmlize.register ( str ) # 各个专门函数使用 @«base_function».register(«type») 装饰。 def _ ( text ): # 专门函数的名称无关紧要；_ 是个不错的选择，简单明了。 content = html . escape ( text ) . replace ( ' \\n ' , '<br> \\n ' ) return '<p>{0}</p>' . format ( content ) @htmlize.register ( numbers . Integral ) # 为每个需要特殊处理的类型注册一个函数。numbers.Integral 是 int 的虚拟超类。 def _ ( n ): return '<pre>{0} (0x{0:x})</pre>' . format ( n ) @htmlize.register ( tuple ) # 可以叠放多个 register 装饰器，让同一个函数支持不同类型。 @htmlize.register ( abc . MutableSequence ) def _ ( seq ): inner = '</li> \\n <li>' . join ( htmlize ( item ) for item in seq ) return '<ul> \\n <li>' + inner + '</li> \\n </ul>' singledispatch 机制的一个显著特征是，你可以在系统的任何地方和任何模块中注册专门函数。如果后来在新的模块中定义了新的类型，可以轻松地添加一个新的专门函数来处理那个类型。此外，你还可以为不是自己编写的或者不能修改的类添加自定义函数。 Multidispatch 这是两个python cookbook中利用annotation对多参数方法的重载的示例。在第一个示例中我们利用元类来生成支持参数重载的类。在__prepare__方法中将类的字典变成自定义的MultiDict字典。 使用元类实现 import inspect import types class MultiMethod : ''' Represents a single multimethod. ''' def __init__ ( self , name ): self . _methods = {} self . __name__ = name def register ( self , meth ): ''' Register a new method as a multimethod ''' sig = inspect . signature ( meth ) #利用inpsect模块来获取函数签名 # Build a type signature from the method's annotations types = [] for name , parm in sig . parameters . items (): if name == 'self' : continue if parm . annotation is inspect . Parameter . empty : raise TypeError ( 'Argument {} must be annotated with a type' . format ( name ) ) if not isinstance ( parm . annotation , type ): raise TypeError ( 'Argument {} annotation must be a type' . format ( name ) ) if parm . default is not inspect . Parameter . empty : self . _methods [ tuple ( types )] = meth types . append ( parm . annotation ) self . _methods [ tuple ( types )] = meth def __call__ ( self , * args ): ''' Call a method based on type signature of the arguments ''' types = tuple ( type ( arg ) for arg in args [ 1 :]) meth = self . _methods . get ( types , None ) if meth : return meth ( * args ) else : raise TypeError ( 'No matching method for types {}' . format ( types )) def __get__ ( self , instance , cls ): ''' Descriptor method needed to make calls work in a class ''' if instance is not None : return types . MethodType ( self , instance ) else : return self class MultiDict ( dict ): ''' Special dictionary to build multimethods in a metaclass ''' def __setitem__ ( self , key , value ): if key in self : # If key already exists, it must be a multimethod or callable current_value = self [ key ] if isinstance ( current_value , MultiMethod ): current_value . register ( value ) else : mvalue = MultiMethod ( key ) #装饰符来控制mvalue行为 mvalue . register ( current_value ) mvalue . register ( value ) super () . __setitem__ ( key , mvalue ) else : super () . __setitem__ ( key , value ) class MultipleMeta ( type ): ''' Metaclass that allows multiple dispatch of methods ''' def __new__ ( cls , clsname , bases , clsdict ): return type . __new__ ( cls , clsname , bases , dict ( clsdict )) @classmethod def __prepare__ ( cls , clsname , bases ): return MultiDict () 为了使用这个类，你可以像下面这样写： class Spam ( metaclass = MultipleMeta ): def bar ( self , x : int , y : int ): print ( 'Bar 1:' , x , y ) def bar ( self , s : str , n : int = 0 ): print ( 'Bar 2:' , s , n ) ### Example: overloaded __init__ import time class Date ( metaclass = MultipleMeta ): def __init__ ( self , year : int , month : int , day : int ): self . year = year self . month = month self . day = day def __init__ ( self ): t = time . localtime () self . __init__ ( t . tm_year , t . tm_mon , t . tm_mday ) 测试结果 >>> s = Spam () >>> s . bar ( 2 , 3 ) Bar 1 : 2 3 >>> s . bar ( 'hello' ) Bar 2 : hello 0 >>> s . bar ( 'hello' , 5 ) Bar 2 : hello 5 >>> s . bar ( 2 , 'hello' ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"multiple.py\" , line 42 , in __call__ raise TypeError ( 'No matching method for types {}' . format ( types )) TypeError : No matching method for types ( < class ' int '>, <class ' str '>) >>> # Overloaded __init__ >>> d = Date ( 2012 , 12 , 21 ) >>> # Get today's date >>> e = Date () >>> e . year 2012 >>> e . month 12 >>> e . day 3 装饰器实现 作为使用元类和注解的一种替代方案，可以通过描述器来实现类似的效果。 import types class multimethod : def __init__ ( self , func ): self . _methods = {} self . __name__ = func . __name__ self . _default = func def match ( self , * types ): def register ( func ): ndefaults = len ( func . __defaults__ ) if func . __defaults__ else 0 for n in range ( ndefaults + 1 ): self . _methods [ types [: len ( types ) - n ]] = func return self return register def __call__ ( self , * args ): types = tuple ( type ( arg ) for arg in args [ 1 :]) meth = self . _methods . get ( types , None ) if meth : return meth ( * args ) else : return self . _default ( * args ) def __get__ ( self , instance , cls ): if instance is not None : return types . MethodType ( self , instance ) else : return self 为了使用描述器版本，你需要像下面这样写： class Spam : @multimethod def bar ( self , * args ): # Default method called if no match raise TypeError ( 'No matching method for bar' ) @bar.match ( int , int ) def bar ( self , x , y ): print ( 'Bar 1:' , x , y ) @bar.match ( str , int ) def bar ( self , s , n = 0 ): print ( 'Bar 2:' , s , n ) 缺陷 本节的实现中的主要思路其实是很简单的。 MutipleMeta 元类使用它的 __prepare__() 方法 来提供一个作为 MultiDict 实例的自定义字典。这个跟普通字典不一样的是， MultiDict 会在元素被设置的时候检查是否已经存在，如果存在的话，重复的元素会在 MultiMethod 实例中合并。 MultiMethod 实例通过构建从类型签名到函数的映射来收集方法。 在这个构建过程中，函数注解被用来收集这些签名然后构建这个映射。 这个过程在 MultiMethod.register() 方法中实现。 这种映射的一个关键特点是对于多个方法，所有参数类型都必须要指定，否则就会报错。 为了让 MultiMethod 实例模拟一个调用，它的 call () 方法被实现了。 这个方法从所有排除 self 的参数中构建一个类型元组，在内部map中查找这个方法， 然后调用相应的方法。为了能让 MultiMethod 实例在类定义时正确操作， get ()` 是必须得实现的。 它被用来构建正确的绑定方法。 不过本节的实现还有一些限制，其中一个是它不能使用关键字参数。 同样对于继承也是有限制的，例如，类似下面这种代码就不能正常工作： class A : pass class B ( A ): pass class C : pass class Spam ( metaclass = MultipleMeta ): def foo ( self , x : A ): print ( 'Foo 1:' , x ) def foo ( self , x : C ): print ( 'Foo 2:' , x ) 原因是因为 x:A 注解不能成功匹配子类实例（比如B的实例），如下： >>> s = Spam () >>> a = A () >>> s . foo ( a ) Foo 1 : < __main__ . A object at 0x1006a5310 > >>> c = C () >>> s . foo ( c ) Foo 2 : < __main__ . C object at 0x1007a1910 > >>> b = B () >>> s . foo ( b ) Traceback ( most recent call last ): File \"<stdin>\" , line 1 , in < module > File \"multiple.py\" , line 44 , in __call__ raise TypeError ( 'No matching method for types {}' . format ( types )) TypeError : No matching method for types ( < class ' __main__ . B '>,) Additional","tags":"Python","url":"pythonicde-fan-han-shu.html"},{"title":"Python Memoryview and Annotation Ideas","text":"Description Memoryview (内存视图)是一个内置类，它能让用户在不复制内容的情况下操作同一个数组的不同切片。memoryview 的概念受到了 NumPy 的启发（参见 2.9.3 节）。Travis Oliphant 是 NumPy 的主要作者，他在回答\" When should a memoryview be used?\" [link](http://stackoverflow.com/questions/4845418/when-should-a-memoryview-be-used/）这个问题时是这样说的： 内存视图其实是泛化和去数学化的 NumPy 数组。它让你在不需要复制内容的前提下，在数据结构之间共享内存。其中数据结构可以是任何形式，比如 PIL 图片、SQLite 数据库和 NumPy 的数组，等等。这个功能在处理大型数据集合的时候非常重要。 memoryview.cast 的概念跟数组模块类似，能用不同的方式读写同一块内存数据，而且内容字节不会随意移动。这听上去又跟 C 语言中类型转换的概念差不多。memoryview.cast 会把同一块内存里的内容打包成一个全新的 memoryview 对象给你。 Python 3 提供了一种句法，用于为函数声明中的参数和返回值附加元数据。 参考文献2 中描述了python引入annotations的一种解释。 Python 对注解所做的唯一的事情是，把它们存储在函数的 annotations 属性里。仅此而已，Python 不做检查、不做强制、不做验证，什么操作都不做。换句话说，注解对 Python 解释器没有任何意义。注解只是元数据，可以供 IDE、框架和装饰器等工具使用。即便没有强制语义，我们依然可以利用注解来规范函数输入。 使用Memoryview导入文件字符流 在这里我们比较了三种读取字节流的方式。后两种方法能减少一次数据的拷贝，但令人惊讶的是，使用memoryview并没有提高读取程序的效率和速度。 如果要进行本地测试的话，请将FILENAME改为你本地环境中的大文件的路径。 代码： import os from time import time ### 大文件路径 FILENAME = r 'C:\\Users\\chu060\\Downloads\\ubuntu-16.04.2-desktop-i386.iso' def test_load_file_copy (): \"\"\"使用普通的读取字节流的方式，该方式会进行一次拷贝\"\"\" f = open ( FILENAME , 'rb' ) buf = bytearray ( f . read ()) f . close () return buf [: 100 ] def test_load_file_mv (): \"\"\"memoryview测试\"\"\" f = open ( FILENAME , 'rb' ) buf = bytearray ( os . path . getsize ( FILENAME )) mv = memoryview ( buf ) f . readinto ( mv ) f . close () return buf [: 100 ] def test_load_file_ba (): \"\"\"无memoryview，使用bytearray\"\"\" f = open ( FILENAME , 'rb' ) buf = bytearray ( os . path . getsize ( FILENAME )) f . readinto ( buf ) f . close () return buf [: 100 ] def load_tester ( func , n = 3 ): \"\"\"进行测试并输出结果\"\"\" print ( '=' * 50 ) start = time () for i in range ( n ): result = func () if i == 0 : print ( result ) print ( 'try {test_times} times, {name} avg running time: {avg_time}' . format ( test_times = n , name = func . __name__ , avg_time = str (( time () - start ) / n ))) ### 获取所有待测函数 test_funcs = [ globals ()[ name ] for name in globals () if name . startswith ( 'test' )] ### 进行测试,此处取十次测试的平均值 for func in test_funcs : load_tester ( func , 10 ) source file 运行结果为： ================================================== bytearray(b'3\\xed\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x9 0\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x903\\xed\\xfa\\x8e\\xd5\\xbc\\x 00|\\xfb\\xfcf1\\xdbf1\\xc9fSfQ\\x06W\\x8e\\xdd\\x8e\\xc5R\\xbe\\x00|\\xbf\\x00\\x06\\xb9\\x00\\x 01\\xf3\\xa5\\xeaK\\x06\\x00\\x00R\\xb4A\\xbb\\xaaU1\\xc90\\xf6\\xf9\\xcd\\x13r\\x16\\x81\\xfbU\\x aau\\x10\\x83\\xe1\\x01t') try 10 times, test_load_file_ba avg running time: 0.9056999921798706 ================================================== bytearray(b'3\\xed\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x9 0\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x903\\xed\\xfa\\x8e\\xd5\\xbc\\x 00|\\xfb\\xfcf1\\xdbf1\\xc9fSfQ\\x06W\\x8e\\xdd\\x8e\\xc5R\\xbe\\x00|\\xbf\\x00\\x06\\xb9\\x00\\x 01\\xf3\\xa5\\xeaK\\x06\\x00\\x00R\\xb4A\\xbb\\xaaU1\\xc90\\xf6\\xf9\\xcd\\x13r\\x16\\x81\\xfbU\\x aau\\x10\\x83\\xe1\\x01t') try 10 times, test_load_file_copy avg running time: 1.3784000158309937 ================================================== bytearray(b'3\\xed\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x9 0\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x90\\x903\\xed\\xfa\\x8e\\xd5\\xbc\\x 00|\\xfb\\xfcf1\\xdbf1\\xc9fSfQ\\x06W\\x8e\\xdd\\x8e\\xc5R\\xbe\\x00|\\xbf\\x00\\x06\\xb9\\x00\\x 01\\xf3\\xa5\\xeaK\\x06\\x00\\x00R\\xb4A\\xbb\\xaaU1\\xc90\\xf6\\xf9\\xcd\\x13r\\x16\\x81\\xfbU\\x aau\\x10\\x83\\xe1\\x01t') try 10 times, test_load_file_mv avg running time: 0.9180000066757202 使用annotations进行参数检查 我们可以简单将annotations(注解)当成函数参数说明文档。注解会被储存在函数的__annotations__属性中，因此我们也可以利用这个字典来做更多的事情：比如参数类型、有效性的检验。 代码： def validate ( func , locals ): for var , test in func . __annotations__ . items (): value = locals [ var ] msg = 'Var: {0} \\t Value: {1} \\t Test: {2.__name__}' . format ( var , value , test ) assert test ( value ), msg def is_int ( x ): return isinstance ( x , int ) def between ( lo , hi ): def _between ( x ): return lo <= x <= hi return _between def f ( x : between ( 3 , 10 ), y : is_int ): validate ( f , locals ()) print ( x , y ) 结果： >>> f(0, 31.1) Traceback (most recent call last): ... AssertionError: Var: y Value: 31.1 Test: is_int Additional 参考文献: 1. Less Copies in Python with the Buffer Protocol and memoryviews 2. Memoryview Q&A","tags":"Python","url":"python-memoryview-and-annotation-ideas.html"},{"title":"Python Descriptor Behavior","text":"Description 描述符是对多个属性运用相同存取逻辑的一种方式。例如，Django ORM 和 SQL Alchemy 等 ORM 中的字段类型是描述符，把数据库记录中字段里的数据与 Python 对象的属性对应起来。 描述符是实现了特定协议的类，这个协议包括 get 、 set 和 delete 方法。property 类实现了完整的描述符协议。通常，可以只实现部分协议。其实，我们在真实的代码中见到的大多数描述符只实现了 get 和 set 方法，还有很多只实现了其中的一个。 描述符是 Python 的独有特征，不仅在应用层中使用，在语言的基础设施中也有用到。除了特性之外，使用描述符的 Python 功能还有方法及 classmethod 和 staticmethod 装饰器。理解描述符是精通 Python 的关键。 本文在简要介绍描述符的基础上探讨描述符在python中的显隐性。 描述符种类 在正式探讨描述符在python中的显隐性前，先了解下三种描述符。 覆盖型描述符 实现 set 方法的描述符属于覆盖型描述符，因为虽然描述符是类属性，但是实现 set 方法的话，会覆盖对实例属性的赋值操作。 没有 get 方法的覆盖型描述符 通常，覆盖型描述符既会实现 set 方法，也会实现 get 方法，不过也可以只实现 set 方法，此时，只有写操作由描述符处理。通过实例读取描述符会返回描述符对象本身，因为没有处理读操作的 get 方法。如果直接通过实例的 dict 属性创建同名实例属性，以后再设置那个属性时，仍会由 set 方法插手接管，但是读取那个属性的话，就会直接从实例中返回新赋予的值，而不会返回描述符对象。也就是说，实例属性会遮盖描述符，不过只有读操作是如此。 非覆盖型描述符 没有实现 set 方法的描述符是非覆盖型描述符。如果设置了同名的实例属性，描述符会被遮盖，致使描述符无法处理那个实例的那个属性。方法是以非覆盖型描述符实现的。 Python属性查找策略 优先找到Python自动产生的属性。 查找obj. class . dict ，如果attr存在并且是覆盖型描述符，返回覆盖型描述符的__get__方法的结果，如果没有继续在obj. class 的父类以及祖先类中寻找覆盖型描述符。 在obj. dict 中查找，这一步分两种情况，第一种情况是obj是一个普通实例，找到就直接返回，找不到进行下一步。第二种情况是obj是一个类，依次在obj和它的父类、祖先类的__dict__中查找，如果找到一个描述符就返回描述符的__get__方法的结果，否则直接返回attr。如果没有找到，进行下一步。 在obj. class . dict 中查找，如果找到了一个描述符(这里的描述符一定是非覆盖型描述符)，返回描述符的__get__方法的结果。如果找到一个普通属性，直接返回属性值。如果没有继续在obj. class 的父类以及祖先类中寻找非覆盖型描述符。 很不幸，Python终于受不了。在这一步，它raise AttributeError 。 代码验证 接下来我们编写示例代码来对以上策略进行验证。 \"\"\" 覆盖型描述符: >>> obj = Model() >>> obj.__dict__['over'] = 'obj instance property over' >>> obj.over # doctest: +ELLIPSIS Overriding.__get__() invoked with args: self = <descriptorkinds.Overriding object at 0x...> instance = <descriptorkinds.Model object at 0x...> owner = <class 'descriptorkinds.Model'> >>> Model.over # doctest: +ELLIPSIS Overriding.__get__() invoked with args: self = <descriptorkinds.Overriding object at 0x...> instance = None owner = <class 'descriptorkinds.Model'> >>> sub_obj = SubClass() >>> sub_obj.__dict__['over'] = 'sub_obj instance property over' >>> sub_obj.over # doctest: +ELLIPSIS Overriding.__get__() invoked with args: self = <descriptorkinds.Overriding object at 0x...> instance = <descriptorkinds.SubClass object at 0x...> owner = <class 'descriptorkinds.SubClass'> >>> SubClass.over # doctest: +ELLIPSIS Overriding.__get__() invoked with args: self = <descriptorkinds.Overriding object at 0x...> instance = None owner = <class 'descriptorkinds.SubClass'> #行为完全符合策略2。额外的，这里测试了获取类属性时传入参数的值。 没有 __get__ 方法的覆盖型描述符: >>> obj.__dict__['over_no_get'] = 'obj instance property over_no_get' >>> obj.over_no_get # doctest: +ELLIPSIS 'obj instance property over_no_get' >>> sub_obj.over_no_get # doctest: +ELLIPSIS <descriptorkinds.OverridingNoGet object at 0x...> >>> sub_obj.__dict__['over_no_get'] = 'sub_obj instance property over_no_get' >>> sub_obj.over_no_get # doctest: +ELLIPSIS 'sub_obj instance property over_no_get' #行为符合策略4。其行为更像非覆盖型描述符。但无法直接使用obj.over_no_get的方式给实例属性赋值。 非覆盖型描述符： >>> obj.non_over # doctest: +ELLIPSIS NonOverriding.__get__() invoked with args: self = <descriptorkinds.NonOverriding object at 0x...> instance = <descriptorkinds.Model object at 0x...> owner = <class 'descriptorkinds.Model'> >>> obj.non_over = 'obj instance property non_over' >>> obj.non_over # doctest: +ELLIPSIS 'obj instance property non_over' >>> sub_obj.non_over # doctest: +ELLIPSIS NonOverriding.__get__() invoked with args: self = <descriptorkinds.NonOverriding object at 0x...> instance = <descriptorkinds.SubClass object at 0x...> owner = <class 'descriptorkinds.SubClass'> >>> sub_obj.__dict__['non_over'] = 'sub_obj instance property non_over' >>> sub_obj.non_over # doctest: +ELLIPSIS 'sub_obj instance property non_over' #行为符合策略4。 \"\"\" ### BEGIN DESCRIPTORKINDS def print_args ( name , * args ): # <1> cls_name = args [ 0 ] . __class__ . __name__ arg_names = [ 'self' , 'instance' , 'owner' ] if name == 'set' : arg_names [ - 1 ] = 'value' print ( '{}.__{}__() invoked with args:' . format ( cls_name , name )) for arg_name , value in zip ( arg_names , args ): print ( ' {:8} = {}' . format ( arg_name , value )) class Overriding : # <2> \"\"\"a.k.a. data descriptor or enforced descriptor\"\"\" def __get__ ( self , instance , owner ): print_args ( 'get' , self , instance , owner ) # <3> def __set__ ( self , instance , value ): print_args ( 'set' , self , instance , value ) class OverridingNoGet : # <4> \"\"\"an overriding descriptor without ``__get__``\"\"\" def __set__ ( self , instance , value ): print_args ( 'set' , self , instance , value ) class NonOverriding : # <5> \"\"\"a.k.a. non-data or shadowable descriptor\"\"\" def __get__ ( self , instance , owner ): print_args ( 'get' , self , instance , owner ) class Model : # <6> over = Overriding () over_no_get = OverridingNoGet () non_over = NonOverriding () def spam ( self ): # <7> print ( 'Model.spam() invoked with arg:' ) print ( ' self =' , self ) class SubClass ( Model ): def spam ( self ): print ( 'SubClass.spam() invoked with arg:' ) print ( ' self =' , self ) 另外，在类中定义的函数属于绑定方法（bound method），因为用户定义的函数都有 get 方法，所以依附到类上时，就相当于描述符。 \"\"\" ### BEGIN FUNC_DESCRIPTOR_DEMO >>> word = Text('forward') >>> word # <1> Text('forward') >>> word.reverse() # <2> Text('drawrof') >>> Text.reverse(Text('backward')) # <3> Text('drawkcab') >>> type(Text.reverse), type(word.reverse) # <4> (<class 'function'>, <class 'method'>) >>> list(map(Text.reverse, ['repaid', (10, 20, 30), Text('stressed')])) # <5> ['diaper', (30, 20, 10), Text('desserts')] >>> Text.reverse.__get__(word) # <6> <bound method Text.reverse of Text('forward')> >>> Text.reverse.__get__(None, Text) # <7> <function Text.reverse at 0x101244e18> >>> word.reverse # <8> <bound method Text.reverse of Text('forward')> >>> word.reverse.__self__ # <9> Text('forward') >>> word.reverse.__func__ is Text.reverse # <10> True ### END FUNC_DESCRIPTOR_DEMO \"\"\" ### BEGIN FUNC_DESCRIPTOR_EX import collections class Text ( collections . UserString ): def __repr__ ( self ): return 'Text({!r})' . format ( self . data ) def reverse ( self ): return self [:: - 1 ] ### END FUNC_DESCRIPTOR_EX 总结 描述符的作用方式可用以下函数表示： x = C () x . foo ==> if hasattr ( C , 'foo' ): d = C . foo ; D = d . __class__ if hasattr ( D , '__get__' ) and ( hasattr ( D , '__set__' ) or 'foo' not in x . __dict__ ): return D . __get__ ( d , x , C ) return x . __dict__ [ 'foo' ] # or from C, &c Additional 描述符用法建议： 1.使用特性以保持简单 内置的 property 类创建的其实是覆盖型描述符， set 方法和 get 方法都实现了，即便不定义设值方法也是如此。特性的 set 方法默认抛出 AttributeError: can't set attribute，因此创建只读属性最简单的方式是使用特性，这能避免下一条所述的问题。 2.只读描述符必须有 set 方法 如果使用描述符类实现只读属性，要记住， get 和 set 两个方法必须都定义，否则，实例的同名属性会遮盖描述符。只读属性的 set 方法只需抛出 AttributeError 异常，并提供合适的错误消息。 3.用于验证的描述符可以只有 set 方法 对仅用于验证的描述符来说， set 方法应该检查 value 参数获得的值，如果有效，使用描述符实例的名称为键，直接在实例的 dict 属性中设置。这样，从实例中读取同名属性的速度很快，因为不用经过 get 方法处理。 4 仅有 get 方法的描述符可以实现高效缓存 如果只编写了 get 方法，那么创建的是非覆盖型描述符。这种描述符可用于执行某些耗费资源的计算，然后为实例设置同名属性，缓存结果。同名实例属性会遮盖描述符，因此后续访问会直接从实例的 dict 属性中获取值，而不会再触发描述符的 get 方法。 5.非特殊的方法可以被实例属性遮盖 由于函数和方法只实现了 get 方法，它们不会处理同名实例属性的赋值操作。因此，像 my_obj.the_method = 7 这样简单赋值之后，后续通过该实例访问 the_method 得到的是数字 7——但是不影响类或其他实例。然而，特殊方法不受这个问题的影响。解释器只会在类中寻找特殊的方法，也就是说，repr(x) 执行的其实是 x. class . repr (x)，因此 x 的 repr 属性对 repr(x) 方法调用没有影响。出于同样的原因，实例的 getattr 属性不会破坏常规的属性访问规则。","tags":"Python","url":"python-descriptor-behavior.html"},{"title":"Python Coroutine Example -- Rock Paper Scissors","text":"Description 本代码来自Ian Ward的Jupyter Notebook -- \"Iterables, Iterators, and Generators\" 教程，实现了剪刀石头布的游戏。 预激协程 The flow is always the same when working with generators. a generator object is created by the caller the caller starts the generator the generator passes data to the caller (or signals the end of the sequence) the caller passes data to the generator repeat from (3) For generators that are driven by input to .send() no data is transferred in the first 3 steps above. This is a decorator that arranges for .next() to be called once immediately after a generator is created. This will turn a generator function into a function that returns a generator immediately ready to receive data (step 4). def advance_generator_once ( original_fn ): \"decorator to advance a generator once immediately after it is created\" def actual_call ( * args , ** kwargs ): gen = original_fn ( * args , ** kwargs ) assert gen . next () is None return gen return actual_call 实例协程 As shown, one of the ways to pass a message to a generator is with .send(). This interface allows you to pass a single object to a generator. For this object we can pass tuples, dicts or anything else we choose. You decide the protocol for your generator by documenting the types and values of objects you will send from caller to generator and yield from generator to caller. Tuples are perfect for a generator that needs two objects each time, e.g. a player number and a key press. This is a Rock-Paper-Scissors game where each player's play is passed in separately, and once both players have played the result of the game is yielded. Players can change their mind choose a different play if the other player hasn't chosen yet. Games will continue indefinitately. This generator uses a common pattern of storing the result that will be yielded in a local variable so that there are fewer yield statements in the generator function. Having fewer yield statements makes it easier to understand where it is possible for execution to be paused within the generator function. The outer while loop runs once for each full game. The inner while loop collects input from the users until the game result can be decided. @advance_generator_once def rock_paper_scissors (): \"\"\" coroutine for playing rock-paper-scissors yields: 'invalid key': invalid input was sent ('win', player, choice0, choice1): when a player wins ('tie', None, choice0, choice1): when there is a tie None: when waiting for more input accepts to .send(): (player, key): player is 0 or 1, key is a character in 'rps' \"\"\" valid = 'rps' wins = 'rs' , 'sp' , 'pr' result = None while True : chosen = [ None , None ] while None in chosen : player , play = yield result result = None if play in valid : chosen [ player ] = play else : result = 'invalid key' if chosen [ 0 ] + chosen [ 1 ] in wins : result = ( 'win' , 0 ) + tuple ( chosen ) elif chosen [ 1 ] + chosen [ 0 ] in wins : result = ( 'win' , 1 ) + tuple ( chosen ) else : result = ( 'tie' , None ) + tuple ( chosen ) Additional 参考文献: 1. 源教程","tags":"Python","url":"python-coroutine-example-rock-paper-scissors.html"},{"title":"Python Coroutine Example -- Game of life","text":"Description 本文所列代码实现了 John Conway 发明的\"生命游戏\"（https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life），使用协程管理游戏运行过程中各个细胞的状态。 源自Brett Slatkin 写的《Effective Python：编写高质量 Python 代码的 59 个有效方法》一书。 代码及注释 from collections import namedtuple ALIVE = '*' EMPTY = '-' TICK = object () Query = namedtuple ( 'Query' , 'y x' ) Transition = namedtuple ( 'Transition' , 'y x state' ) def count_neighbors ( y , x ): \"\"\" 获取邻居状态的子协程。 返回存活邻居的数量给step_cell的neighbor变量。 \"\"\" n_ = yield Query ( y + 1 , x + 0 ) # North ne = yield Query ( y + 1 , x + 1 ) # Northeast e_ = yield Query ( y + 0 , x + 1 ) # East se = yield Query ( y - 1 , x + 1 ) # Southeast s_ = yield Query ( y - 1 , x + 0 ) # South sw = yield Query ( y - 1 , x - 1 ) # Southwest w_ = yield Query ( y + 0 , x - 1 ) # West nw = yield Query ( y + 1 , x - 1 ) # Northwest neighbor_states = [ n_ , ne , e_ , se , s_ , sw , w_ , nw ] count = 0 for state in neighbor_states : if state == ALIVE : count += 1 return count def game_logic ( state , neighbors ): \"\"\" 细胞转换的条件。 \"\"\" if state == ALIVE : if neighbors < 2 : return EMPTY # Die: Too few elif neighbors > 3 : return EMPTY # Die: Too many else : if neighbors == 3 : return ALIVE # Regenerate return state def step_cell ( y , x ): \"\"\" 先抛出目标细胞自身，再逐个抛出邻居。 获得以上所有细胞的状态之后再根据细胞转换条件抛出目标细胞 在下一世代的状态。 \"\"\" state = yield Query ( y , x ) neighbors = yield from count_neighbors ( y , x ) next_state = game_logic ( state , neighbors ) yield Transition ( y , x , next_state ) def simulate ( height , width ): \"\"\" 为gird的每个元素抛出其自身及其邻居给客户程序。 TICK为终止条件。 \"\"\" while True : for y in range ( height ): for x in range ( width ): yield from step_cell ( y , x ) yield TICK class Grid ( object ): \"\"\" 细胞生活的地方。实现了getitem和setitem方法方便live_a_generation方法操作。 \"\"\" def __init__ ( self , height , width ): self . height = height self . width = width self . rows = [] for _ in range ( self . height ): self . rows . append ([ EMPTY ] * self . width ) def __str__ ( self ): output = '' for row in self . rows : for cell in row : output += cell output += ' \\n ' return output def __getitem__ ( self , position ): y , x = position return self . rows [ y % self . height ][ x % self . width ] def __setitem__ ( self , position , state ): y , x = position self . rows [ y % self . height ][ x % self . width ] = state def live_a_generation ( grid , sim ): \"\"\" 一世代的生命模拟。 \"\"\" progeny = Grid ( grid . height , grid . width ) item = next ( sim ) #TICK为一世代的模拟的结束信号。 while item is not TICK : #如果协程抛出的是query，则传入这一世代该query对应细胞的存活情况 #如果是transition则表示对某一细胞的转换状态判断已完成，改变其存活情况 if isinstance ( item , Query ): state = grid [ item . y , item . x ] item = sim . send ( state ) else : # Must be a Transition progeny [ item . y , item . x ] = item . state item = next ( sim ) return progeny class ColumnPrinter ( object ): \"\"\" 将测试结果打印出来。 columns的每个元素是每次模拟后的结果grid。 \"\"\" def __init__ ( self ): self . columns = [] def append ( self , data ): self . columns . append ( data ) def __str__ ( self ): row_count = 1 for data in self . columns : row_count = max ( row_count , len ( data . splitlines ()) + 1 ) rows = [ '' ] * row_count print ( rows ) for j in range ( row_count ): for i , data in enumerate ( self . columns ): line = data . splitlines ()[ max ( 0 , j - 1 )] if j == 0 : rows [ j ] += str ( i ) . center ( len ( line )) else : rows [ j ] += line if ( i + 1 ) < len ( self . columns ): rows [ j ] += ' | ' return ' \\n ' . join ( rows ) def main (): \"\"\" 测试代码。 初始生成的grid为： ---*----- ----*---- --***---- --------- --------- \"\"\" grid = Grid ( 5 , 9 ) grid [ 0 , 3 ] = ALIVE grid [ 1 , 4 ] = ALIVE grid [ 2 , 2 ] = ALIVE grid [ 2 , 3 ] = ALIVE grid [ 2 , 4 ] = ALIVE columns = ColumnPrinter () sim = simulate ( grid . height , grid . width ) for i in range ( 10 ): columns . append ( str ( grid )) grid = live_a_generation ( grid , sim ) print ( columns ) if __name__ == \"__main__\" : main () Additional 参考文献: 1. 原书示例 2. 原书代码","tags":"Python","url":"python-coroutine-example-game-of-life.html"},{"title":"Flunet Python 16.7 coroutine understading","text":"Description 援引自《Fluent Python》16.7节中关于调用方通过yield from委派生成器调用子生成器的例子。因为一个代码细节造成了对于委派生成器理解上的困难，因此基于个人的理解做出相应地修改来优化该段代码。 原例 from collections import namedtuple Result = namedtuple ( 'Result' , 'count average' ) ### the subgenerator def averager (): # <1> total = 0.0 count = 0 average = None while True : term = yield # <2> if term is None : # <3> break total += term count += 1 average = total / count return Result ( count , average ) # <4> ### the delegating generator def grouper ( results , key ): # <5># <6> while True : results [ key ] = yield from averager () # <7> ### the client code, a.k.a. the caller def main ( data ): # <8> results = {} for key , values in data . items (): group = grouper ( results , key ) # <9> next ( group ) # <10> for value in values : group . send ( value ) # <11> group . send ( None ) # important! <12> #print(results) # uncomment to debug report ( results ) ### output report def report ( results ): for key , result in sorted ( results . items ()): group , unit = key . split ( ';' ) print ( '{:2} {:5} averaging {:.2f}{}' . format ( result . count , group , result . average , unit )) data = { 'girls;kg' : [ 40.9 , 38.5 , 44.3 , 42.2 , 45.2 , 41.7 , 44.5 , 38.0 , 40.6 , 44.5 ], 'girls;m' : [ 1.6 , 1.51 , 1.4 , 1.3 , 1.41 , 1.39 , 1.33 , 1.46 , 1.45 , 1.43 ], 'boys;kg' : [ 39.0 , 40.8 , 43.2 , 40.8 , 43.1 , 38.6 , 41.4 , 40.6 , 36.3 ], 'boys;m' : [ 1.38 , 1.5 , 1.32 , 1.25 , 1.37 , 1.48 , 1.25 , 1.49 , 1.46 ], } 与示例 16-13 中的 averager 协程一样。这里作为子生成器使用。 main 函数中的客户代码发送的各个值绑定到这里的 term 变量上。 至关重要的终止条件。如果不这么做，使用 yield from 调用这个协程的生成器会永远阻塞。 返回的 Result 会成为 grouper 函数中 yield from 表达式的值。 grouper 是委派生成器。 这个循环每次迭代时会新建一个 averager 实例；每个实例都是作为协程使用的生成器对象。 grouper 发送的每个值都会经由 yield from 处理，通过管道传给 averager 实例。grouper 会在 yield from 表达式处暂停，等待 averager 实例处理客户端发来的值。averager 实例运行完毕后，返回的值绑定到 results[key] 上。while 循环会不断创建 averager 实例，处理更多的值。 main 函数是客户端代码，用 PEP 380 定义的术语来说，是\"调用方\"。这是驱动一切的函数。 group 是调用 grouper 函数得到的生成器对象，传给 grouper 函数的第一个参数是 results，用于收集结果；第二个参数是某个键。group 作为协程使用。 预激 group 协程。 把各个 value 传给 grouper。传入的值最终到达 averager 函数中 term = yield 那一行；grouper 永远不知道传入的值是什么。 把 None 传入 grouper，导致当前的 averager 实例终止，也让 grouper 继续运行，再创建一个 averager 实例，处理下一组值。 示例 16-17 中最后一个标号前面有个注释——\"重要！\"，强调这行代码（group.send(None)）至关重要：终止当前的 averager 实例，开始执行下一个。如果注释掉那一行，这个脚本不会输出任何报告。此时，把 main 函数靠近末尾的 print(results) 那行的注释去掉，你会发现，results 字典是空的。 源码 理解与勘误 注解 6 表示在委派生成器中将会循环创建averager迭代器。这一点让我在理解起来产生了极大的困惑。因为以我之见对应每个key，委托迭代器只会在一个子迭代器中遍历并计算平均值。 遂我们将while True语句去掉，当main函数的for循环内给委托迭代器send None的时候便会抛出StopIteration异常。 之后将grouper生成器改写成如下便可成功运行。 def grouper ( results , key ): results [ key ] = yield from averager () yield 在这里接受到None之后子迭代器averager迭代器也能成功返值Result，委托迭代器在第二个yield处阻塞所以不会抛出StopIteration异常。 当然也可以将while语句去掉后在main函数加入异常处理来捕获StopIteration异常，这样还能知道每个子迭代器停止的时间。 def main ( data ): results = {} for key , values in data . items (): group = grouper ( results , key ) next ( group ) for value in values : group . send ( value ) try : group . send ( None ) except StopIteration as ex : print ( 'end of child iterater' ) 假如使用源程序的while true这段代码的话，对于每个key子迭代器会生成两遍，当然第二遍的子迭代器是不会使用的。 下面简要说明示例的运作方式，还会说明把 main 函数中调用 group.send(None) 那一行代码（带有\"重要！\"注释的那一行）去掉会发生什么事。 外层 for 循环每次迭代会新建一个 grouper 实例，赋值给 group 变量；group 是委派生成器。 调用 next(group)，预激委派生成器 grouper，此时进入 while True 循环，调用子生成器 averager 后，在 yield from 表达式处暂停。 内层 for 循环调用 group.send(value)，直接把值传给子生成器 averager。同时，当前的 grouper 实例（group）在 yield from 表达式处暂停。 内层循环结束后，group 实例依旧在 yield from 表达式处暂停，因此，grouper 函数定义体中为 results[key] 赋值的语句还没有执行。 如果外层 for 循环的末尾没有 group.send(None)，那么 averager 子生成器永远不会终止，委派生成器 group 永远不会再次激活，因此永远不会为 results[key] 赋值。 外层 for 循环重新迭代时会新建一个 grouper 实例，然后绑定到 group 变量上。前一个 grouper 实例（以及它创建的尚未终止的 averager 子生成器实例）被垃圾回收程序回收。 Additional 参考文献: 1. 流畅的python","tags":"Python","url":"flunet-python-167-coroutine-understading.html"},{"title":"Linklist Problems Summary","text":"Description 链表问题是经常会遇到的数据结构问题。该类问题的难点主要集中再单向遍历以及链表有环的情况下。很多链表问题只要利用两个速度不一的指针来完成单向遍历就可以简化问题、减少复杂度。本文基于一些常见的链表问题来分析比较良好的方案以及python实现。 准备工作 在解决问题之前，我们先要定义这次使用的链表的数据结构。 在此我们定义了一个普通的value和next指针的链表结构，并重写了__len__魔术方法来计算无环链表自身的长度，添加了traversal方法来帮助我们遍历链表验证结论。 在SampleLinklist类中我们定义了一些链表示例来辅助我们验证结论。 ### -*- coding: utf-8 -*- class Linklist ( object ): def __init__ ( self , value , next ): self . value = value self . next = next def traversal ( self ): \"\"\" 遍历该节点开始的链表 :return: \"\"\" print ( self . value ) if self . next : self . next . traversal () def __bool__ ( self ): \"\"\" 用于之后的if Linklist判断 :return: \"\"\" return True def __len__ ( self ): \"\"\" :return: 无环链表的长度 \"\"\" count = 0 node = self while node is not None : node = node . next count += 1 return count class SampleLinklist ( object ): def __init__ ( self ): # 创建一个值为0-19的链表数组 nodelist = [ Linklist ( i , None ) for i in range ( 20 )] # 0-6为无环单链表；7-11为有环单链表（入口为8）； # 12-14交于无环单链表(入口为5)；15-19交于有环链表（入口为9）； for key , value in enumerate ( nodelist ): if key != 6 and key != 11 and key != 14 and key != 19 : value . next = nodelist [ key + 1 ] elif key == 11 : value . next = nodelist [ 8 ] elif key == 14 : value . next = nodelist [ 5 ] elif key == 19 : value . next = nodelist [ 9 ] #无环单链表 self . normal_linklist = nodelist [ 0 ] #有环单链表（入口为8） self . loop_linklist = nodelist [ 7 ] #与normal_linklist交于5 self . intersect_normal_linklist = ( nodelist [ 0 ], nodelist [ 12 ]) # 与nloop_linklist交于9 self . intersect_loop_linklist = ( nodelist [ 7 ], nodelist [ 15 ]) 解决问题 在O(1)时间删除链表节点 题目描述： 给定链表的头指针和一个节点指针，在O(1)时间删除该节点。 分析： 本题与《编程之美》上的「从无头单链表中删除节点」类似。主要思想都是「狸猫换太子」，即用下一个节点数据覆盖要删除的节点，然后删除下一个节点。但是如果节点是尾节点时，python无法直接在内存中删除传入函数的对象，暂时还没找到办法解决。 ### -*- coding: utf-8 -*- from linklist import SampleLinklist def delete_specified_node ( node ): assert ( node != None ) if node . next != None : storeNode = node . next node . value = node . next . value node . next = node . next . next 反转单链表 题目描述： 输入一个单向链表，输出逆序反转后的链表。 分析： 链表的转置是一个很常见、很基础的数据结构题了。我们在这里运用了递归算法，将尾节点冒泡返回，然后对每个节点的指针反置。 ### -*- coding: utf-8 -*- from linklist import SampleLinklist def revert_linklist ( node ): if not node or not node . next : # 返回链表尾节点为，既反转后链表头结点 return node # 暂存头结点 header = revert_linklist ( node . next ) node . next . next = node node . next = None return header 找到单链表倒数第k个节点 题目描述： 输入一个单向链表，输出该链表中倒数第k个节点，链表的倒数第0个节点为链表的尾指针。 分析： 设置两个指针 p1、p2，首先 p1 和 p2 都指向 head，然后 p2 向前走 k 步，这样 p1 和 p2 之间就间隔 k 个节点，最后 p1 和 p2 同时向前移动，直至 p2 走到链表末尾。 ### -*- coding: utf-8 -*- from linklist import SampleLinklist def find_last_nth ( header , n ): if not header or n < 0 : return None # 将fast和slow指针设至链表起点 fast = slow = header # 将fast走n个节点 while fast . next and n > 0 : fast = fast . next n -= 1 # n大于链表长度的情况 if n > 0 : return None while fast . next : fast = fast . next slow = slow . next return slow 找到单链表中间节点 题目描述： 求链表的中间节点，如果链表的长度为偶数，返回中间两个节点的任意一个，若为奇数，则返回中间节点。 分析： 此题的解决思路和第3题「求链表的倒数第 k 个节点」很相似。可以先求链表的长度， 然后计算出中间节点所在链表顺序的位置。但是如果要求只能扫描一遍链表，如何解决呢？ 最高效的解法和第3题一样，通过两个指针来完成。用两个指针从链表头节点开始，一个指针每次向后移动两步，一个每次移动一步，直到快指针移到到尾节点，那么慢指针即是所求。 ### -*- coding: utf-8 -*- from linklist import SampleLinklist def find_mid ( header ): if not header : return None # 将fast和slow指针设至链表起点 fast = slow = header while fast and fast . next : #fast速度为slow两倍 fast = fast . next . next slow = slow . next return slow 判断单链表是否有环 题目描述： 输入一个单向链表，判断链表是否有环。如果链表存在环，如何找到环的入口点？ 分析： 通过两个指针，分别从链表的头节点出发，一个每次向后移动一步，另一个移动两步，两个指针移动速度不一样，如果存在环，那么两个指针一定会在环里相遇。按照 p2 每次两步，p1 每次一步的方式走，发现 p2 和 p1 重合，确定了单向链表有环路了。接下来，让p2回到链表的头部，重新走，每次步长不是走2了，而是走1，那么当 p1 和 p2 再次相遇的时候，就是环路的入口了。为什么？：假定起点到环入口点的距离为 a，p1 和 p2 的相交点M与环入口点的距离为b，环路的周长为L，当 p1 和 p2 第一次相遇的时候，假定 p1 走了 n 步。那么有：p1走的路径： a+b ＝ n；p2走的路径： a+b+k L = 2 n； p2 比 p1 多走了k圈环路，总路程是p1的2倍根据上述公式可以得到 k*L=a+b=n显然，如果从相遇点M开始，p1 再走 n 步的话，还可以再回到相遇点，同时p2从头开始走的话，经过n步，也会达到相遇点M。显然在这个步骤当中 p1 和 p2只有前 a 步走的路径不同，所以当 p1 和 p2 再次重合的时候，必然是在链表的环路入口点上。 ### -*- coding: utf-8 -*- from linklist import SampleLinklist def is_loop ( header ): if not header : return False fast = slow = header while fast and fast . next : fast = fast . next . next slow = slow . next # 找到两步长交汇点 if fast == slow : break if fast != slow : return None # 将fast发配回起点 fast = header #当两节点再次相遇的时候则为环入口 while not fast == slow : fast = fast . next slow = slow . next return fast 判断两个单链表是否相交 题目描述： 给出两个链表的头指针，判断其是否相交。 分析： 如果两个无环链表相交，则其尾指针一定相同；如果两个有环链表相交，则两个链表都有共同一个环，即环上的任意一个节点都存在于两个链表上。因此，就可以判断一链表上俩指针相遇的那个节点，在不在另一条链表上。 ### -*- coding: utf-8 -*- from linklist import SampleLinklist from whether_linklist_has_loop import is_loop def is_intersect ( header1 , header2 ): if not header1 or not header2 : return False if not is_loop ( header1 ) and not is_loop ( header2 ): # 两个无环链表的尾节点是否相等决定了它们是否相交 while header1 : header1 = header1 . next while header2 : header2 = header2 . next return True if header1 == header2 else False elif not is_loop ( header1 ) or not is_loop ( header2 ): return False else : # 两个带环链表的入口必然在它们的环内 intersect1 = is_loop ( header1 ) intersect2 = is_loop ( header2 ) node = intersect2 . next # 如果任意链表环中有另一列表的节点则相交 while node != intersect2 : if node == intersect1 : return True node = node . next return False 找到链表相交点 题目描述： 如果两个单链表相交，怎么求出他们相交的第一个节点呢？ 分析： 当两链表无环时，则可采用对齐的思想。计算两个链表的长度 L1 , L2，分别用两个指针 p1 , p2 指向两个链表的头，然后将较长链表的 p1（假设为 p1）向后移动L2 - L1个节点，然后再同时向后移动p1 , p2，直到 p1 = p2。相遇的点就是相交的第一个节点。当两链表有环时，如果个环入口相等，则可看成以环入口为尾节点的无环情况。如果不等，则首公共节点为两个入口较近的那个。 ### -*- coding: utf-8 -*- from linklist import SampleLinklist def find_intersect_first_common ( header1 , header2 ): len1 = len ( header1 ) len2 = len ( header2 ) if len1 > len2 : for i in range ( len1 - len2 ): header1 = header1 . next else : for i in range ( len2 - len1 ): header2 = header2 . next while header1 : if header1 == header2 : return header1 header1 = header1 . next header2 = header2 . next return None 源码下载 Additional 参考文献: 1. 面试精选：链表问题集锦 2. 流畅的python","tags":"Python","url":"linklist-problems-summary.html"},{"title":"LeetCode - Next Greater Element II","text":"Description Given a circular array (the next element of the last element is the first element of the array), print the Next Greater Number for every element. The Next Greater Number of a number x is the first greater number to its traversing-order next in the array, which means you could search circularly to find its next greater number. If it doesn't exist, output -1 for this number. Example 1: Input : [ 1 , 2 , 1 ] Output : [ 2 , - 1 , 2 ] Explanation : The first 1 's next greater number is 2; The number 2 can' t find next greater number ; The second 1 ' s next greater number needs to search circularly , which is also 2 . Note: The length of given array won't exceed 10000. Source link Best practice TBD C++ version -- TBD Mark: 0ms 使用栈来判断各个元素next greater的元素。用1至2n序号以及取余的方法来遍历两遍列表，模拟循环数组的情况 python version class Solution ( object ): def nextGreaterElements ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[int] \"\"\" length = len ( nums ) stack , res = [], [ - 1 ] * length for i in range ( length * 2 ): #如果出现大于栈顶序号得元素值的元素时， #就可以疯狂出掉所有小于该元素的所有元素序号 while stack and ( nums [ stack [ - 1 ]] < nums [ i % length ]): res [ stack . pop ()] = nums [ i % length ] #仅仅将每个元素的序号入栈一次 if i < length : stack . append ( i ) return res Mark: 275ms Additional","tags":"Python","url":"leetcode-next-greater-element-ii.html"},{"title":"LeetCode - Database Problems Summary","text":"Description Here is a summary of LeetCode Database section. Source link 删除重复数据 Write a SQL query to delete all duplicate email entries in a table named Person, keeping only unique emails based on its smallest Id. +----+------------------+ | Id | Email | +----+------------------+ | 1 | john@example.com | | 2 | bob@example.com | | 3 | john@example.com | +----+------------------+ Id is the primary key column for this table. For example, after running your query, the above Person table should have the following rows: +----+------------------+ | Id | Email | +----+------------------+ | 1 | john@example.com | | 2 | bob@example.com | +----+------------------+ 删除重复数据是非常普遍和基础的操作。首先我们可以用连接自身以及where条件过滤来完成。 delete p1 from Person p1,Person p2 where p1.Email=p2.Email and p1.id>p2.id 用子查询和min选出每个同名组中最小的id号，删除除了这些id以外的行。 delete from Person where id not in (select b.id from (select min(id) as id from Person group by Email) b) 排序问题 Write a SQL query to rank scores. If there is a tie between two scores, both should have the same ranking. Note that after a tie, the next ranking number should be the next consecutive integer value. In other words, there should be no \"holes\" between ranks. +----+-------+ | Id | Score | +----+-------+ | 1 | 3.50 | | 2 | 3.65 | | 3 | 4.00 | | 4 | 3.85 | | 5 | 4.00 | | 6 | 3.65 | +----+-------+ For example, given the above Scores table, your query should generate the following report (order by highest score): +-------+------+ | Score | Rank | +-------+------+ | 4.00 | 1 | | 4.00 | 1 | | 3.85 | 2 | | 3.65 | 3 | | 3.65 | 3 | | 3.50 | 4 | +-------+------+ 这里用结果集中大于等于当前score的distinct行数来表示某分数在整个结果集中的序列。 select s.Score, (select count(*) from (select distinct Score from Scores) s2 where s2.Score>=s.Score) Rank from Scores s order by Rank asc 分组比较问题 The Trips table holds all taxi trips. Each trip has a unique Id, while Client_Id and Driver_Id are both foreign keys to the Users_Id at the Users table. Status is an ENUM type of (‘completed', ‘cancelled_by_driver', ‘cancelled_by_client'). +----+-----------+-----------+---------+--------------------+----------+ | Id | Client_Id | Driver_Id | City_Id | Status |Request_at| +----+-----------+-----------+---------+--------------------+----------+ | 1 | 1 | 10 | 1 | completed |2013-10-01| | 2 | 2 | 11 | 1 | cancelled_by_driver|2013-10-01| | 3 | 3 | 12 | 6 | completed |2013-10-01| | 4 | 4 | 13 | 6 | cancelled_by_client|2013-10-01| | 5 | 1 | 10 | 1 | completed |2013-10-02| | 6 | 2 | 11 | 6 | completed |2013-10-02| | 7 | 3 | 12 | 6 | completed |2013-10-02| | 8 | 2 | 12 | 12 | completed |2013-10-03| | 9 | 3 | 10 | 12 | completed |2013-10-03| | 10 | 4 | 13 | 12 | cancelled_by_driver|2013-10-03| +----+-----------+-----------+---------+--------------------+----------+ The Users table holds all users. Each user has an unique Users_Id, and Role is an ENUM type of (‘client', ‘driver', ‘partner'). +----------+--------+--------+ | Users_Id | Banned | Role | +----------+--------+--------+ | 1 | No | client | | 2 | Yes | client | | 3 | No | client | | 4 | No | client | | 10 | No | driver | | 11 | No | driver | | 12 | No | driver | | 13 | No | driver | +----------+--------+--------+ Write a SQL query to find the cancellation rate of requests made by unbanned clients between Oct 1, 2013 and Oct 3, 2013. For the above tables, your SQL query should return the following rows with the cancellation rate being rounded to two decimal places. +------------+-------------------+ | Day | Cancellation Rate | +------------+-------------------+ | 2013-10-01 | 0.33 | | 2013-10-02 | 0.00 | | 2013-10-03 | 0.50 | +------------+-------------------+ 将数据集以时间分组，利用case when 来判断是否计入取消订单数。也可以用if语句。 select t.Request_at as Day, round((sum(case when t.Status in (\"cancelled_by_driver\",\"cancelled_by_client\") then 1 else 0 end)/count(*)),2) as \"Cancellation Rate\" from Trips t inner join Users u on t.Client_Id =u.Users_Id where u.Banned <> \"Yes\" and t.Request_at between \"2013-10-01\" and \"2013-10-03\" group by t.Request_at The Employee table holds all employees. Every employee has an Id, and there is also a column for the department Id. +----+-------+--------+--------------+ | Id | Name | Salary | DepartmentId | +----+-------+--------+--------------+ | 1 | Joe | 70000 | 1 | | 2 | Henry | 80000 | 2 | | 3 | Sam | 60000 | 2 | | 4 | Max | 90000 | 1 | | 5 | Janet | 69000 | 1 | | 6 | Randy | 85000 | 1 | +----+-------+--------+--------------+ The Department table holds all departments of the company. +----+----------+ | Id | Name | +----+----------+ | 1 | IT | | 2 | Sales | +----+----------+ Write a SQL query to find employees who earn the top three salaries in each of the department. For the above tables, your SQL query should return the following rows. +------------+----------+--------+ | Department | Employee | Salary | +------------+----------+--------+ | IT | Max | 90000 | | IT | Randy | 85000 | | IT | Joe | 70000 | | Sales | Henry | 80000 | | Sales | Sam | 60000 | +------------+----------+--------+ 取top3的分组数据和取最大分组salary有所不同。后者可以用where salary in （select max(salary) ..group by ）子查询语句来实现。 这里运用了排序问题中的思想。将当前salary与同一group中（表现为departmentid相等），大于等于当前distinct的salary的数量作为该salary的rank。其他限制条件就迎刃而解了。 select d.Name Department, e1.Name Employee, e1.Salary from Employee e1 join Department d on e1.DepartmentId = d.Id where 3 > (select count(distinct(e2.Salary)) from Employee e2 where e2.Salary > e1.Salary and e1.DepartmentId = e2.DepartmentId );","tags":"SQL","url":"leetcode-database-problems-summary.html"},{"title":"LeetCode - Construct the Rectangle","text":"Description For a web developer, it is very important to know how to design a web page's size. So, given a specific rectangular web page's area, your job by now is to design a rectangular web page, whose length L and width W satisfy the following requirements: The area of the rectangular web page you designed must equal to the given target area. The width W should not be larger than the length L, which means L >= W. The difference between length L and width W should be as small as possible. You need to output the length L and the width W of the web page you designed in sequence. Example: Input : 4 Output : [ 2 , 2 ] Explanation : The target area is 4 , and all the possible ways to construct it are [ 1 , 4 ], [ 2 , 2 ], [ 4 , 1 ]. But according to requirement 2 , [ 1 , 4 ] is illegal ; according to requirement 3 , [ 4 , 1 ] is not optimal compared to [ 2 , 2 ]. So the length L is 2 , and the width W is 2 . Note: 1. The given area won't exceed 10,000,000 and is a positive integer 2. The web page's width and length you designed must be positive integers. Source link Best practice 其实很简单，放上来是因为记录一下python和c各自不同的解决方式。然后c的0ms实在是太夸张了。 C++ version class Solution { public : vector < int > constructRectangle ( int area ) { int ceil = floor ( sqrt ( area )); while ( area % ceil != 0 ) { -- ceil ; } return vector < int > ({ area / ceil , ceil }); } }; Mark: 0ms python version import math class Solution ( object ): def constructRectangle ( self , area ): \"\"\" :type area: int :rtype: List[int] \"\"\" for i in range ( math . floor ( math . sqrt ( area )), 0 , - 1 ): if area % i == 0 : return [ int ( area / i ), i ] Mark: 46ms Additional","tags":"C,Python","url":"leetcode-construct-the-rectangle.html"},{"title":"LeetCode - Move Zeroes","text":"Description Given an array nums , write a function to move all 0 's to the end of it while maintaining the relative order of the non-zero elements. For example, given nums = [0, 1, 0, 3, 12] , after calling your function, nums should be [1, 3, 12, 0, 0] . Note: 1. You must do this in-place without making a copy of the array. 2. Minimize the total number of operations. Source link Best practice 每当遇到一个0位就将之后的数字向前移动x位（x为遇到的0的个数），最后将后x位置为0。 C++ version class Solution { public : void moveZeroes ( vector < int >& nums ) { int count = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ){ if ( nums [ i ] == 0 ){ ++ count ; } else { nums [ i - count ] = nums [ i ]; } } for ( int i = 0 ; i < count ; ++ i ){ nums [ nums . size () - 1 - i ] = 0 ; } } }; Mark: 16ms 用一个变量j来记录非0位应在的位置，最后处理末置0位。 python version class Solution ( object ): def moveZeroes ( self , nums ): \"\"\" :type nums: List[int] :rtype: void Do not return anything, modify nums in-place instead. \"\"\" j = 0 for item in nums : if item != 0 : nums [ j ] = item j += 1 for index in range ( j , len ( nums )): nums [ index ] = 0 Mark: 59ms Additional","tags":"C,Python","url":"leetcode-move-zeroes.html"},{"title":"LeetCode - Add One Row to Tree","text":"Description Given the root of a binary tree, then value v and depth d , you need to add a row of nodes with value v at the given depth d . The root node is at depth 1. The adding rule is: given a positive integer depth d , for each NOT null tree nodes N in depth d-1 , create two tree nodes with value v as N's left subtree root and right subtree root. And N's original left subtree should be the left subtree of the new left subtree root, its original right subtree should be the right subtree of the new right subtree root. If depth d is 1 that means there is no depth d-1 at all, then create a tree node with value v as the new root of the whole original tree, and the original tree is the new root's left subtree. Example 1: Input : A binary tree as following : 4 / \\ 2 6 / \\ / 3 1 5 v = 1 d = 2 Output : 4 / \\ 1 1 / \\ 2 6 / \\ / 3 1 5 Example 2: Input : A binary tree as following : 4 / 2 / \\ 3 1 v = 1 d = 3 Output : 4 / 2 / \\ 1 1 / \\ 3 1 Note: 1. The given d is in range [1, maximum depth of the given tree + 1]. 2. The given binary tree has at least one tree node. Source link Best practice 题目中d=1时实为一种特殊情况————整颗树变为左子树。相似的当d=0时整棵树相应会变成右子树嘛。以此推理可以得以下终极解决方法。将d=0和d=1作为递归终止条件从而将d=1的特殊情况概化，又省去了helper函数。 C++ version class Solution { public : TreeNode * addOneRow ( TreeNode * root , int v , int d ) { if ( d == 0 || d == 1 ) { TreeNode * newroot = new TreeNode ( v ); ( d ? newroot -> left : newroot -> right ) = root ; return newroot ; } if ( root && d >= 2 ) { root -> left = addOneRow ( root -> left , v , d > 2 ? d - 1 : 1 ); root -> right = addOneRow ( root -> right , v , d > 2 ? d - 1 : 0 ); } return root ; } }; Mark: 16ms 这个python版本显得非常不pythonic。helper函数增加了一个长度参数记录了递归树深度，其实可以用使参数d递减的方式来传递深度。 python version ### Definition for a binary tree node. ### class TreeNode(object): ### def __init__(self, x): ### self.val = x ### self.left = None ### self.right = None class Solution : def addOneRow ( self , root , v , d ): \"\"\" :type root: TreeNode :type v: int :type d: int :rtype: TreeNode \"\"\" if d == 1 : a = TreeNode ( v ) a . left = root return a return self . helper ( root , v , d , 1 ) def helper ( self , node , v , d , dep ): if not node : return if dep == d - 1 : a = TreeNode ( v ) b = TreeNode ( v ) a . left = node . left b . right = node . right node . left = a node . right = b self . helper ( node . left , v , d , dep + 1 ) self . helper ( node . right , v , d , dep + 1 ) return node Mark: 82ms Additional","tags":"C,Python","url":"leetcode-add-one-row-to-tree.html"},{"title":"LeetCode - Array Nesting","text":"Description A zero-indexed array A consisting of N different integers is given. The array contains all integers in the range [0, N - 1]. Sets S[K] for 0 <= K < N are defined as follows: S[K] = { A[K], A[A[K]], A[A[A[K]]], ... }. Sets S[K] are finite for each K and should NOT contain duplicates. Write a function that given an array A consisting of N integers, return the size of the largest set S[K] for this array. Example 1: Input : A = [ 5 , 4 , 0 , 3 , 1 , 6 , 2 ] Output : 4 Explanation : A [ 0 ] = 5 , A [ 1 ] = 4 , A [ 2 ] = 0 , A [ 3 ] = 3 , A [ 4 ] = 1 , A [ 5 ] = 6 , A [ 6 ] = 2 . One of the longest S [ K ]: S [ 0 ] = { A [ 0 ], A [ 5 ], A [ 6 ], A [ 2 ]} = { 5 , 6 , 2 , 0 } Note: 1. N is an integer within the range [1, 20,000]. 2. The elements of A are all distinct. 3. Each element of array A is an integer within the range [0, N-1]. Source link Best practice 这里用到了一个之前用过的入栈遍历方式，取max时候的运算还可以进行优化。 Python version 1 class Solution ( object ): def arrayNesting ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" length = len ( nums ) def helper ( stack ): for item in stack : #忽略被标记的元素 if nums [ item ] < length : stack . append ( nums [ item ]) #将所有遍历过的元素的序号+length做唯一标记 nums [ item ] += length return len ( stack ) - 1 #遍历所有未被标记的元素 return max ([ helper ([ item ]) for item in nums if item < length ]) Mark: 85ms 这是一个比较普适性的方法，创建一个记录列表。 python version 2 class Solution ( object ): def arrayNesting ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" ans , step , n = 0 , 0 , len ( nums ) seen = [ False ] * n for i in range ( n ): while not seen [ i ]: seen [ i ] = True i , step = nums [ i ], step + 1 ans = max ( ans , step ) step = 0 return ans Mark: 85ms Additional","tags":"Python","url":"leetcode-array-nesting.html"},{"title":"LeetCode - Construct String from Binary Tree","text":"Description You need to construct a string consists of parenthesis and integers from a binary tree with the preorder traversing way. The null node needs to be represented by empty parenthesis pair \"()\". And you need to omit all the empty parenthesis pairs that don't affect the one-to-one mapping relationship between the string and the original binary tree. Example 1: Input : Binary tree : [ 1 , 2 , 3 , 4 ] 1 / \\ 2 3 / 4 Output : \"1(2(4))(3)\" Explanation : Originallay it needs to be \"1(2(4)())(3()())\" , but you need to omit all the unnecessary empty parenthesis pairs . And it will be \"1(2(4))(3)\" . Example 2: Input : Binary tree : [ 1 , 2 , 3 , null , 4 ] 1 / \\ 2 3 \\ 4 Output : \"1(2()(4))(3)\" Explanation : Almost the same as the first example , except we can ' t omit the first parenthesis pair to break the one - to - one mapping relationship between the input and the output . Source link Best practice 递归方法。 C++ version class Solution { public : string tree2str ( TreeNode * t ) { if ( ! t ) return \"\" ; if ( ! t -> left &&! t -> right ) return to_string ( t -> val ); if ( ! t -> left ) return to_string ( t -> val ) + \"()(\" + tree2str ( t -> right ) + \")\" ; if ( ! t -> right ) return to_string ( t -> val ) + \"(\" + tree2str ( t -> left ) + \")\" ; return to_string ( t -> val ) + \"(\" + tree2str ( t -> left ) + \")(\" + tree2str ( t -> right ) + \")\" ; } }; Mark: 12ms python version class Solution ( object ): def tree2str ( self , t ): \"\"\" :type t: TreeNode :rtype: str \"\"\" if not t : return \"\" leftstring = \"({})\" . format ( self . tree2str ( t . left )) if t . left or ( not t . left and t . right ) else \"\" rightstring = \"({})\" . format ( self . tree2str ( t . right )) if t . right else \"\" return \"{}{}{}\" . format ( t . val , leftstring , rightstring ) Mark: 88ms Additional","tags":"C,Python","url":"leetcode-construct-string-from-binary-tree.html"},{"title":"LeetCode - Sort Characters by Frequency","text":"Description Given a string, sort it in decreasing order based on the frequency of characters. Example 1: Input: \"tree\" Output: \"eert\" Explanation: 'e' appears twice while 'r' and 't' both appear once. So 'e' must appear before both 'r' and 't'. Therefore \"eetr\" is also a valid answer. Example 2: Input: \"cccaaa\" Output: \"cccaaa\" Explanation: Both 'c' and 'a' appear three times, so \"aaaccc\" is also a valid answer. Note that \"cacaca\" is incorrect, as the same characters must be together. Example 3: Input: \"Aabb\" Output: \"bbAa\" Explanation: \"bbaA\" is also a valid answer, but \"Aabb\" is incorrect. Note that 'A' and 'a' are treated as two different characters. Source link Best practice 使用队列。 C++ version class Solution { public : string frequencySort ( string s ) { vector < int > m ( 256 , 0 ); priority_queue < pair < int , char > > pq ; for ( int i = 0 ; i < s . length (); i ++ ) m [ s [ i ]] ++ ; for ( int i = 0 ; i < 256 ; i ++ ) pq . push ( make_pair ( m [ i ], i )); string ans ; while ( pq . size ()){ ans . append ( pq . top (). first , pq . top (). second ); pq . pop (); } return ans ; } }; Mark: 12ms python版本使用了sorted函数并通过opterator库的itemgetter函数来提高检索字典效率。string乘法则是python的另一个特性。 python version from operator import itemgetter class Solution ( object ): def frequencySort ( self , s ): \"\"\" :type s: str :rtype: str \"\"\" dct = {} result = \"\" for item in s : dct [ item ] = dct . get ( item , 0 ) + 1 for item in sorted ( dct . items (), key = itemgetter ( 1 ), reverse = True ): result += item [ 0 ] * item [ 1 ] return result Mark: 76ms Additional","tags":"C,Python","url":"leetcode-sort-characters-by-frequency.html"},{"title":"Complex Number Multiplication","text":"Description Given two strings representing two complex numbers. You need to return a string representing their multiplication. Note $$i&#94;2 = -1$$ according to the definition. Example 1: Input: \"1+1i\", \"1+1i\" Output: \"0+2i\" Explanation: (1 + i) * (1 + i) = 1 + i2 + 2 * i = 2i, and you need convert it to the form of 0+2i. Example 2: Input: \"1+-1i\", \"1+-1i\" Output: \"0+-2i\" Explanation: (1 - i) * (1 - i) = 1 + i2 - 2 * i = -2i, and you need convert it to the form of 0+-2i. Note: 1. The input strings will not have extra blank. 2. The input strings will be given in the form of a+bi , where the integer a and b will both belong to the range of [-100, 100]. And the output should be also in this form. Source link Best practice 在c++中使用Stringstream来提取字符串中的信息。 C++ version class Solution { public : string complexNumberMultiply ( string a , string b ) { int ra , ia , rb , ib ; char buff ; stringstream aa ( a ), bb ( b ), ans ; aa >> ra >> buff >> ia >> buff ; bb >> rb >> buff >> ib >> buff ; ans << ra * rb - ia * ib << \"+\" << ra * ib + rb * ia << \"i\" ; return ans . str (); } }; Mark: Python version -- use re class Solution ( object ): def complexNumberMultiply ( self , a , b ): \"\"\" :type a: str :type b: str :rtype: str \"\"\" pattern = re . compile ( r '([\\-0-9]*)\\+([\\-0-9]*)i' ) la = [ int ( item ) for item in pattern . search ( a ) . groups ()] lb = [ int ( item ) for item in pattern . search ( b ) . groups ()] lc = [ la [ 0 ] * lb [ 0 ] - la [ 1 ] * lb [ 1 ], la [ 0 ] * lb [ 1 ] + la [ 1 ] * lb [ 0 ]] return '{}+{}i' . format ( * lc ) Mark: 36ms Additional if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"C,Python","url":"complex-number-multiplication.html"},{"title":"LeetCode - Merge Two Binary Trees","text":"Description Given two binary trees and imagine that when you put one of them to cover the other, some nodes of the two trees are overlapped while the others are not. You need to merge them into a new binary tree. The merge rule is that if two nodes overlap, then sum node values up as the new value of the merged node. Otherwise, the NOT null node will be used as the node of new tree. Example 1: Input : Tree 1 Tree 2 1 2 / \\ / \\ 3 2 1 3 / \\ \\ 5 4 7 Output : Merged tree : 3 / \\ 4 5 / \\ \\ 5 4 7 Note: The merging process must start from the root nodes of both trees. Source link Best practice 用递归的方法来合并两树：如果一树任意节点不存在则返回另一个树的节点（如果都为空则返回一个空节点，很合理）。如果都存在则生成新节点其值为两树节点值得和。 TreeNode *node =new TreeNode(t1->val + t2->val); 返回一个TreeNode指针。 C++ version class Solution { public : TreeNode * mergeTrees ( TreeNode * t1 , TreeNode * t2 ) { if ( ! t1 ) return t2 ; if ( ! t2 ) return t1 ; TreeNode * node = new TreeNode ( t1 -> val + t2 -> val ); node -> left = mergeTrees ( t1 -> left , t2 -> left ); node -> right = mergeTrees ( t1 -> right , t2 -> right ); return node ; } }; Mark: 10ms Additional","tags":"C","url":"leetcode-merge-two-binary-trees.html"},{"title":"LeetCode - Single Number III","text":"Description Given an array of numbers nums , in which exactly two elements appear only once and all the other elements appear exactly twice. Find the two elements that appear only once. Example: Given nums = [1, 2, 1, 3, 2, 5]``, return ``[3, 5] . Note: The order of the result is not important. So in the above example, [5, 3] is also correct. Your algorithm should run in linear runtime complexity. Could you implement it using only constant space complexity? Source link Best practice Once again, we need to use XOR to solve this problem. But this time, we need to do it in two passes: In the first pass, we XOR all elements in the array, and get the XOR of the two numbers we need to find. Note that since the two numbers are distinct, so there must be a set bit (that is, the bit with value '1') in the XOR result. Find out an arbitrary set bit (for example, the rightmost set bit). In the second pass, we divide all numbers into two groups, one with the aforementioned bit set, another with the aforementinoed bit unset. Two different numbers we need to find must fall into thte two distrinct groups. XOR numbers in each group, we can find a number in either group. C++ version class Solution { public : vector < int > singleNumber ( vector < int >& nums ) { // Pass 1 : //Get the XOR of the two numbers we need to find int diff = accumulate ( nums . begin (), nums . end (), 0 , bit_xor < int > ()); // Get its last set bit diff &= - diff ; // Pass 2 : vector < int > rets = { 0 , 0 }; // this vector stores the two numbers we will return for ( int num : nums ) { if (( num & diff ) == 0 ) // the bit is not set { rets [ 0 ] &#94;= num ; } else // the bit is set { rets [ 1 ] &#94;= num ; } } return rets ; } }; Python version class Solution ( object ): def singleNumber ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[int] \"\"\" diff = reduce ( lambda x , y : x &#94; y , nums ) diff &= - diff a = b = 0 for item in nums : if item & diff : a &#94;= item else : b &#94;= item return [ a , b ] Mark: 49ms Additional","tags":"C","url":"leetcode-single-number-iii.html"},{"title":"Python 单例实现的几种方式","text":"Python Singleton Metaclass Decorator category: Python make all array elements equal, where a move is incrementing a selected element by 1 or decrementing a selected element by 1. Description 单例模式 ，也叫 单子模式 ，是一种常用的软件设计模式。在应用这个模式时，单例对象的类必须保证只有一个实例存在。许多时候整个系统只需要拥有一个的全局对象，这样有利于我们协调系统整体的行为。比如在某个服务器程序中，该服务器的配置信息存放在一个文件中，这些配置数据由一个单例对象统一读取，然后服务进程中的其他对象再通过这个单例对象获取这些配置信息。这种方式简化了在复杂环境下的配置管理。 实现单例模式的思路是：一个类能返回对象一个引用(永远是同一个)和一个获得该实例的方法（必须是静态方法，通常使用getInstance这个名称）；当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用；同时我们还将该类的构造函数定义为私有方法，这样其他处的代码就无法通过调用该类的构造函数来实例化该类的对象，只有通过该类提供的静态方法来得到该类的唯一实例。 单例模式在多线程的应用场合下必须小心使用。如果当唯一实例尚未创建时，有两个线程同时调用创建方法，那么它们同时没有检测到唯一实例的存在，从而同时各自创建了一个实例，这样就有两个实例被构造出来，从而违反了单例模式中实例唯一的原则。 解决这个问题的办法是为指示类是否已经实例化的变量提供一个互斥锁(虽然这样会降低效率)。 Implementation new方法 new方法 class Singleton ( object ): def __new__ ( cls , * args , ** kw ): if not hasattr ( cls , '_instance' ): orig = super ( Singleton , cls ) cls . _instance = orig . __new__ ( cls , * args , ** kw ) return cls . _instance class MyClass ( Singleton ): a = 1 metaclass方法 覆盖 init 类 class Singleton ( type ): def __init__ ( cls , * args , ** kwargs ): cls . __instance = None super () . __init__ ( * args , ** kwargs ) def __call__ ( cls , * args , ** kwargs ): if cls . __instance is None : cls . __instance = super () . __call__ ( * args , ** kwargs ) return cls . __instance class Spam ( metaclass = Singleton ): def __init__ ( self ): print ( 'Creating Spam' ) a = Spam () b = Spam () print ( a is b ) 覆盖 new 类 class Singleton ( type ): def __new__ ( cls , name , base , dct ): dct [ '_instance' ] = None return super () . __new__ ( cls , name , base , dct ) def __call__ ( cls , * args , ** kwargs ): if cls . _instance is None : cls . _instance = super () . __call__ ( * args , ** kwargs ) return cls . _instance class Spam ( metaclass = Singleton ): def __init__ ( self ): print ( 'Creating Spam' ) a = Spam () b = Spam () print ( a is b ) decorator方法 def singleton ( cls , * args , ** kw ): instances = {} def getinstance (): if cls not in instances : instances [ cls ] = cls ( * args , ** kw ) return instances [ cls ] return getinstance @singleton class MyClass : ... import方法 ### mysingleton.py class My_Singleton ( object ): def foo ( self ): pass my_singleton = My_Singleton () ### to use from mysingleton import my_singleton my_singleton . foo () Additional","tags":"posts","url":"python-dan-li-shi-xian-de-ji-chong-fang-shi.html"},{"title":"LeetCode - Minimum Moves to Equal Array Elements II","text":"Description Given a non-empty integer array, find the minimum number of moves required to make all array elements equal, where a move is incrementing a selected element by 1 or decrementing a selected element by 1. You may assume the array's length is at most 10,000 . Example: **Input:** [1,2,3] **Output:** 2 **Explanation:** Only two moves are needed (remember each move increments or decrements one element): [1,2,3] => [2,2,3] => [2,2,2] Source link Best practice 我咋觉得昨天的比较复杂。 需要用到algorithms库的sort函数。中间序号的数的值为数列的目标变化值。Minimum moves等于所有元素与该目标变化值的差值之和。 C++ class Solution { public : int minMoves2 ( vector < int >& nums ) { sort ( nums . begin (), nums . end ()); int mid = nums [ floor ( nums . size () / 2 )]; int result = 0 ; for ( auto i : nums ) { result += abs ( i - mid ); } return result ; } }; Mark: 19ms Additional","tags":"C","url":"leetcode-minimum-moves-to-equal-array-elements-ii.html"},{"title":"LeetCode - Minimum Index Sum of Two Lists","text":"Description Suppose Andy and Doris want to choose a restaurant for dinner, and they both have a list of favorite restaurants represented by strings. You need to help them find out their common interest with the least list index sum . If there is a choice tie between answers, output all of them with no order requirement. You could assume there always exists an answer. Example 1: Input: [\"Shogun\", \"Tapioca Express\", \"Burger King\", \"KFC\"] [\"Piatti\", \"The Grill at Torrey Pines\", \"Hungry Hunter Steakhouse\", \"Shogun\"] Output: [\"Shogun\"] Explanation: The only restaurant they both like is \"Shogun\". Example 2: Input: [\"Shogun\", \"Tapioca Express\", \"Burger King\", \"KFC\"] [\"KFC\", \"Shogun\", \"Burger King\"] Output: [\"Shogun\"] Explanation: The restaurant they both like and have the least index sum is \"Shogun\" with index sum 1 (0+1). Note: 1. The length of both lists will be in the range of [1, 1000]. 2. The length of strings in both lists will be in the range of [1, 30]. 3. The index is starting from 0 to the list length minus 1. 4. No duplicates in both lists. Source link Best practice 首先以第一个列表建一个以index为key，string为item的哈希表。 然后遍历第二个列表，当两个列表的index和比之前小时刷新输出列表，相等时在输出列表后添加第二个列表真的string。 C++ : hash table class Solution { public : int max_int = 2147483647 ; vector < string > findRestaurant ( vector < string >& list1 , vector < string >& list2 ) { vector < string > result ; unordered_map < string , int > hashtable ; int count1 = list1 . size (); int count2 = list2 . size (); for ( int i = 0 ; i < count1 ; i ++ ) { hashtable [ list1 [ i ]] = i ; } for ( int i = 0 ; i < count2 ; i ++ ) { int j = hashtable . count ( list2 [ i ]) > 0 ? hashtable [ list2 [ i ]] : - 1 ; if ( j != - 1 && i + j <= max_int ) { if ( i + j < max_int ) { result . clear (); max_int = i + j ; } result . push_back ( list2 [ i ]); } } return result ; } }; Mark: 92ms Additional","tags":"C","url":"leetcode-minimum-index-sum-of-two-lists.html"},{"title":"LeetCode - Invert binary tree","text":"Description Invert a binary tree. 4 / \\ 2 7 / \\ / \\ 1 3 6 9 to 4 / \\ 7 2 / \\ / \\ 9 6 3 1 Trivia: This problem was inspired by this original tweet by Max Howell: Google: 90% of our engineers use the software you wrote (Homebrew), but you can't invert a binary tree on a whiteboard so fuck off. Source link Best practice 很容易就能想到用递归的方式，实际一做非常简单。被这个老哥怼的谷歌内心一定感到很委屈吧。 C++ -- recursion class Solution { public : TreeNode * invertTree ( TreeNode * root ) { if ( root != NULL ) { invertTree ( root -> left ); invertTree ( root -> right ); swap ( root -> left , root -> right ); } return root ; } }; Mark: 3ms 这是一个用队列储存二叉树节点的非递归算法，可能会比较快一点吧。 C++ -- queue class Solution { public : TreeNode * invertTree ( TreeNode * root ) { queue < TreeNode *> record ; record . push ( root ); while ( ! record . empty ()){ TreeNode * node = record . front (); record . pop (); if ( node != NULL ){ record . push ( node -> left ); record . push ( node -> right ); swap ( node -> left , node -> right ); } } return root ; } }; Mark: 0ms Additional","tags":"C","url":"leetcode-invert-binary-tree.html"},{"title":"总结：如何用位运算来简单高效地解决问题","text":"Question Calculate the sum of two integers a and b, but you are not allowed to use the operator + and - . Example: Given a = 1 and b = 2, return 3. Source link 简介 位运算是指用算法来操作比特或者其他小于一个字母的数据. 常见的需要利用位运算实现的编程任务有：底层设备控制、错误检测和矫正算法、数据压缩、加密算法及优化算法. 对于其他大部分任务而言， 现代编程语言通常允许程序员跟抽象化的位运算交互而非直接使用位运算. 位运算通常包含以下操作符: AND, OR, XOR, NOT和bit shifts. 在某些情况下，由于位操作是并行进行的， 因此通常能够去除或者减少对一个数据格式的循环遍历， 带来运算速度的成倍提升. 但与此同时， 位操作的代码也会更难编写和维护。 详细内容 基础 位操作的核心是位运算符 & (与), | (或), ~ (非) and &#94; (异或) 和 移位操作 a << b and a >> b. 异或运算符没有对应的布尔运算符，在这里我们对它作一个简单的解释. 异或操作符接受两个输入，当且仅当有且只有一个输入为1的时候它才会返回1. 也就是当两个输入不同的时候返回1，相同的时候就会返回0. 异或操作符通常用&#94;符号表示，缩写为XOR. Set 并集 A | B Set 交集 A & B Set 差集 A & ~B Set 非集 ALL_BITS&#94; A or ~A 将A的第bit位设为1 A |= 1 << bit 将A的第bit位设为0 `A &= ~(1 << bit)`` 测试第bit位是否为0 (A & 1 << bit) != 0 取出最后1的值 A&-A or A&~(A-1) or x&#94;(x&(x-1)) 删除最后1的值 A&(A-1) 构建全为1的二进制数 ~0 (((unsigned)~0) >> 1 == 01111111111111111111111111111111) 负数的运算是以补码形式进行的，如果运算结果首位为1，结果也需要求一次补码. 实例 求所给数字二进制表示中的1的数量： int count_one ( int n ) { while ( n ) { n = n & ( n - 1 ); count ++ ; } return count ; } 是4的幂数嘛？ (事实上图检查方法、 迭代和递归可以做到一样的效果) bool isPowerOfFour ( int n ) { return ! ( n & ( n - 1 )) && ( n & 0x55555555 ); //只有一个1位(0x55555555 =1010101010101010101010101010101); } &#94; 的技巧 &#94; 可以用来消除偶数个个一模一样的数字并且保留奇数的数字对, 或者保存不一样的对应位并且移除一样的对应位. 两数求和 使用 &#94; 和 & 来进行两数求和 int getSum ( int a , int b ) { return b == 0 ? a : getSum ( a &#94; b , ( a & b ) << 1 ); //注意终止条件，(0,1)对应位为1，(1,1)对应位进位; } 丢失的数字 已知一个数组包含n个不同的数字： 0, 1, 2, ..., n, 找到那个不在数组中的数字。 例如, 所给数组 = [0, 1, 3] 则返回 2. int missingNumber ( vector < int >& nums ) { int ret = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { ret &#94;= i ; ret &#94;= nums [ i ]; } return ret &#94;= nums . size (); } | 的技巧 保存尽可能多的1位 找到小于或等于N的2的最大的幂数 (最大二进制数). long largest_power ( long N ) { //将所有右侧的位 置为 1. N = N | ( N >> 1 ); N = N | ( N >> 2 ); N = N | ( N >> 4 ); N = N | ( N >> 8 ); N = N | ( N >> 16 ); return ( N + 1 ) >> 1 ; } 反转比特 反转一个所给的 32 bits unsigned integer. uint32_t reverseBits ( uint32_t n ) { unsigned int mask = 1 << 31 , res = 0 ; for ( int i = 0 ; i < 32 ; ++ i ) { if ( n & 1 ) res |= mask ; mask >>= 1 ; n >>= 1 ; } return res ; } uint32_t reverseBits ( uint32_t n ) { uint32_t mask = 1 , ret = 0 ; for ( int i = 0 ; i < 32 ; ++ i ){ ret <<= 1 ; if ( mask & n ) ret |= 1 ; mask <<= 1 ; } return ret ; } & 的技巧 & 具有选择指定的位的功能。 反转整数中的位 x = (( x & 0xaaaaaaaa ) >> 1 ) | (( x & 0x55555555 ) << 1 ); x = (( x & 0xcccccccc ) >> 2 ) | (( x & 0x33333333 ) << 2 ); x = (( x & 0xf0f0f0f0 ) >> 4 ) | (( x & 0x0f0f0f0f ) << 4 ); x = (( x & 0xff00ff00 ) >> 8 ) | (( x & 0x00ff00ff ) << 8 ); x = (( x & 0xffff0000 ) >> 16 ) | (( x & 0x0000ffff ) << 16 ); 位运算符 AND 数字范围 已知范围 [m, n] 其中 0 <= m <= n <= 2147483647, 返回对范围中的所有数据按位计算符AND计算的结果。 例如, 输入 [5, 7], 则返回 4. int rangeBitwiseAnd ( int m , int n ) { int a = 0 ; while ( m != n ) { m >>= 1 ; n >>= 1 ; a ++ ; } return m << a ; } 1的数量 int hammingWeight ( uint32_t n ) { ulong mask = 1 ; int count = 0 ; for ( int i = 0 ; i < 32 ; ++ i ){ //31 will not do, delicate; if ( mask & n ) count ++ ; mask <<= 1 ; } return count ; } 应用 重复DNA序列 所有DNA由一系列缩写为A、G、C、T的核苷酸组成, 例如: \"ACGAATTCCG\". 当研究DNA序列的时候，DNA中的重复序列是一个很重要的的部分. 编写一个函数来找到出现多于两次的十字符长度序列。 例如, 给出 s = \"AAAAACCCCCAAAAACCCCCCAAAAAGGGTTT\", 返回: [\"AAAAACCCCC\", \"CCCCCAAAAA\"]. class Solution { public : vector < string > findRepeatedDnaSequences ( string s ) { int sLen = s . length (); vector < string > v ; if ( sLen < 11 ) return v ; char keyMap [ 1 << 21 ]{ 0 }; int hashKey = 0 ; for ( int i = 0 ; i < 9 ; ++ i ) hashKey = ( hashKey << 2 ) | ( s [ i ] - 'A' + 1 ) % 5 ; for ( int i = 9 ; i < sLen ; ++ i ) { if ( keyMap [ hashKey = (( hashKey << 2 ) | ( s [ i ] - 'A' + 1 ) % 5 ) & 0xfffff ] ++ == 1 ) v . push_back ( s . substr ( i - 9 , 10 )); } return v ; } }; 以上方法会在重复序列出现太多次时候失效。 为了避免这种情况的发生，我们可以使用 unordered_map<int, int> keyMap 来替代这里的 char keyMap[1<<21]{0} . 主元素 对于一个大小为n的数组，求其主元素. 主元素是在数组中出现次数大于 ⌊ n/2 ⌋ 次的元素. (比特计数是不是一个通常方法, 我们通常会应用排序和Moore Voting算法) int majorityElement ( vector < int >& nums ) { int len = sizeof ( int ) * 8 , size = nums . size (); * int count = 0 , mask = 1 , ret = 0 ; for ( int i = 0 ; i < len ; ++ i ) { count = 0 ; for ( int j = 0 ; j < size ; ++ j ) if ( mask & nums [ j ]) count ++ ; if ( count > size / 2 ) ret |= mask ; mask <<= 1 ; } return ret ; } 单一数字 III 对于给予的一个整数数组, 除了一个元素以外的所有元素会出现三次，你的目标是找到那个单一的数字. (这种类型的问题同样也可以用比特计数简单地解决，但在这里我们将使用 digital logic design 来处理它) //inspired by logical circuit design and boolean algebra; //counter - unit of 3; //current incoming next //a b c a b //0 0 0 0 0 //0 1 0 0 1 //1 0 0 1 0 //0 0 1 0 1 //0 1 1 1 0 //1 0 1 0 0 //a = a&~b&~c + ~a&b&c; //b = ~a&b&~c + ~a&~b&c; //return a|b since the single number can appear once or twice; int singleNumber ( vector < int >& nums ) { int t = 0 , a = 0 , b = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { t = ( a &~ b &~ nums [ i ]) | ( ~ a & b & nums [ i ]); b = ( ~ a & b &~ nums [ i ]) | ( ~ a &~ b & nums [ i ]); a = t ; } return a | b ; } 最大字母长度组合 对于给予的一组单词, 找到单词长度(word[i]) * length(word[j])之积的最大值， 要求两个单词不能有相同的字母. 你可以假定所有单词只包含小写字母. 如果没有符合条件的单词组存在则返回 0. 示例 1: 给予 [\"abcw\", \"baz\", \"foo\", \"bar\", \"xtfn\", \"abcdef\"] 返回 16 符合条件的两个单词可以是 \"abcw\", \"xtfn\". 示例 2: 给予 [\"a\", \"ab\", \"abc\", \"d\", \"cd\", \"bcd\", \"abcd\"] 返回 4 符合条件的两个单词可以是 \"ab\", \"cd\". 示例 3: 给予 [\"a\", \"aa\", \"aaa\", \"aaaa\"] 返回 0 没有符合条件的单词组. 显然我们将会频繁使用单词的长度以及比较两个单词是否拥有相同的字母: 使用一个整形数组去预存每个单词的长度将能够有效减少测量长度这一过程频度; 整形是一个四个字节三十二位存储单元, 而我们只有二十六个不同的字母, 所以我们就可以用每一位来表示单词是否包含某一字母. int maxProduct ( vector < string >& words ) { vector < int > mask ( words . size ()); vector < int > lens ( words . size ()); for ( int i = 0 ; i < words . size (); ++ i ) lens [ i ] = words [ i ]. length (); int result = 0 ; for ( int i = 0 ; i < words . size (); ++ i ) { for ( char c : words [ i ]) mask [ i ] |= 1 << ( c - 'a' ); for ( int j = 0 ; j < i ; ++ j ) if ( ! ( mask [ i ] & mask [ j ])) result = max ( result , lens [ i ] * lens [ j ]); } return result ; } 注意 左右位移太多的返回值是 undefined 对于负数的右移太多的返回值是 undefined right operand in shifting should be non-negative, otherwise the result is undefined & 和 | 运算符相对于比较运算符来说拥有更低的优先级 集合 所有的子集 在这种情况下位操作会体现出巨大的优势：当遍历一个N元素集合的所有子集是非常繁琐的时候，而一个N位比特值能够表示其所有子集。 如果A是B的子集，则表达A所需的数字小于直接表示B时候，位操作对于一些动态的编程方案是一个更好的选择。 如果你不介意以逆序遍历子集的话，你也可以在一个特定的子集里遍历到所有可能的子集星矢 (用比特模式表示). 使用的技巧和找到数字中的最低位的方法相似. 如果我们从一个子集中减去1, 则集合的最低的元素将会被清楚, 并且每个更低的元素将会被设置. 然而, 我们仅仅想要在父集合中设置这些更低的元素. 因此遍历的步骤只会是 i = (i - 1) & superset. vector < vector < int >> subsets ( vector < int >& nums ) { vector < vector < int >> vv ; int size = nums . size (); if ( size == 0 ) return vv ; int num = 1 << size ; vv . resize ( num ); for ( int i = 0 ; i < num ; ++ i ) { for ( int j = 0 ; j < size ; ++ j ) if (( 1 << j ) & i ) vv [ i ]. push_back ( nums [ j ]); } return vv ; } 事实上还有另外两种方式： recursion 和 iteration 也能分别解决这个问题. Bitset类 Bitset储存位 (只有两种可能值的元素: 0 or 1, true or false, ...). 这个类有些类似布尔值数组，不过具有空间优化：通常每个元素只占一位 (在大部分系统中比最小的元素种类：char小八倍). // bitset::count #include <iostream> // std::cout #include <string> // std::string #include <bitset> // std::bitset int main () { std :: bitset < 8 > foo ( std :: string ( \"10110011\" )); std :: cout << foo << \" has \" ; std :: cout << foo . count () << \" ones and \" ; std :: cout << ( foo . size () - foo . count ()) << \" zeros. \\n \" ; return 0 ; }","tags":"Python,C","url":"zong-jie-ru-he-yong-wei-yun-suan-lai-jian-dan-gao-xiao-di-jie-jue-wen-ti.html"},{"title":"LeetCode - Find the Difference","text":"Description Given two strings s and t which consist of only lowercase letters. String t is generated by random shuffling string s and then add one more letter at a random position. Find the letter that was added in t . Example: Input : s = \"abcd\" t = \"abcde\" Output : e Explanation : 'e' is the letter that was added . Source link Best practice 使用异或和C++11的遍历特性。 C++ class Solution { public : char findTheDifference ( string s , string t ) { char r = 0 ; for ( char c : s ) r &#94;= c ; for ( char c : t ) r &#94;= c ; return r ; } }; Mark: 6ms Python version class Solution ( object ): def findTheDifference ( self , s , t ): \"\"\" :type s: str :type t: str :rtype: str \"\"\" asc = 0 ; for item in t : asc += ord ( item ) for item in s : asc -= ord ( item ) return chr ( asc ) class Solution ( object ): def findTheDifference ( self , s , t ): \"\"\" :type s: str :type t: str :rtype: str \"\"\" ans = 0 for c in s + t : ans &#94;= ord ( c ) return chr ( ans ) Mark: 35ms Additional","tags":"Python,C","url":"leetcode-find-the-difference.html"},{"title":"LeetCode - Most Frequent Subtree Sum","text":"Description Given the root of a tree, you are asked to find the most frequent subtree sum. The subtree sum of a node is defined as the sum of all the node values formed by the subtree rooted at that node (including the node itself). So what is the most frequent subtree sum value? If there is a tie, return all the values with the highest frequency in any order. Examples 1 Input : 5 / \\ 2 - 3 return [2, -3, 4], since all the values happen only once, return all of them in any order. Examples 2 Input : 5 / \\ 2 - 5 return [2], since 2 happens twice, however -5 only occur once. Note: You may assume the sum of values in any subtree is in the range of 32-bit signed integer. Source link Best practice C++的reg有点慢啊! C++ reg verison /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { private : unordered_map < int , int > mp ; int max = 0 ; public : vector < int > findFrequentTreeSum ( TreeNode * root ) { vector < pair < int , int >> v ; vector < int > res ; calSum ( root ); for ( auto & it : mp ) if ( it . second == max ) res . push_back ( it . first ); return res ; } int calSum ( TreeNode * root ) { if ( root == NULL ) return 0 ; int k ; k = root -> val + calSum ( root -> left ) + calSum ( root -> right ); mp [ k ] ++ ; max = max < mp [ k ] ? mp [ k ] : max ; return k ; } }; Mark: 12ms 普通的dfs遍历,遍历的同时保存最大值出现次数。 Python version - dfs class Solution ( object ): count = 0 def findFrequentTreeSum ( self , root ): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" dic = {} self . helper ( root , dic ) return [ item for item in dic . keys () if dic [ item ] == self . count ] def helper ( self , node , dic ): if not node : return 0 sums = node . val + self . helper ( node . left , dic ) + self . helper ( node . right , dic ) dic [ sums ] = dic . get ( sums , 0 ) + 1 self . count = max ( self . count , dic [ sums ]) return sums Mark: 78ms Additional","tags":"Python,C","url":"leetcode-most-frequent-subtree-sum.html"},{"title":"LeetCode - Detect Capital","text":"Description Given a word, you need to judge whether the usage of capitals in it is right or not. We define the usage of capitals in a word to be right when one of the following cases holds: All letters in this word are capitals, like \"USA\". All letters in this word are not capitals, like \"leetcode\". Only the first letter in this word is capital if it has more than one letter, like \"Google\". Otherwise, we define that this word doesn't use capitals in a right way. Example 1: Input: \"USA\" Output: True Example 2: Input: \"FlaG\" Output: False Note: The input will be a non-empty word consisting of uppercase and lowercase latin letters. Source link Best practice C++的reg有点慢啊! C++ reg verison class Solution { public : bool detectCapitalUse ( string & word ) { const regex pattern ( \"[A-Z]+$|[a-z]+$|[A-Z][a-z]*$\" ); match_results < string :: const_iterator > result ; bool valid = regex_match ( word , result , pattern ); return valid ; } }; Mark: 149ms 利用正则表达式写的标准匹配程序。 Python version v1 - Re solution class Solution ( object ): def detectCapitalUse ( self , word ): \"\"\" :type word: str :rtype: bool \"\"\" return True if re . match ( r '[A-Z]+$|[a-z]+$|[A-Z][a-z]*$' , word ) else False Mark: 45ms 看了其他小伙子的代码之后发现python有自己定义的方法匹配题目所述的三钟string的情况，便有了以下代码。 Python version v2 - Bulit in solution class Solution ( object ): def detectCapitalUse ( self , word ): \"\"\" :type word: str :rtype: bool \"\"\" return word . isupper () or word . istitle () or word . islower () Mark: 38ms Additional","tags":"Python,C","url":"leetcode-detect-capital.html"},{"title":"LeetCode - Convert BST to Greater Tree","text":"Description Given a Binary Search Tree (BST), convert it to a Greater Tree such that every key of the original BST is changed to the original key plus sum of all keys greater than the original key in BST. Example: Input : The root of a Binary Search Tree like this : 5 / \\ 2 13 Output : The root of a Greater Tree like this : 18 / \\ 20 13 Source link Best practice 利用二叉搜索树的特性，以右中左的方式遍历全树。python版本先完成，使用了全局变量记录累计值来辅助递归。C++版本则将变量加入递归,不使用全局变量。 C++ version class Solution { public : int helper ( TreeNode * root , int cum ) { if ( root -> right ) cum = helper ( root -> right , cum ); root -> val = root -> val + cum ; return ( root -> left ) ? helper ( root -> left , root -> val ) : root -> val ; } TreeNode * convertBST ( TreeNode * root ) { if ( ! root ) return NULL ; helper ( root , 0 ); return root ; } }; Mark: 35ms Python version class Solution ( object ): cum = 0 def convertBST ( self , root ): \"\"\" :type root: TreeNode :rtype: TreeNode \"\"\" self . helper ( root ) return root def helper ( self , node ): if not node : return self . helper ( node . right ) self . cum += node . val node . val = self . cum self . helper ( node . left ) Mark: 178ms Additional","tags":"Python,C","url":"leetcode-convert-bst-to-greater-tree.html"},{"title":"LeetCode - Longest Uncommon Subsequence I","text":"Description Given a group of two strings, you need to find the longest uncommon subsequence of this group of two strings. The longest uncommon subsequence is defined as the longest subsequence of one of these strings and this subsequence should not be any subsequence of the other strings. A subsequence is a sequence that can be derived from one sequence by deleting some characters without changing the order of the remaining elements. Trivially, any string is a subsequence of itself and an empty string is a subsequence of any string. The input will be two strings, and the output needs to be the length of the longest uncommon subsequence . If the longest uncommon subsequence doesn't exist, return -1. Example 1: Input: \"aba\", \"cdc\" Output: 3 Explanation: The longest uncommon subsequence is \"aba\" (or \"cdc\"), because \"aba\" is a subsequence of \"aba\", but not a subsequence of any other strings in the group of two strings. Note: Both strings' lengths will not exceed 100. Only letters from a ~ z will appear in input strings. Source link Best practice 这其实是个很憨厚的问题。 For strings A, B, when len(A) > len(B), the longest possible subsequence of either A or B is A, and no subsequence of B can be equal to A. Answer: len(A). When len(A) == len(B), the only subsequence of B equal to A is B; so as long as A != B, the answer remains len(A). When A == B, any subsequence of A can be found in B and vice versa, so the answer is -1. Python version class Solution ( object ): def findLUSlength ( self , a , b ): \"\"\" :type a: str :type b: str :rtype: int \"\"\" if a == b : return - 1 return max ( len ( a ), len ( b )) Mark: 45ms Additional Python String","tags":"Python,C","url":"leetcode-longest-uncommon-subsequence-i.html"},{"title":"LeetCode - Single Number","text":"Description Given an array of integers, every element appears twice except for one. Find that single one. Note: Your algorithm should have a linear runtime complexity. Could you implement it without using extra memory? Source link Best practice 利用异或位运算消除成对出现的元素只留下那个单身元素。 C++ version class Solution { public : int singleNumber ( vector < int >& nums ) { int result = 0 ; int n = nums . size (); for ( int i = 0 ; i < n ; ++ i ) { result = result &#94; nums [ i ]; } return result ; } }; Mark: 16ms Python version class Solution ( object ): def singleNumber ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" result = 0 for item in nums : result &#94;= item return result Mark: 45ms Additional 来谈谈C++ 位运算","tags":"Python,C","url":"leetcode-single-number.html"},{"title":"LeetCode - Find Largest Value in Each Tree Row","text":"Description You need to find the largest value in each row of a binary tree. Example: Input : 1 / \\ 3 2 / \\ \\ 5 3 9 Output : [ 1 , 3 , 9 ] Source link Best practice 设树根为第0层，则第i层的最大值等于结果集result[i]。遍历每一层比较相同序号的根值的最大值。 C++ version #include <iostream> #include <vector> #include <algorithm> class Solution { vector < int > result ; public : vector < int > largestValues ( TreeNode * root ) { helper ( root , 0 ); return result ; } void helper ( TreeNode * node , int level ) { if ( ! node ) return ; if ( result . size () <= level ) { result . push_back ( node -> val ); } else { result [ level ] = max ( result [ level ], node -> val ); } helper ( node -> left , level + 1 ); helper ( node -> right , level + 1 ); } }; Mark: 12ms 本题先生成一行中的所有元素，之后再比较出最大值。 Python version class Solution ( object ): def largestValues ( self , root ): \"\"\" :type root: TreeNode :rtype: List[int] \"\"\" lst = [] child = [ root ] while any ( child ): lst . append ( max ([ item . val for item in child ])) child = [ item for node in child for item in filter ( None ,( node . left , node . right ))] return lst Mark: 96ms Additional C++指针详解","tags":"Python,C","url":"leetcode-find-largest-value-in-each-tree-row.html"},{"title":"LeetCode - Find All Duplicates in an Array","text":"Description Given an array of integers, 1 ≤ a[i] ≤ n (n = size of array), some elements appear twice and others appear once . Find all the elements that appear twice in this array. Could you do it without extra space and in O(n) runtime? Example: Input: [4,3,2,7,8,2,3,1] Output: [2,3] Source link Best practice 数列有两个特征：一是长度为n的数列只会由1到n的数字组成，二是重复数量不超过两次。对于任意一个数列元素i，序号i-1必定存在且唯一，因此我们可以用数列的序号使数列本身成为一个存放已匹配到数字的哈希表——当匹配元素值时，将其对应的序号的元素值设为负数。 C++ version class Solution { public : vector < int > findDuplicates ( vector < int >& nums ) { vector < int > res ; for ( int i = 0 ; i < nums . size (); i ++ ) { int index = abs ( nums [ i ]) - 1 ; if ( nums [ index ] < 0 ) { res . push_back ( abs ( nums [ i ])); } else { nums [ index ] = - nums [ index ]; } } return res ; } }; Mark: 139ms Python version class Solution ( object ): def findDuplicates ( self , nums ): \"\"\" :type nums: List[int] :rtype: List[int] \"\"\" res = [] for x in nums : if nums [ abs ( x ) - 1 ] < 0 : res . append ( abs ( x )) else : nums [ abs ( x ) - 1 ] *= - 1 print ( nums ) return res Mark: 365ms Additional Find duplicates in O(n) time and O(1) extra space","tags":"Python,C","url":"leetcode-find-all-duplicates-in-an-array.html"},{"title":"LeetCode - Beautiful Arrangement","text":"Description Suppose you have N integers from 1 to N. We define a beautiful arrangement as an array that is constructed by these N numbers successfully if one of the following is true for the i th position ( 1 ≤ i ≤ N ) in this array: The number at the i th position is divisible by i . i is divisible by the number at the i th position. Now given N, how many beautiful arrangements can you construct? Example 1: Input : 2 Output : 2 Explanation : The first beautiful arrangement is [ 1 , 2 ]: Number at the 1 st position ( i = 1 ) is 1 , and 1 is divisible by i ( i = 1 ). Number at the 2 nd position ( i = 2 ) is 2 , and 2 is divisible by i ( i = 2 ). The second beautiful arrangement is [ 2 , 1 ]: Number at the 1 st position ( i = 1 ) is 2 , and 2 is divisible by i ( i = 1 ). Number at the 2 nd position ( i = 2 ) is 1 , and i ( i = 2 ) is divisible by 1 . Note : N is a positive integer and will not exceed 15 . Source link Most popular C++ version class Solution { public : int countArrangement ( int N ) { vector < int > vs ; for ( int i = 0 ; i < N ; ++ i ) vs . push_back ( i + 1 ); return counts ( N , vs ); } int counts ( int n , vector < int >& vs ) { if ( n <= 0 ) return 1 ; int ans = 0 ; for ( int i = 0 ; i < n ; ++ i ) { if ( vs [ i ] % n == 0 || n % vs [ i ] == 0 ) { swap ( vs [ i ], vs [ n - 1 ]); ans += counts ( n - 1 , vs ); swap ( vs [ i ], vs [ n - 1 ]); } } return ans ; } }; Mark: 6ms My solution C++ version class Solution { public : int count = 0 ; int size = 0 ; int countArrangement ( int N ) { vector < int > vs ; if ( N == 0 ) return 0 ; for ( int i = 0 ; i < N ; ++ i ) vs . push_back ( 0 ); size = N ; helper ( N , vs ); return count ; } void helper ( int n , vector < int > & vs ) { if ( n <= 0 ) { //边界条件 count ++ ; return ; } for ( int i = size - 1 ; i >= 0 ; -- i ) { if (( vs [ i ] == 0 ) && (( i + 1 ) % n == 0 || n % ( i + 1 ) == 0 )) { //剪枝 vs [ i ] = 1 ; helper ( n - 1 , vs ); vs [ i ] = 0 ; } } } }; Mark: 16ms DP Python version of leetcode user cache = {} class Solution ( object ): def countArrangement ( self , N ): def helper ( i , X ): if i == 1 : return 1 key = ( i , X ) if key in cache : return cache [ key ] total = 0 for j in range ( len ( X )): if X [ j ] % i == 0 or i % X [ j ] == 0 : total += helper ( i - 1 , X [: j ] + X [ j + 1 :]) cache [ key ] = total #用哈希表来储存(位置,([剩余元素])):配对数量 信息 return total return helper ( N , tuple ( range ( 1 , N + 1 ))) Mark: 66ms Additional 回溯算法介绍: https://segmentfault.com/a/1190000006121957","tags":"Python,C","url":"leetcode-beautiful-arrangement.html"},{"title":"LeetCode - Max Consecutive Ones","text":"Description Given a binary array, find the maximum number of consecutive 1s in this array. Example 1: Input: [1,1,0,1,1,1] Output: 3 Explanation: The first two digits or the last three digits are consecutive 1s. The maximum number of consecutive 1s is 3. Note: The input array will only contain 0 and 1. The length of input array is a positive integer and will not exceed 10,000 Source link Most popular My solution C++ version class Solution { public : int findMaxConsecutiveOnes ( vector < int >& nums ) { int cnt = 0 ; int ans = 0 ; for ( int i = 0 ; i < nums . size (); ++ i ) { if ( nums [ i ] == 1 ) { ans = ans > ++ cnt ? ans : cnt ; } else { cnt = 0 ; } } return ans ; } }; Mark: 2ms Python version class Solution ( object ): def findMaxConsecutiveOnes ( self , nums ): \"\"\" :type nums: List[int] :rtype: int \"\"\" lst = [ - 1 ] length = len ( nums ) for i in range ( length ): if nums [ i ] == 0 : lst . append ( i ) lst . append ( length ) return max ( map ( lambda x : x [ 1 ] - x [ 0 ] - 1 , zip ( lst [: - 1 ], lst [ 1 :]))) Mark: 105ms Additional","tags":"Python,C","url":"leetcode-max-consecutive-ones.html"},{"title":"LeetCode - Arithmetic Slices","text":"Description A sequence of number is called arithmetic if it consists of at least three elements and if the difference between any two consecutive elements is the same. For example, these are arithmetic sequence: 1, 3, 5, 7, 9 7, 7, 7, 7 3, -1, -5, -9 The following sequence is not arithmetic. 1, 1, 2, 5, 7 A zero-indexed array A consisting of N numbers is given. A slice of that array is any pair of integers (P, Q) such that 0 <= P < Q < N. A slice (P, Q) of array A is called arithmetic if the sequence: A[P], A[p + 1], ..., A[Q - 1], A[Q] is arithmetic. In particular, this means that P + 1 < Q. The function should return the number of arithmetic slices in the array A. Example: A = [1, 2, 3, 4] return: 3, for 3 arithmetic slices in A: [1, 2, 3], [2, 3, 4] and [1, 2, 3, 4] itself. Source link Most popular python version class Solution ( object ): def numberOfArithmeticSlices ( self , A ): \"\"\" :type A: List[int] :rtype: int \"\"\" opt , i = [ 0 , 0 ], 1 for j in xrange ( 2 , len ( A )): if A [ j ] - A [ j - 1 ] == A [ j - 1 ] - A [ j - 2 ]: opt . append ( opt [ j - 1 ] + i ) i += 1 else : opt . append ( opt [ j - 1 ]) i = 1 return opt [ - 1 ] c++ version for 2ms class Solution { public : int numberOfArithmeticSlices ( vector < int >& A ) { int n = A . size (); if ( n < 3 ) return 0 ; vector < int > dp ( n , 0 ); // dp[i] means the number of arithmetic slices ending with A[i] if ( A [ 2 ] - A [ 1 ] == A [ 1 ] - A [ 0 ]) dp [ 2 ] = 1 ; // if the first three numbers are arithmetic or not int result = dp [ 2 ]; for ( int i = 3 ; i < n ; ++ i ) { // if A[i-2], A[i-1], A[i] are arithmetic, then the number of arithmetic slices ending with A[i] (dp[i]) // equals to: // the number of arithmetic slices ending with A[i-1] (dp[i-1], all these arithmetic slices appending A[i] are also arithmetic) // + // A[i-2], A[i-1], A[i] (a brand new arithmetic slice) // it is how dp[i] = dp[i-1] + 1 comes if ( A [ i ] - A [ i - 1 ] == A [ i - 1 ] - A [ i - 2 ]) dp [ i ] = dp [ i - 1 ] + 1 ; result += dp [ i ]; // accumulate all valid slices } return result ; } }; My solution too ugly to post Additional","tags":"Python","url":"leetcode-arithmetic-slices.html"},{"title":"LeetCode - Optimal Division","text":"Description Given a list of positive integers , the adjacent integers will perform the float division. For example, [2,3,4] -> 2 / 3 / 4. However, you can add any number of parenthesis at any position to change the priority of operations. You should find out how to add parenthesis to get the maximum result, and return the corresponding expression in string format. Your expression should NOT contain redundant parenthesis . Example: **Input**: [1000,100,10,2] **Output**: \"1000/(100/10/2)\" **Explanation**: 1000/(100/10/2) = 1000/((100/10)/2) = 200 However, the bold parenthesis in \"1000/((100/10)/2)\" are redundant, since they don't influence the operation priority. So you should return \"1000/(100/10/2)\". Other cases: 1000/(100/10)/2 = 50 1000/(100/(10/2)) = 50 1000/100/10/2 = 0.5 1000/100/(10/2) = 2 Note: 1. The length of the input array is [1, 10]. 2. Elements in the given array will be in range [2, 1000]. 3. There is only one optimal division for each test case. Source link Most popular Regardless of parentheses, every element is either in the numerator or denominator of the final fraction. The expression A[0] / ( A[1] / A[2] / ... / A[N-1] ) has every element in the numerator except A[1], and it is impossible for A[1] to be in the numerator, so it is the largest. We must also be careful with corner cases. class Solution ( object ): def optimalDivision ( self , A ): A = map ( str , A ) if len ( A ) <= 2 : return '/' . join ( A ) return '{}/({})' . format ( A [ 0 ], '/' . join ( A [ 1 :])) My solution class Solution ( object ): def optimalDivision ( self , nums ): \"\"\" :type nums: List[int] :rtype: str \"\"\" length = len ( nums ) if length == 1 : return str ( nums [ 0 ]) elif length == 2 : return str ( nums [ 0 ]) + \"/\" + str ( nums [ 1 ]) return str ( nums [ 0 ]) + \"/(\" + \"/\" . join ( map ( str , nums [ 1 :])) + \")\" Additional","tags":"Python","url":"leetcode-optimal-division.html"},{"title":"LeetCode - Queue Reconstruction by Height","text":"Description Suppose you have a random list of people standing in a queue. Each person is described by a pair of integers (h, k) , where h is the height of the person and k is the number of people in front of this person who have a height greater than or equal to h . Write an algorithm to reconstruct the queue. Example: Input: [[7,0], [4,4], [7,1], [5,0], [6,1], [5,2]] Output: [[5,0], [7,0], [5,2], [6,1], [4,4], [7,1]] Note: The number of people is less than 1,100. Source link Most popular My solution 先将列表以h DEC, k ASC排序 将元素一一入栈, 若 k > 栈深, 则无解; 否则插入到列表的位置k class Solution ( object ): def reconstructQueue ( self , people ): \"\"\" :type people: List[List[int]] :rtype: List[List[int]] \"\"\" stack = [] for item in sorted ( people , key = lambda x : ( - x [ 0 ], x [ 1 ])): if len ( stack ) < item [ 1 ]: return False stack . insert ( item [ 1 ], item ) return stack Additional","tags":"Python","url":"leetcode-queue-reconstruction-by-height.html"},{"title":"LeetCode - Find Bottom Left Tree Value","text":"Description Given a binary tree, find the leftmost value in the last row of the tree. Example 1: Input : 2 / \\ 1 3 Output : 1 Example 2: Input : 1 / \\ 2 3 / / \\ 4 5 6 / 7 Output : 7 Note: You may assume the tree (i.e., the given root node) is not NULL. Source link Most popular solution class Solution ( object ): def findLeftMostNode ( self , root ): queue = [ root ] for node in queue : queue += filter ( None , ( node . right , node . left )) return node . val My solution Additional It's leftmost in last row not left left node","tags":"Python","url":"leetcode-find-bottom-left-tree-value.html"},{"title":"LeetCode - Island Perimeter","text":"Description You are given a map in form of a two-dimensional integer grid where 1 represents land and 0 represents water. Grid cells are connected horizontally/vertically (not diagonally). The grid is completely surrounded by water, and there is exactly one island (i.e., one or more connected land cells). The island doesn't have \"lakes\" (water inside that isn't connected to the water around the island). One cell is a square with side length 1. The grid is rectangular, width and height don't exceed 100. Determine the perimeter of the island. Example 1: [[0,1,0,0], [1,1,1,0], [0,1,0,0], [1,1,0,0]] Answer: 16 Explanation: The perimeter is the 16 yellow stripes in the image below: Note: 1. All elements in nums1 and nums2 are unique. 2. The length of both nums1 and nums2 would not exceed 1000. Source link Most popular Since there are no lakes, every pair of neighbour cells with different values is part of the perimeter (more precisely, the edge between them is). So just count the differing pairs, both horizontally and vertically (for the latter I simply transpose the grid). def islandPerimeter ( self , grid ): return sum ( sum ( map ( operator . ne , [ 0 ] + row , row + [ 0 ])) for row in grid + map ( list , zip ( * grid ))) My solution Add 4 for each land and remove 2 for each internal edge. class Solution ( object ): def islandPerimeter ( self , grid ): \"\"\" :type grid: List[List[int]] :rtype: int \"\"\" total = 0 for row in range ( len ( grid )): for column in range ( len ( grid [ row ])): if grid [ row ][ column ] == 1 : total += ( int ( grid [ row - 1 ][ column ] == 1 ) * ( row != 0 ) + int ( grid [ row ][ column - 1 ] == 1 ) * ( column != 0 )) * ( - 2 ) + 4 return total Additional","tags":"Python","url":"leetcode-island-perimeter.html"},{"title":"LeetCode - Keyboard Row","text":"Description Given a List of words, return the words that can be typed using letters of alphabet on only one row's of American keyboard like the image below. Example 1: Input: [\"Hello\", \"Alaska\", \"Dad\", \"Peace\"] Output: [\"Alaska\", \"Dad\"] Note: 1. You may use one character in the keyboard more than once. 2. You may assume the input string will only contain letters of alphabet. Source link My solution class Solution ( object ): def findWords ( self , words ): return list ( filter ( re . compile ( r '(?i)&#94;([qwertyuiop]+|[asdfghjkl]+|[zxcvbnm]+)$' ) . match , words )) Additional Regular Expression","tags":"Python","url":"leetcode-keyboard-row.html"},{"title":"LeetCode - Next Greater Element I","text":"Description You are given two arrays (without duplicates) nums1 and nums2 where nums1 's elements are subset of nums2 . Find all the next greater numbers for nums1 's elements in the corresponding places of nums2 . The Next Greater Number of a number x in nums1 is the first greater number to its right in nums2 . If it does not exist, output -1 for this number. Example 1: Input : nums1 = [ 4 , 1 , 2 ], nums2 = [ 1 , 3 , 4 , 2 ]. Output : [- 1 , 3 ,- 1 ] Explanation : For number 4 in the first array , you cannot find the next greater number for it in the second array , so output - 1 . For number 1 in the first array , the next greater number for it in the second array is 3 . For number 2 in the first array , there is no next greater number for it in the second array , so output - 1 . Example 2: Input : nums1 = [ 2 , 4 ], nums2 = [ 1 , 2 , 3 , 4 ]. Output : [ 3 ,- 1 ] Explanation : For number 2 in the first array , the next greater number for it in the second array is 3 . For number 4 in the first array , there is no next greater number for it in the second array , so output - 1 . Note: 1. All elements in nums1 and nums2 are unique. 2. The length of both nums1 and nums2 would not exceed 1000. Source link Most popular class Solution ( object ): def nextGreaterElement ( self , findNums , nums ): \"\"\" :type findNums: List[int] :type nums: List[int] :rtype: List[int] \"\"\" d = {} st = [] ans = [] for x in nums : while len ( st ) and st [ - 1 ] < x : d [ st . pop ()] = x st . append ( x ) for x in findNums : ans . append ( d . get ( x , - 1 )) return ans Mark: 83% My solution class Solution ( object ): def nextGreaterElement ( self , findNums , nums ): \"\"\" :type findNums: List[int] :type nums: List[int] :rtype: List[int] \"\"\" lst = [] for k , i in enumerate ( findNums ): for j in nums [ nums . index ( i ):]: if j > i : lst . append ( j ) break if len ( lst ) != k + 1 : lst . append ( - 1 ) return lst Mark: 50% Additional","tags":"Python","url":"leetcode-next-greater-element-i.html"},{"title":"LeetCode - Reverse Words in a String III","text":"Description Given a string, you need to reverse the order of characters in each word within a sentence while still preserving whitespace and initial word order. Example 1: Input: \"Let's take LeetCode contest\" Output: \"s'teL ekat edoCteeL tsetnoc\" Note: In the string, each word is separated by single space and there will not be any extra space in the string. Source link Most popular solution class Solution ( object ): def reverseWords ( self , s ): \"\"\" :type s: str :rtype: str \"\"\" return \" \" . join ( map ( lambda x : x [:: - 1 ], s . split ())) My solution class Solution ( object ): def reverseWords ( self , s ): \"\"\" :type s: str :rtype: str \"\"\" string = '' for item in s . split (): string += ( item [:: - 1 ] + ' ' ) return string [: - 1 ] Mark: 38% Additional","tags":"Python","url":"leetcode-reverse-words-in-a-string-iii.html"},{"title":"Markdown Syntax","text":"Please Note: This is an unofficial kramdown sandbox, the official complete documentation for kramdown is here . documentation Code This demo uses highlight.js to enable client-side syntax highlighting inline or as a block (auto detect language): def ruby puts \"ruby\" end def hello(): print \"python\" Force language: function hello () { alert ( \"Javascript\" ); } Lists One Two Three Lorem Ipsum Dolar Etc. Dolar Example Meep Meep LaTeX Using MathJax . Inline: $$ \\varphi = \\frac{1+\\sqrt{5}}{2} = 1.61803\\,39887\\ldots. $$ Block: $$ \\int_0&#94;{2\\pi}\\sin{x}\\ dx=0 $$ Tables Header1 Header2 Header3 cell1 cell2 cell3 cell4 cell5 cell6 ---- cell1 cell2 cell3 cell4 cell5 cell6 ===== Foot1 Foot2 Foot3 etc. if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = \"center\", indent = \"0em\", linebreak = \"false\"; if (false) { align = (screen.width < 768) ? \"left\" : align; indent = (screen.width < 768) ? \"0em\" : indent; linebreak = (screen.width < 768) ? 'true' : linebreak; } var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: '\"+ align +\"',\" + \" displayIndent: '\"+ indent +\"',\" + \" showMathMenu: true,\" + \" messageStyle: 'normal',\" + \" tex2jax: { \" + \" inlineMath: [ ['\\\\\\\\(','\\\\\\\\)'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} },\" + \" linebreaks: { automatic: \"+ linebreak +\", width: '90% container' },\" + \" }, \" + \"}); \" + \"if ('default' !== 'default') {\" + \"MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {\" + \"var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;\" + \"VARIANT['normal'].fonts.unshift('MathJax_default');\" + \"VARIANT['bold'].fonts.unshift('MathJax_default-bold');\" + \"VARIANT['italic'].fonts.unshift('MathJax_default-italic');\" + \"VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');\" + \"});\" + \"}\"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"Python","url":"markdown-syntax.html"},{"title":"LeetCode - Array Partition I","text":"Description Given an array of 2n integers, your task is to group these integers into n pairs of integer, say (a1, b1), (a2, b2), ..., (an, bn) which makes sum of min(ai, bi) for all i from 1 to n as large as possible. - Example: ''' Input: [1,4,3,2] Output: 4 Explanation: n is 2, and the maximum sum of pairs is 4. ''' - Note: n is a positive integer, which is in the range of [1, 10000]. All the integers in the array will be in the range of [-10000, 10000]. Source link Most popular solution class Solution ( object ) : def arrayPairSum ( self , nums ) : \"\"\" :type nums: List[int] :rtype: int \"\"\" return sum ( sorted ( nums )[ :: 2 ]) Mark: 87% My solution class Solution ( object ) : def arrayPairSum ( self , nums ) : \"\"\" :type nums: List[int] :rtype: int \"\"\" nums . sort () return sum ( nums [ :: 2 ]) Mark: 85% Additional ''' Help on method_descriptor: sort(...) L.sort(cmp=None, key=None, reverse=False) -- stable sort IN PLACE ; cmp(x, y) -> -1, 0, 1 help(sorted) Help on built-in function sorted in module builtin : sorted(...) sorted(iterable, cmp=None, key=None, reverse=False) --> new sorted list '''","tags":"Python","url":"leetcode-array-partition-i.html"},{"title":"LeetCode - Distribute Candies","text":"Description Given an integer array with even length, where different numbers in this array represent different kinds of candies. Each number means one candy of the corresponding kind. You need to distribute these candies equally in number to brother and sister. Return the maximum number of kinds of candies the sister could gain. - Example 1: Input : candies = [ 1 , 1 , 2 , 2 , 3 , 3 ] Output : 3 Explanation : There are three different kinds of candies ( 1 , 2 and 3 ), and two candies for each kind . Optimal distribution : The sister has candies [ 1 , 2 , 3 ] and the brother has candies [ 1 , 2 , 3 ], too . The sister has three different kinds of candies . - Example 2: Input : candies = [ 1 , 1 , 2 , 3 ] Output : 2 Explanation : For example , the sister has candies [ 2 , 3 ] and the brother has candies [ 1 , 1 ]. The sister has two different kinds of candies , the brother has only one kind of candies . - Note: The length of the given array is in range [2, 10,000], and will be even. The number in given array is in range [-100,000, 100,000]. Source link Most popular solution class Solution ( object ): def distributeCandies ( self , candies ): \"\"\" :type candies: List[int] :rtype: int \"\"\" return min ( len ( candies ) / 2 , len ( set ( candies ))) My solution Same Mark: 97% Additional","tags":"Python","url":"leetcode-distribute-candies.html"},{"title":"LeetCode - Hamming Distance","text":"Description The Hamming distance 1 between two integers is the number of positions at which the corresponding bits are different. Given two integers x and y, calculate the Hamming distance. - Note: 0 ≤ x, y < 231. - Example: Input : x = 1 , y = 4 Output : 2 Explanation : 1 ( 0 0 0 1 ) 4 ( 0 1 0 0 ) ↑ ↑ The above arrows point to positions where the corresponding bits are different . Source link Most popular solution class Solution ( object ): def hammingDistance ( self , x , y ): \"\"\" :type x: int :type y: int :rtype: int \"\"\" return bin ( x &#94; y ) . count ( '1' ) My solution class Solution ( object ): def hammingDistance ( self , x , y ): \"\"\" :type x: int :type y: int :rtype: int \"\"\" bit = x &#94; y count = 0 while bit : if bit & 1 : count += 1 bit = bit >> 1 return count Mark: 42 ms Additional Python 位运算 Description Mark Comment 按位与: & | 按位或: | | | | | 按位异或: | &#94; | | 按位翻转: | ~ | +1 之后乘以 -1| | 左移运算符 | << | X << N 将一个数字X向左移动N位| | 右移运算符 | >> | | 在信息论中，两个等长字符串之间的 汉明距离 (Hamming distance）是两个字符串对应位置的不同字符的个数。换句话说，它就是将一个字符串变换成另外一个字符串所需要替换的字符个数。 汉明重量 是字符串相对于同样长度的零字符串的汉明距离，也就是说，它是字符串中非零的元素个数：对于二进制字符串来说，就是1的个数，所以11101的汉明重量是4。 ↩","tags":"Python","url":"leetcode-hamming-distance.html"},{"title":"LeetCode - Two Sum","text":"Description Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Example: Given nums = [2, 7, 11, 15], target = 9, Because nums[0] + nums[1] = 2 + 7 = 9, return [0, 1]. Subscribe to see which companies asked this question. Source page Most popular solution class Solution ( object ): def twoSum ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" if len ( nums ) <= 1 : return False dict = {} for i in range ( len ( nums )): if nums [ i ] in dict : return [ dict [ nums [ i ]], i ] else : dict [ target - nums [ i ]] = i My solution class Solution ( object ): def twoSum ( self , nums , target ): \"\"\" :type nums: List[int] :type target: int :rtype: List[int] \"\"\" length = len ( nums ) num_temp = sorted ( range ( length ), key = lambda k : nums [ k ]) #排序前的数组序号 nums . sort () i = 0 j = length - 1 while 1 : sumed = nums [ i ] + nums [ j ] if sumed > target : j -= 1 elif sumed < target : i += 1 else : return [ num_temp [ i ], num_temp [ j ]] Mark: 86% Additional","tags":"Python","url":"leetcode-two-sum.html"},{"title":"Top 10 Python libraries of 2016","text":"We try to avoid most established choices such as Django, Flask, etc. that are kind of standard nowadays. Also, some of these libraries date prior to 2016, but either they had an explosion in popularity this year or we think they are great enough to deserve the spot. Here we go! Last year, we did a recap with what we thought were the best Python libraries of 2015 , which was widely shared within the Python community (see post in r/Python ). A year has gone by, and again it is time to give due credit for the awesome work that has been done by the open source community this year. Again, we try to avoid most established choices such as Django, Flask, etc. that are kind of standard nowadays. Also, some of these libraries date prior to 2016, but either they had an explosion in popularity this year or we think they are great enough to deserve the spot. Here we go! 1. Zappa Since the release of AWS Lambda (and others that have followed ), all the rage has been about serverless architectures . These allow microservices to be deployed in the cloud, in a fully managed environment where one doesn't have to care about managing any server, but is assigned stateless, ephemeral computing containers that are fully managed by a provider. With this paradigm, events (such as a traffic spike) can trigger the execution of more of these containers and therefore give the possibility to handle \"infinite\" horizontal scaling. Zappa is the serverless framework for Python , although (at least for the moment) it only has support for AWS Lambda and AWS API Gateway. It makes building so-architectured apps very simple, freeing you from most of the tedious setup you would have to do through the AWS Console or API, and has all sort of commands to ease deployment and managing different environments. 2. Sanic + uvloop Who said Python couldn't be fast? Apart from competing for the best name of a software library ever, Sanic also competes for the fastest Python web framework ever, and appears to be the winner by a clear margin. It is a Flask-like Python 3.5+ web server that is designed for speed. Another library, uvloop , is an ultra fast drop-in replacement for asyncio 's event loop that uses libuv under the hood. Together, these two things make a great combination! According to the Sanic author's benchmark , uvloop could power this beast to handle more than 33k requests/s which is just insane (and faster than node.js ). Your code can benefit from the new async/await syntax so it will look neat too; besides we love the Flask-style API. Make sure to give Sanic a try, and if you are using asyncio , you can surely benefit from uvloop with very little change in your code! 3. asyncpg In line with recent developments for the asyncio framework, the folks from MagicStack bring us this efficient asynchronous (currently CPython 3.5 only) database interface library designed specifically for PostgreSQL. It has zero dependencies, meaning there is no need to have libpq installed. In contrast with psycopg2 (the most popular PostgreSQL adapter for Python) which exchanges data with the database server in text format, asyncpg implements PostgreSQL binary I/O protocol , which not only allows support for generic types but also comes with numerous performance benefits. The benchmarks are clear: asyncpg is on average, at least 3x faster than psycopg2 (or aiopg ), and faster than the node.js and Go implementations. 4. boto3 If you have your infrastructure on AWS or otherwise make use of their services (such as S3), you should be very happy that boto , the Python interface for AWS API, got a completely rewrite from the ground up. The great thing is that you don't need to migrate your app all at once: you can use boto3 and boto (2) at the same time ; for example using boto3 only for new parts of your application. The new implementation is much more consistent between different services, and since it uses a data-driven approach to generate classes at runtime from JSON description files, it will always get fast updates. No more lagging behind new Amazon API features, move to boto3 ! 5. TensorFlow Do we even need an introduction here? Since it was released by Google in November 2015, this library has gained a huge momentum and has become the #1 trendiest GitHub Python repository. In case you have been living under a rock for the past year, TensorFlow is a library for numerical computation using data flow graphs, which can run over GPU or CPU. We have quickly witnessed it become a trend in the Machine Learning community (especially Deep Learning, see our post on 10 main takeaways from MLconf ), not only growing its uses in research but also being widely used in production applications. If you are doing Deep Learning and want to use it through a higher level interface, you can try using it as a backend for Keras (which made it to last years post) or the newer TensorFlow-Slim . 6. gym + universe If you are into AI, you surely have heard about the OpenAI non-profit artificial intelligence research company (backed by Elon Musk et al.). The researchers have open sourced some Python code this year! Gym is a toolkit for developing and comparing reinforcement learning algorithms. It consists of an open-source library with a collection of test problems (environments) that can be used to test reinforcement learning algorithms, and a site and API that allows to compare the performance of trained algorithms (agents). Since it doesn't care about the implementation of the agent, you can build them with the computation library of your choice: bare numpy, TensorFlow, Theano, etc. We also have the recently released universe , a software platform for researching into general intelligence across games, websites and other applications. This fits perfectly with gym , since it allows any real-world application to be turned into a gym environment. Researchers hope that this limitless possibility will accelerate research into smarter agents that can solve general purpose tasks. 7. Bokeh You may be familiar with some of the libraries Python has to offer for data visualization; the most popular of which are matplotlib and seaborn . Bokeh, however, is created for interactive visualization , and targets modern web browsers for the presentation. This means Bokeh can create a plot which lets you_explore_ the data from a web browser. The great thing is that it integrates tightly with Jupyter Notebooks , so you can use it with your probably go-to tool for your research. There is also an optional server component, bokeh-server , with many powerful capabilities like server-side downsampling of large dataset (no more slow network tranfers/browser!), streaming data, transformations, etc. Make sure to check the gallery for examples of what you can create. They look awesome! 8. Blaze Sometimes, you want to run analytics over a dataset too big to fit your computer's RAM. If you cannot rely on numpy or Pandas, you usually turn to other tools like PostgreSQL, MongoDB, Hadoop, Spark, or many others. Depending on the use case, one or more of these tools can make sense, each with their own strengths and weaknesses. The problem? There is a big overhead here because you need to learn how each of these systems work and how to insert data in the proper form. Blaze provides a uniform interface that abstracts you away from several database technologies. At the core, the library provides a way to express computations . Blaze itself doesn't actually do any computation: it just knows how to instruct a specific backend who will be in charge of performing it. There is so much more to Blaze (thus the ecosystem), as libraries that have come out of its development. For example, Dask implements a drop-in replacement for NumPy array that can handle content larger than memory and leverage multiple cores, and also comes with dynamic task scheduling. Interesting stuff. 9. arrow There is a famous saying that there are only two hard problems in Computer Science: cache invalidation and naming things. I think the saying is clearly missing one thing: managing datetimes . If you have ever tried to do that in Python, you will know that the standard library has a gazillion modules and types: datetime , date , calendar , tzinfo , timedelta , relativedelta , pytz , etc. Worse, it is timezone naive by default. Arrow is \"datetime for humans\", offering a sensible approach to creating, manipulating, formatting and converting dates, times, and timestamps. It is a replacement for the datetime type that supports Python 2 or 3, and provides a much nicer interface as well as filling the gaps with new functionality (such as humanize ). Even if you don't really need arrow, using it can greatly reduce the boilerplate in your code. 10. hug Expose your internal API externally, drastically simplifying Python API development. Hug is a next-generation Python 3 (only) library that will provide you with the cleanest way to create HTTP REST APIs in Python. It is not a web framework per se (although that is a function it performs exceptionally well), but only focuses on exposing idiomatically correct and standard internal Python APIs externally. The idea is simple: you define logic and structure once, and you can expose your API through multiple means . Currently, it supports exposing REST API or command line interface. You can use type annotations that let hug not only generate documentation for your API but also provide with validation and clean error messages that will make your life (and your API user's) a lot easier. Hug is built on Falcon's high performance HTTP library, which means you can deploy this to production using any wsgi-compatible server such as gunicorn . Follow the discussion of this post on: Reddit Original source: https://tryolabs.com/blog/2016/12/20/top-10-python…","tags":"posts","url":"top-10-python-libraries-of-2016.html"}]}